{
  "hash": "16d42aaa3ba37c611e941c2c14681515",
  "result": {
    "markdown": "---\ntitle: \"Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore\"\nexecute:\n  freeze: true\n  echo: TRUE\n  warning: false\ndate: \"February 8, 2024\"\n---\n\n\n# 1 Overview\n\n## 1.1 Background\n\nHuman mobility, the movement of human beings in space and time, reflects the spatial-temporal characteristics of human behavior. With the advancement Information and Communication Technologies (ICT) especially in smartphones, a large volume of data related to human mobility have been collected. By using appropriate GIS analysis methods, these data are potentially useful in supporting smart city planning and management.\n\nIn 2020, a very interesting human mobility data set called Grab Posisi was released by GRAB, one of the largest shared taxi operator in South-east Asia. This provides an opportunity for us to explore the geographical and spatio-temporal distribution of Grab hailing services locations in Singapore.\n\n## 1.2 Our Objectives\n\nIn this exercise, we will be exploring the geographical and spatio-network distribution of Grab hailing services locations in Singapore with the use of spatial point patterns analysis techniques.\n\n## 1.3 Our Task\n\n-   Use appropriate functions of **sf** and **tidyverse** to prepare the following geospatial data layer in sf tibble data.frames:\n\n    -   Grab taxi location points either by origins or destinations.\n\n    -   Road layer within Singapore excluding outer islands.\n\n    -   Singapore boundary layer excluding outer islands\n\n-   Use the extracted data to derive traditional Kernel Density Estimation layers.\n\n-   Use the extracted data to derive either Network Kernel Density Estimation (NKDE) or Temporal Network Kernel Density Estimation (TNKDE)\n\n-   Use appropriate **tmap** functions to display the kernel density layers on openstreetmap of Singapore.\n\n-   Describe the spatial patterns revealed by the kernel density maps.\n\n## 1.4 Data Acquisition\n\nTo address the above questions, we would be using the following data sets:\n\n| Type       | Content                                         | Source                                                                                                    |\n|------------------|------------------|-------------------------------------|\n| Geospatial | Road data set of Malaysia, Singapore and Brunei | [OpenStreetMap of Geofabrik download server](https://download.geofabrik.de/)                              |\n| Geospatial | Master Plan 2019 Subzone Boundary (No Sea)      | [data.gov.sg](https://beta.data.gov.sg/collections/1749/datasets/d_8594ae9ff96d0c708bc2af633048edfb/view) |\n| Aspatial   | Grab-Posisi of Singapore                        | [engineering.grab.com](https://engineering.grab.com/grab-posisi)                                          |\n\n## 1.5 Install and Load R Packages\n\nIn this exercise, the following R packages will be used:\n\n-   **tidyverse**: to read, manipulate and create tidy data, and to create data graphics\n\n-   **sf**: to provide simple features access to represent and work with spatial vector data such as points and polygons\n\n-   **spatstat**: to perform statistical analysis of spatial data\n\n-   **raster**: to read, write, manipulate, analyze and model spatial data\n\n-   **maptools**: tools for handling spatial objects\n\n-   **tmap**: to create thematic and high-quality cartographic maps\n\n-   **arrow**: improve the performance of data analysis methods, and to increase the efficiency of moving data from one system or programming language to another\n\n-   **spNetwork**: to perform Spatial Point Patterns Analysis such as kernel density estimation (KDE) and K-function on network. It also can be used to build spatial matrices (‘listw’ objects like in ‘spdep’ package) to conduct any kind of traditional spatial analysis with spatial weights based on reticular distances.\n\n-   **classInt**: provides a uniform interface to finding class intervals for continuous numerical variables\n\n-   **lubridate**: to parse and manipulate dates\n\n-   **ggplot2**: to create data visualizations\n\nTo install and load the packages, we will use p_load() from the pacman package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(tidyverse, sf, spatstat, raster, maptools, tmap, arrow, spNetwork, classInt, lubridate, ggplot2)\n```\n:::\n\n\n# 2 Data Preparation (Geospatial)\n\nLet's begin getting our hands dirty by introducing and preparing the geospatial data sets in R!\n\n## 2.1 Data Import\n\nTo import geospatial data, we will be using **st_read()** from the **sf** package.\n\nRoad data set from OSM (shapefile format):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroaddata_sf <- st_read(dsn = \"data/geospatial\", \n                       layer = \"gis_osm_roads_free_1\")\n```\n:::\n\n\nMaster Plan 2019 Subzone Boundary (No Sea) (geojson format):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpsz_sf <- st_read(\"data/geospatial/MasterPlan2019SubzoneBoundaryNoSeaGEOJSON.geojson\")\n```\n:::\n\n\n## 2.2 Data Preparation\n\nIt is important to ensure our geospatial data is clean, in the correct coordinate reference system (CRS) and extracted to contain only relevant data to prevent complications later on.\n\nIn this section, we will go through the procedures to prepare our geospatial data.\n\n### 2.2.1 Data Pre-Processing\n\nTo begin with, let's examine the data sets to understand their features.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroaddata_sf\n```\n:::\n\n\nFrom the above, we can see that roaddata_sf is an sf object, with linestring geometry type and dimension XY.\n\nWe also notice that it is in WGS84 geodetic CRS, which is not our desired coordinate reference system (svy 21). Hence, we would have to reproject it later (Section 2.2.2).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpsz_sf\n```\n:::\n\n\nFor mpsz_sf, we see that it is an sf object with multipolygon geometry type. It comprises of records with XYZ coordinates, indicating a Z-dimension, quite redundant to us.\n\nHere, we notice that mpsz_sf has geodetic CRS of WGS84 as well. Hence, we will need to fix the CRS for mpsz_sf later (Section 2.2.2) as well.\n\n#### 2.2.1.1 Dropping Z-dimension\n\nAfter having an understanding of our data sets, we will start to modify them into our desired dimensions and systems.\n\nIn this step, we will remove the Z-dimension in mpsz_sf, with the use of st_zm(). st_zm() is a function used to drop or add Z and/or M dimensions, from sf package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpsz_sf <- st_zm(mpsz_sf)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmpsz_sf\n```\n:::\n\n\nWith that, we can see that the mpsz_sf has become two-dimensional (XY).\n\n#### 2.2.1.2 Invalid Geometries\n\nTo check whether our data sets contain invalid geometries, we can apply the following code chunks:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(which(st_is_valid(roaddata_sf) == FALSE))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(which(st_is_valid(mpsz_sf) == FALSE))\n```\n:::\n\n\nWe see that roaddata_sf has no invalid geometry, while mpsz_sf has 6 invalid geometries.\n\nTo correct the invalid geometries in mpsz_sf, we can use st_make_valid() from sf package,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpsz_sf <- st_make_valid(mpsz_sf)\n```\n:::\n\n\nand check confirm whether the modified mpsz_sf data set now contains fully valid geometries.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(which(st_is_valid(mpsz_sf) == FALSE))\n```\n:::\n\n\nGreat! Our geographic data are now cleared of invalid geometries.\n\n#### 2.2.1.3 Handling Missing Values\n\nNext, we will check for missing values in our geographic data.\n\nLet's begin with checking roaddata_sf, we can applying the following code chunk.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroaddata_sf[rowSums(is.na(roaddata_sf)) != 0, ]\n```\n:::\n\n\nWow, there is an absurd 1719007 records with missing values?!\n\nWe can investigate what could be wrong using View(roaddata_sf) (not shown here).\n\nFrom the result of View(), it seems like it is the \"ref\" field comprises of many missing values! Let's try removing it, and check again!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroaddata_sf <- roaddata_sf %>% dplyr::select(-ref) \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nroaddata_sf[rowSums(is.na(roaddata_sf)) != 0, ]\n```\n:::\n\n\nFrom the code above, which I did not display the result, it would appear that there are still a lot of missing values, particularly in the \"name\" field. However, we should not delete those records because they are not necessarily redundant: in Singapore, if a road length is less than 60m, it need not be named. Thus, these records might still represent valid roads that are shorter than 60m!\n\nThen, ignoring the missing values in \"name\" field, let's check whether there are missing values in the other fields.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroaddata_sf[rowSums(is.na(roaddata_sf %>% dplyr::select(-name))) != 0, ]\n```\n:::\n\n\nPhew, finally! There are no missing values in other fields. Seems like roaddata_sf is cleared of missing values that requires our attention.\n\nNext, let's check mpsz_sf!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpsz_sf[rowSums(is.na(mpsz_sf)) != 0, ]\n```\n:::\n\n\nThankfully, there's no missing values in mpsz_sf!\n\nHurray, we're done with resolving the missing values!\n\n### 2.2.2 Verifying and Transforming CRS\n\nTo check the CRS of the data sets, we can use st_crs() from sf package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nst_crs(roaddata_sf)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nst_crs(mpsz_sf)\n```\n:::\n\n\nFrom the above codes, and as also noticed earlier in Section 2.2.1, we would see that the data sets are in the WGS84 CRS. However, in Singapore, we should use the SVY21 CRS (with EPSG code: 3414) as it is more appropriate for our analysis.\n\nTo change the CRS of the data sets, we can use st_transform() from sf package, inputting the EPSG code for SVY21 (3414) as the second argument of the function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroaddata_sf <- st_transform(roaddata_sf, 3414)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmpsz_sf <- st_transform(mpsz_sf, 3414)\n```\n:::\n\n\nThen, let's confirm that the CRS for the data sets have been correctly modified.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nst_crs(roaddata_sf)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nst_crs(mpsz_sf)\n```\n:::\n\n\nHooray! Our geospatial data are now in the correct CRS!\n\n### 2.2.3 Extraction of relevant data\n\nAfter doing some cleaning of our geospatial data, let's roughly visualize how they look like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\n\ntm_shape(roaddata_sf) + \n  tm_lines()\n```\n:::\n\n\n![](images/map%20of%20malaysia%20sg%20brunei.png){width=\"594\"}\n\nAs expected, roaddata_sf contains the visualization of Malaysia, Singapore and Brunei.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntm_shape(mpsz_sf) +\n  tm_polygons()\n```\n:::\n\n\n![](images/mpsz_sf%20with%20outer%20islands.png){width=\"579\"}\n\nAnd for mpsz_sf, since it is supposed to contain data of Singapore's territories, the visualization displays the map as such.\n\nHowever, in this exercise, we are interested in the data that includes only Singapore without its outer islands, we will have to remove all outer islands outside of Singapore mainland. Hence, we need to do some manipulation to remove the outer islands.\n\nTo begin with, we will remove the outer islands from mpsz_sf first using str_detect() from the **stringr** package and filter() from the **dplyr** package, and name the new data frame mpsz_sgsf\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpsz_sgsf <- mpsz_sf %>% filter(!str_detect(Description, \"ISLAND\"))\n```\n:::\n\n\nThen, to resolve internal boundaries,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsgboundary_sf <- mpsz_sgsf %>% st_union\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsgboundary_sf\n```\n:::\n\n\nLet's make a quick plot using qtm() from the **tmap** package to check if we've successfully extracted data of Singapore's boundary layer excluding outer islands:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\nqtm(sgboundary_sf)\n```\n:::\n\n\n=![](images/sgboundary_sf.png){width=\"591\"}\n\nYay! We've gotten the Singapore boundary layer that excludes the outer islands!\n\nNext, to derive the road layers that lie within Singapore, we can use st_contains() from the **sf** package. Here, we shall form a new data set road_sf which should contain geospatial data with road layers within Singapore, without its outer islands.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroad_sf <- roaddata_sf[st_contains(sgboundary_sf, roaddata_sf, sparse = FALSE),]\n```\n:::\n\n\nLet's break down the code above:\n\n-   The first argument of st_contains() is the boundary which we want our result data to contain, and the second argument is the road data that we want to extract.\n\n-   Then, with sparse = FALSE, we ensure a complete containment check for each feature.\n\n-   However, it is good to note that this option could result potentially longer running time for large datasets.\n\n-   Then, we use the square brackets to extract the data for which st_contains is TRUE, that is, road networks that fall within the Singapore boundary.\n\nNow, let's check that roadsg_sf contains the road layer only within Singapore by plotting a map!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqtm(road_sf)\n```\n:::\n\n\n![](images/road_sf.png){width=\"590\"}\n\nTrue enough, this should be how the road system of Singapore, without its outer islands, looks like. YAY!\n\n# 3 Data Preparation (Aspatial)\n\nNow, it's time to introduce our aspatial data set!\n\n## 3.1 Importing Aspatial Data\n\nTo import aspatial data, we will be using **read_parquet()** from arrow package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrabposisi0 <- read_parquet(\"data/aspatial/part-00000-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\ngrabposisi1 <- read_parquet(\"data/aspatial/part-00001-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\ngrabposisi2 <- read_parquet(\"data/aspatial/part-00002-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\ngrabposisi3 <- read_parquet(\"data/aspatial/part-00003-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\ngrabposisi4 <- read_parquet(\"data/aspatial/part-00004-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\ngrabposisi5 <- read_parquet(\"data/aspatial/part-00005-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\ngrabposisi6 <- read_parquet(\"data/aspatial/part-00006-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\ngrabposisi7 <- read_parquet(\"data/aspatial/part-00007-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\ngrabposisi8 <- read_parquet(\"data/aspatial/part-00008-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\ngrabposisi9 <- read_parquet(\"data/aspatial/part-00009-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\n```\n:::\n\n\nTo combine all of them into one single data frame named grabposisi, we can use rbind().\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrabposisi <- rbind(grabposisi0, grabposisi1, grabposisi2, grabposisi3, grabposisi4, grabposisi5, grabposisi6, grabposisi7, grabposisi8, grabposisi9)\n```\n:::\n\n\nTadah! Our grabposisi data sets are consolidated and ready for further preparation.\n\ngrab## 3.1 Data Preparation\n\nNext, let's examine our new consolidated grabposisi data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(grabposisi)\n```\n:::\n\n\n### 3.1.1 Changing Data Types\n\nFrom running the code above, we would see that the pingtimestamp field is in the integer format, when it is supposed to be a datetime type of data. To convert it into datetime format, we can use the following code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrabposisi$pingtimestamp <- as_datetime(grabposisi$pingtimestamp)\n```\n:::\n\n\nNow, it is in a more appropriate for interpretation and manipulation later!\n\n### 3.1.2 Conversion into sf tibble data frame\n\nNext, we will use the code below to convert the grabposisi data into sf tibble data frame. Since the data we are drawing the geometry from are in longitude (rawlat field) and latitude (rawlng field) (i.e. WGS84; EPSG = 4326) format, we have to use EPSG: 4326 to retrieve the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrabposisi_sf <- st_as_sf(grabposisi, coords = c(\"rawlng\", \"rawlat\"), crs = 4326) \n```\n:::\n\n\nTo check our transformed data frame, we apply the code below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrabposisi_sf\n```\n:::\n\n\n### 3.1.3 Verifying and Transforming CRS\n\nBefore we proceed further, let's reproject our grabposisi_sf into our desired svy21 projection system (EPSG:3414) using st_transform().\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrabposisi_sf <- grabposisi_sf %>% st_transform(3414)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nst_crs(grabposisi_sf)\n```\n:::\n\n\nIt's now in svy21 projection system, we're good to move on!\n\n### 3.1.4 Extracting Study Data\n\nConsidering the fact that we are only interested in the Grab taxi location points by origins in this study, we can apply the following code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\norigin_grab_sf <- grabposisi_sf %>%\n  group_by(trj_id) %>%\n  arrange(pingtimestamp) %>%\n  filter(row_number() == 1) %>%\n  mutate(weekday = wday(pingtimestamp, \n                        label = TRUE,\n                        abbr = TRUE),\n         start_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n```\n:::\n\n\nThe code might look a bit complicating, but let's go through this step by step through the pipelines:\n\n-   Firstly, we use group_by() from dplyr to group the records according to trajectory IDs, so that records of the same journey are together.\n\n-   Then, we use arrange() from dplyr to arrange the records according to time in ascending order. With that, the first record of each group would be the earliest record of the journey, inferring that it is the origin point.\n\n-   Next, we use filter() from dplyr to filter each group, keeping only their first row. This allows us to keep only the records relating to the origin points of each journey.\n\n-   Lastly, we use mutate() from dplyr, along with wday(), hour() and mday() from lubridate to retrieve the weekdays, start_hr, and day of the week to support our analysis later.\n\nTadah! We have successfully extracted the Grab taxi location points by origin.\n\nNext, we should check again whether it is in the correct CRS.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nst_crs(origin_grab_sf)\n```\n:::\n\n\nIt appears to be right! Let's have a glimpse of the result origin_grab_sf data set before we move on.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(origin_grab_sf)\n```\n:::\n\n\n# 4 Exploratory Data Analysis (EDA) and Choropleth Mapping\n\nIn this section, we will conduct some EDA on our aspatial data to gain some rough insights to lead us into\n\nLet's have a look at how our combined data looks like!\n\nSome questions we might be curious about are: - Which are the peak days of the week to take a Grab ride? - When are the peak times throughout the day to hitch a Grab ride? - Where do many of the Grab rides begin? Are they evenly distributed across Singapore? - If so, when are the peak times and days wh\n\n## 4.1 Days of the Week\n\nTo look at the peak days, we can plot a bar chart using ggplot2.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(origin_grab_sf) + \n  geom_bar(aes(x=weekday))\n```\n:::\n\n\n![](images/weekday%20plot.png)\n\nAt a glance, we see that the number of grab rides taken across different days do not appear drastically different.\n\n## 4.2 Peak Times\n\nTo plot the number of rides across different timings of the day, we can use the following code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhours_of_day <- 0:23\n\norigin_grab_sf$start_hr <- factor(origin_grab_sf$start_hr, levels = hours_of_day)\n\nggplot(origin_grab_sf, aes(x = start_hr)) +\n  geom_bar(stat = \"count\") +\n  labs(x = \"Start Hour\", y = \"Count of Rides\") +\n  scale_x_discrete(limits = hours_of_day) \n```\n:::\n\n\n![](images/hours%20plot.png)\n\nIt appears that past 12am, and around 10am to 11am are the peak periods of ride hailing services! This could be due to most public buses and trains not operating past midnight, resulting in a higher demand for the most popular alternative in Singapore: Grab ride services. Also, rides could be peaking around 10am and 11am as people are travelling to work.\n\n## 4.3 Mapping the geospatial data sets\n\nNext, it is also useful for us to create a pin map to show the spatial patterns of our data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\ntm_shape(sgboundary_sf) + tm_polygons() +\n  tm_shape(roadsg_sf) + tm_lines() + \n  tm_shape(origin_grab_sf) + tm_dots()\n```\n:::\n\n\nIn this exercise, we use static mode with \"plot\" argument in tmap_mode() to ease the process of uploading this exercise.\n\nHowever, we can choose to use \"view\" argument instead if we want to navigate and zoom around the map freely. Using \"view\", we can also query the information of each simple feature (i.e the point) by clicking on it.\n\n# 5 First-order Spatial Point Patterns Analysis (SPPA)\n\n## 5.1 Geospatial Data Wrangling for Traditional Kernel Density Estimate (KDE)\n\nTo conduct first-order spatial point analysis on our data, we will be using the **spatstat** package. However, the spatstat package requires our data to be in sp's Spatial classes. In this section, we will convert the sf data frames to sp's Spatial class, then to Spatial objects, and finally ppp and owin for our analysis.\n\n### 5.1.1 Converting sf data frames to sp's Spatial class\n\nTo do so, we would use as_Spatial() from the **sf** package to convert our three geospatial data from sf data frame to sp's Spatial class.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsgboundary <- as_Spatial(sgboundary_sf)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\norigin<- as_Spatial(origin_grab_sf)\n```\n:::\n\n\n### 5.1.2 Converting the Spatial class into generic sp format\n\nspatstat requires the analytical data to be in ppp object form. However, as there is no direct method to convert Spatial classes into ppp objects, we would have to convert the Spatial classes into Spatial objects first.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsgboundary_sp <- as(sgboundary, \"SpatialPolygons\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\norigin_sp <- as(origin, \"SpatialPoints\")\n```\n:::\n\n\n### 5.1.3 Converting the generic sp format into spatstat's ppp format\n\n\n::: {.cell}\n\n```{.r .cell-code}\norigin_ppp <- as(origin_sp, \"ppp\")\n```\n:::\n\n\nHere, we can plot origin_ppp to examine the difference.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(origin_ppp)\n```\n:::\n\n\n![](images/origin_ppp.png)\n\nFor a quick understanding of the summary statistics of the newly created ppp object, we can apply the summary() from base R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(origin_ppp)\n```\n:::\n\n\n### 5.1.4 Handling duplicated points\n\nWe can check for duplication in a ppp object using the following code chunk:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nany(duplicated(origin_ppp))\n```\n:::\n\n\nYay, we have no duplicate points, so we don't have to do anything here.\n\n### 5.1.5 Creating owin object\n\nWhen analyzing spatial potterns, it is good practice to confine our analysis within a geograhical area. In **spatstat**, an object called owin is designed to represent this polygonal region.\n\nThe following code chunk is used to convert sgboundary_sf into owin object of **spatstat**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsgboundary_owin <- as.owin(sgboundary_sf)\n```\n:::\n\n\nThe output object can be displayed by using the plot() function,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(sgboundary_owin)\n```\n:::\n\n\n![](images/sgboundary_owin-01.png)\n\nand summary() function of BaseR.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(sgboundary_owin)\n```\n:::\n\n\n### 5.1.6 Combining point events object and owin object\n\nIn this last step of geospatial data wrangling, we will extract origin locations that are within Singapore using the following code chunk:\n\n\n::: {.cell}\n\n```{.r .cell-code}\noriginSG_ppp <- origin_ppp[sgboundary_owin]\n```\n:::\n\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(originSG_ppp)\n```\n:::\n\n\nTo plot the newly derived originSG_ppp map,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(originSG_ppp)\n```\n:::\n\n\n![](images/originSG_ppp-02.png)\n\n## 5.2 Deriving Traditional Kernel Density Estimation (KDE) Layers\n\nIn this section, we will be performing first-order SPPA using the **spatstat** package. In particular, we will be deriving the kernel density estimation (KDE) layer for visualizing and exploring the intensity of point processes (origin points of Grab rides).\n\n### 5.2.1 Computing KDE using automatic bandwidth selection method\n\nThe code chunk below computed a Kernel Density byy using the following configurations of density() of **spatstat**.\n\nAutomatic bandwidth selection method: bw.ppl() - other methods: bw.CvL(), bw.scott() or bw.diggle()\n\nSmoothing kernel: \"gaussian\" - other methods: \"epanechnikov\", \"quartic\" or \"disc\"\n\nThe intensity estimate is corrected for edge effect bias by using edge = TRUE.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkde_originSG_bwppl <- density(originSG_ppp,\n                           sigma = bw.ppl,\n                           edge = TRUE,\n                           kernel = \"gaussian\")\n```\n:::\n\n\nThen, we will plot the derived kernel density.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(kde_originSG_bwppl)\n```\n:::\n\n\nTo retrieve the bandwidth used to compute the KDE layer, we use the following code chunk:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbw <- bw.ppl(originSG_ppp)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbw\n```\n:::\n\n\n### 5.2.2 Rescaling KDE values\n\nIn the following code chunk, rescale() is used to convert the unit of measurement from meter to kilometer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noriginSG_ppp.km <- rescale(originSG_ppp, 1000, \"km\")\n```\n:::\n\n\nNow, we can re-run density() using the rescaled data set and plot the output KDE map.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkde_originSG_bwppl.km <- density(originSG_ppp.km,\n                              sigma = bw.ppl,\n                              edge = TRUE, \n                              kernel = \"gaussian\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(kde_originSG_bwppl.km)\n```\n:::\n\n\n![](images/kde_originSG_bwppl.km.png)\n\nNow, we can see that the output image looks identical to the earlier version, but with more interpretable data values in the legend.\n\n### 5.2.3 Comparing different automatic bandwidth methods\n\nAside from bw.diggle, as mentioned before, there are three other **spatstat** functions (bw.CvL(), bw.scott(), bw.ppl()) that can be used to determine the bandwidth automatically.\n\nLet's take a look at the bandwidth used by each of these automatic bandwidth calculation methods, keeping all our kernel method (\"gaussian\") constant!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbw.CvL(originSG_ppp.km)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbw.scott(originSG_ppp.km)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbw.diggle(originSG_ppp.km)\n```\n:::\n\n\nThe following code chunk is used to compare the difference in output using the different automatic bandwidth methods.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkde_originSG_bwCvL <- density(originSG_ppp.km,\n                              sigma = bw.CvL,\n                              edge = TRUE,\n                              kernel = \"gaussian\")\nkde_originSG_bwscott <- density(originSG_ppp.km,\n                                sigma = bw.scott,\n                                edge = TRUE,\n                                kernel = \"gaussian\")\nkde_originSG_bwdiggle <- density(originSG_ppp.km,\n                                 sigma = bw.diggle,\n                                 edge = TRUE, \n                                 kernel = \"gaussian\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2,2))\n\nplot(kde_originSG_bwppl.km, main = \"bw.ppl\")\nplot(kde_originSG_bwCvL, main = \"bw.CvL\")\nplot(kde_originSG_bwscott, main = \"bw.scott\")\nplot(kde_originSG_bwdiggle, main = \"bw.diggle\")\n```\n:::\n\n\n![](images/comparing bw-01.png)\n\n### 5.2.4 Comparing different kernel methods\n\nBy default, the kernel method used in density() is Gaussian. However, there are three other available method: Epanechnikov, Quartic and Dics\n\nIn the following code chunk, we will compare the different kernel methods, using originSG_ppp.km and automatic bandwidth method bw.ppl.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,2))\n\nplot(density(originSG_ppp.km,\n             sigma = bw.ppl,\n             edge = TRUE,\n             kernel = \"gaussian\"),\n     main = \"gaussian\")\nplot(density(originSG_ppp.km,\n             sigma = bw.ppl,\n             edge = TRUE,\n             kernel = \"epanechnikov\"),\n     main = \"Epanechnikov\")\nplot(density(originSG_ppp.km,\n             sigma = bw.ppl,\n             edge = TRUE,\n             kernel = \"quartic\"),\n     main = \"Quartic\")\nplot(density(originSG_ppp.km,\n             sigma = bw.ppl,\n             edge = TRUE,\n             kernel = \"disc\"),\n     main = \"Disc\")\n```\n:::\n\n\n![](images/comparing kernels-01.png)\n\n### 5.2.5 Fixed and Adaptive KDE\n\n#### 5.2.5.1 Computing KDE by using fixed bandwidth\n\nHere, we will compute a KDE later by defining a bandwidth of 50m.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkde_originSG_50 <- density(originSG_ppp.km,\n                           sigma = 0.05,\n                           edge = TRUE,\n                           kernel = \"gaussian\")\n```\n:::\n\n\nHere, we use sigma = 0.05 as sigma is expressed in kilometers. Hence, sigma = 0.05 would denote 50m.\n\n#### 5.2.5.2 Computing KDE by using adaptive bandwidth\n\nHere, we will derive adaptive KDE using density.adaptive() of **spatstat**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkde_originSG_adaptive <- adaptive.density(originSG_ppp.km,\n                                          method = \"kernel\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(kde_originSG_adaptive)\n```\n:::\n\n\n![](images/kde_originSG_adaptive-01.png)\n\n### 5.2.6 Converting KDE output into grid object\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngridded_kde_originSG_bwppl <- as.SpatialGridDataFrame.im(kde_originSG_bwppl.km)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nspplot(gridded_kde_originSG_bwppl)\n```\n:::\n\n\n![](images/gridded_kde_originSG_bwppl-02.png)\n\n### 5.2.7 Converting gridded output into raster\n\nNext, we will convert the gridded kernel denstiy objects into RasterLayer object using raster() of **raster** package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkde_originSG_bwppl_raster <- raster(gridded_kde_originSG_bwppl)\n```\n:::\n\n\nWe can look at the properties of this new kde_originSG_bwppl_raster RasterLayer object using the following code chunk:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkde_originSG_bwppl_raster\n```\n:::\n\n\n### 5.2.8 Assigning projection systems\n\nSince the crs property of kde_originSG_bwppl_raster RasterLayer object is NA, we will include CRS information for it using the code below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprojection(kde_originSG_bwppl_raster) <- CRS(\"+init=EPSG:3414\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nkde_originSG_bwppl_raster\n```\n:::\n\n\nNow, the crs property is complete!\n\n### 5.2.9 Visualizing the KDE on OpenStreetMap\n\nFinally, we will display the raster on OpenStreetMap of Singapore. To do so, we need to apply the following two steps.\n\n#### 5.2.9.1 Reprojecting raster object\n\nFrom earlier, we established that our raster object is in SVY21 (EPSG:3414). However, OpenStreetMap works with WGS84 projection system (EPSG:4326). Hence, to lay our KDE raster layer on OpenStreetMap, we must reproject its coordinate system.\n\nTo do so, we can use projectRaster from the **raster** package to apply the following code, with the second argument as the EPSG (4326) of WGS84, creating a new object kde_originSG_bwppl_raster_reproj which is the WGS84 version of our raster object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkde_originSG_bwppl_raster_reproj <- projectRaster(kde_originSG_bwppl_raster, crs = 4326)\n```\n:::\n\n\n#### 5.2.9.2 Creating the visualization\n\nThen, we can use the following code, with **tmap** functions to display the KDE layer on OpenStreetMap. The \"view\" argument in tmap_mode() should give an interactive map. However, I will only display a snapshot of the result here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"view\")\ntm_shape(kde_originSG_bwppl_raster_reproj) +\n  tm_raster(\"v\") + \n  tm_layout(legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_basemap(\"OpenStreetMap\")\n```\n:::\n\n\n![](images/Screenshot 2024-02-08 at 6.19.52 PM.png)\n\nTadah, we're done with displaying the KDE layer on OpenStreetMap of Singapore!\n\n## 5.3 Geospatial Data Preparation for Network Kernel Density Estimation (NetKDE)\n\nNetwork Constrained Spatial Point Patterns Analysis is a collection of spatial point patterns analysis methods specially developed to analyse spatial point event occurrences on or alongside networks. In our case, our event of analysis would be origin points of Grab ride journeys at 1-hour peak hour time frames. In this study, I will look into the 10am and 12am 1-hour peak travel hour in Tampines, a large and highly populated residential town for our analysis.\n\n### 5.3.1 Extracting study area\n\nTo retrieve boundary of Tampines, we apply the following code, similar to what we did in Section 2.2.3.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntampines_sg_sf <- mpsz_sf %>% filter(str_detect(Description, \"TAMPINES\")) %>% st_union()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\norchard_sg_sf <- mpsz_sf %>% filter(str_detect(Description, \"ORCHARD\")) %>% st_union()\n```\n:::\n\n\nTo retrieve the road layer of Tampines and Orchard, we apply the code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntampines_road_sf <- road_sf[st_contains(tampines_sg_sf, road_sf, sparse = FALSE),]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\norchard_road_sf <- road_sf[st_contains(orchard_sg_sf, road_sf, sparse = FALSE),]\n```\n:::\n\n\nAnd to retrieve the origin points of Grab rides in Tampines and Orchard,\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntampines_origin_sf <- origin_grab_sf[unlist(st_contains(tampines_sg_sf, origin_grab_sf)),]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\norchard_origin_sf <- origin_grab_sf[unlist(st_contains(orchard_sg_sf, origin_grab_sf)),]\n```\n:::\n\n\n### 5.3.2 Preparing the lixels objects\n\nBefore computing NetKDE, our road_sf linestring data need to be cut into lixels with a specified minimal distance.\n\nTo do so, we can use lixelize_lines() from **spNetwork** package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntampines_lixels <- lixelize_lines(tampines_road_sf,\n                         500,\n                         mindist = 150)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\norchard_lixels <- lixelize_lines(orchard_road_sf,\n                         500,\n                         mindist = 150)\n```\n:::\n\n\nIn the code chunk above, we have set:\n\n-   length of lixel, lx_length = 500m\n\n-   minimum length of lixel, mindist = 150m; as Singapore is small with a presumably large number of origin points all over the island\n\nNote: After the cut, if the length of the final lixel is shorter than the minimum distance, it would be added to the previous lixel. On the other hand, if mindist is NULL, then mindist = maxdist/10. Segments that are already shorter than the minimum distance are not modified.\n\n### 5.3.3 Generating line centre points\n\nNext, lines_center() of spNetwork will be used to generate a SpatialPointsDataFrame with line centre points as shown below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntampines_centers <- lines_center(tampines_lixels)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\norchard_centers <- lines_center(orchard_lixels)\n```\n:::\n\n\n## 5.4 Performing Network KDE\n\nIn this section, we will perform the NetKDE. To begin with, let's filter our data set so that we only have data for that of Tampines and Orchard areas, in order to ease to focus and reduce the computational complication in our analysis.\n\nNext, we will compute their respective NetKDE using the code chunks below, using a bandwidth of 500.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntampines_densities <- nkde(tampines_road_sf,\n                  events = tampines_origin_sf,\n                  w = rep(1, nrow(tampines_origin_sf)),\n                  samples = tampines_centers,\n                  kernel_name = \"quartic\",\n                  bw = 500,\n                  div = \"bw\",\n                  method = \"simple\",\n                  digits = 1,\n                  tol = 1,\n                  grid_shape = c(1,1),\n                  max_depth = 8, \n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\norchard_densities <- nkde(orchard_road_sf,\n                  events = orchard_origin_sf,\n                  w = rep(1, nrow(orchard_origin_sf)),\n                  samples = orchard_centers,\n                  kernel_name = \"quartic\",\n                  bw = 500,\n                  div = \"bw\",\n                  method = \"simple\",\n                  digits = 1,\n                  tol = 1,\n                  grid_shape = c(1,1),\n                  max_depth = 8, \n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n```\n:::\n\n\nFrom the code chunk above:\n\n-   w is a vector representing the weight of each event\n\n-   samples is the points representing the locations for which the densities will be estimated\n\n-   kernel_name argument indicates that quartic kernel is used\n\n    -   Other possible kernel methods supported by spNetwork : triangle, gaussian, scaled gaussian, tricube, cosine ,triweight, epanechnikov or uniform.\n\n-   method argument indicates that simple method is used to calculate the NKDE.\n\n    -   Currently, spNetwork support three popular methods:\n\n        -   method=“simple”\n\n            -   An intuitive solution: The distances between events and sampling points are replaced by network distances, and the formula of the kernel is adapted to calculate the density over a linear unit instead of an areal unit.\n\n        -   method=“discontinuous”.\n\n            -   Equally “divides” the mass density of an event at intersections of lixels.\n\n        -   method=“continuous”\n\n            -   If the “discontinuous” method is unbiased, it leads to a discontinuous kernel function which is a bit counter-intuitive.\n\n            -   This “continuous” method divides the mass of the density at intersection but adjusts the density before the intersection to make the function continuous.\n\n-   bw denotes the bandwidth used\n\n-   agg indicates if the events must be aggregated within a distance. If NULL, the events are aggregated only by rounding the coordinates\n\n### 5.3.4 Visualizing NetKDE\n\nBefore we can visualize the NetKDE values, we will use the code chunk below to insert the computed density values (i.e. densities) into centers and lixels objects as a density field.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntampines_centers$density <- tampines_densities\ntampines_lixels$density<- tampines_densities\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\norchard_centers$density <- orchard_densities\norchard_lixels$density<- orchard_densities\n```\n:::\n\n\nSince the svy21 projection system is in metres, the computed density values are very small. Hence, we rescale the density values from the number of events per metre to number of events per kilometre using the code chunk below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntampines_centers$density<- tampines_centers$density*1000\ntampines_lixels$density <- tampines_lixels$density*1000\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\norchard_centers$density<- orchard_centers$density*1000\norchard_lixels$density <- orchard_lixels$density*1000\n```\n:::\n\n\nFinally, we can prepare a high cartographic quality map of the NKDE visualization for Tampines and Orchard on openstreetmap of Singapore using the follow codes. However, I will not display the actual interactive map here, and will show only a snapshot of it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"view\")\n\ntm_shape(tampines_lixels) + \n  tm_lines(col = \"density\") +\n  tm_shape(tampines_origin_sf) +\n  tm_dots() +\n  tm_basemap(\"OpenStreetMap\")\n```\n:::\n\n\n![](images/Screenshot%202024-02-08%20at%206.18.10%20PM.png)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"view\")\n\ntm_shape(orchard_lixels) + \n  tm_lines(col = \"density\") +\n  tm_shape(orchard_origin_sf) +\n  tm_dots() +\n  tm_basemap(\"OpenStreetMap\")\n```\n:::\n\n\nFrom running the above codes, we can observe the NKDE of Grab journey origin points in the Tampines and Orchard road networks!\n\n# 6 Conclusion\n\nWe have reached the end of this exercise. From this exercise, I hope you have gained some insight into Singapore's Grab ride origins like I did. Thank you for following through and have a good day!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}