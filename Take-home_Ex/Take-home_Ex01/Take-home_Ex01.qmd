---
title: "(NOT DONE) Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore"
execute:
  eval: FALSE
  echo: FALSE
  warning: false
date: "January 23, 2024"
---

# 1 Overview

## 1.1 Background

Human mobility, the movement of human beings in space and time, reflects the spatial-temporal characteristics of human behavior. With the advancement Information and Communication Technologies (ICT) especially in smartphones, a large volume of data related to human mobility have been collected. By using appropriate GIS analysis methods, these data are potentially useful in supporting smart city planning and management.

In 2020, a very interesting human mobility data set called Grab Posisi was released by GRAB, one of the largest shared taxi operator in South-east Asia. This provides an opportunity for us to explore the geographical and spatio-temporal distribution of Grab hailing services locations in Singapore.

## 1.2 Our Objectives

In this exercise, we will be exploring the geographical and spatial-temporal distribution of Grab hailing services locations in Singapore with the use of spatial point patterns analysis techniques.

## 1.3 Our Task

-   Use appropriate functions of **sf** and **tidyverse** to prepare the following geospatial data layer in sf tibble data.frames:

    -   Grab taxi location points either by origins or destinations.

    -   Road layer within Singapore excluding outer islands.

    -   Singapore boundary layer excluding outer islands

-   Use the extracted data to derive traditional Kernel Density Estimation layers.

-   Use the extracted data to derive either Network Kernel Density Estimation (NKDE) or Temporal Network Kernel Density Estimation (TNKDE)

-   Use appropriate **tmap** functions to display the kernel density layers on openstreetmap of Singapore.

-   Describe the spatial patterns revealed by the kernel density maps.

## 1.4 Data Acquisition

To address the above questions, we would be using the following data sets:

| Type       | Content                                         | Source                                                                                                    |
|------------------|---------------------------|---------------------------|
| Geospatial | Road data set of Malaysia, Singapore and Brunei | [OpenStreetMap of Geofabrik download server](https://download.geofabrik.de/)                              |
| Geospatial | Master Plan 2019 Subzone Boundary (No Sea)      | [data.gov.sg](https://beta.data.gov.sg/collections/1749/datasets/d_8594ae9ff96d0c708bc2af633048edfb/view) |
| Aspatial   | Grab-Posisi of Singapore                        | [engineering.grab.com](https://engineering.grab.com/grab-posisi)                                          |

## 1.5 Install and Load R Packages

In this exercise, the following R packages will be used:

-   **tidyverse**: to read, manipulate and create tidy data, and to create data graphics

-   **sf**: to provide simple features access to represent and work with spatial vector data such as points and polygons

-   **spatstat**: to perform statistical analysis of spatial data

-   **raster**: to read, write, manipulate, analyze and model spatial data

-   **maptools**: tools for handling spatial objects

-   **tmap**: to create thematic and high-quality cartographic maps

-   **arrow**: improve the performance of data analysis methods, and to increase the efficiency of moving data from one system or programming language to another

-   **spNetwork**: to perform Spatial Point Patterns Analysis such as kernel density estimation (KDE) and K-function on network. It also can be used to build spatial matrices (‘listw’ objects like in ‘spdep’ package) to conduct any kind of traditional spatial analysis with spatial weights based on reticular distances.

-   **classInt**: provides a uniform interface to finding class intervals for continuous numerical variables

-   **viridis**: to create colorblind-friendly maps

-   **lubridate**: to parse and manipulate dates

-   **ggplot2**

To install and load the packages, we will use p_load() from the pacman package:

```{r}
#|eval: FALSE
pacman::p_load(tidyverse, sf, spatstat, raster, maptools, tmap, arrow, spNetwork, classInt, viridis, lubridate, ggplot2)
```

# 2 Data Preparation (Geospatial)

Let's begin getting our hands dirty by introducing and preparing the geospatial data sets in R!

## 2.1 Data Import

To import geospatial data, we will be using **st_read()** from the **sf** package.

Road data set from OSM (shapefile format):

```{r}
#|eval: FALSE
roaddata_sf <- st_read(dsn = "data/geospatial", 
                       layer = "gis_osm_roads_free_1")
```

Master Plan 2019 Subzone Boundary (No Sea) (geojson format):

```{r}
#|eval: FALSE
mpsz_sf <- st_read("data/geospatial/MasterPlan2019SubzoneBoundaryNoSeaGEOJSON.geojson")
```

## 2.2 Data Preparation

It is important to ensure our geospatial data is clean, in the correct coordinate reference system (CRS) and extracted to contain only relevant data to prevent complications later on.

In this section, we will go through the procedures to prepare our geospatial data.

### 2.2.1 Data Pre-Processing

To begin with, let's examine the data sets to understand their features.

```{r}
#|eval: FALSE
roaddata_sf
```

From the above, we can see that roaddata_sf is an sf object, with linestring geometry type and dimension XY.

We also notice that it is in WGS84 geodetic CRS, which is not our desired coordinate reference system (svy 21). Hence, we would have to reproject it later (#### INDICATE SECTION NUMBER).

```{r}
#|eval: FALSE
mpsz_sf
```

For mpsz_sf, we see that it is an sf object with multipolygon geometry type. It comprises of records with XYZ coordinates, indicating a Z-dimension, quite redundant to us.

Here, we notice that mpsz_sf has geodetic CRS of WGS84 as well. Hence, we will need to fix the CRS for mpsz_sf later (###INDICATE SECTION NUMBER) as well.

#### 2.2.1.1 Dropping Z-dimension

After having an understanding of our data sets, we will start to modify them into our desired dimensions and systems.

In this step, we will remove the Z-dimension in mpsz_sf, with the use of st_zm(). st_zm() is a function used to drop or add Z and/or M dimensions, from sf package.

```{r}
mpsz_sf <- st_zm(mpsz_sf)
```

```{r}
mpsz_sf
```

With that, we can see that the mpsz_sf has become two-dimensional (XY).

#### 2.2.1.2 Invalid Geometries

To check whether our data sets contain invalid geometries, we can apply the following code chunks:

```{r}
length(which(st_is_valid(roaddata_sf) == FALSE))
```

```{r}
length(which(st_is_valid(mpsz_sf) == FALSE))
```

We see that roaddata_sf has no invalid geometry, while mpsz_sf has 6 invalid geometries.

To correct the invalid geometries in mpsz_sf, we can use st_make_valid() from sf package,

```{r}
mpsz_sf <- st_make_valid(mpsz_sf)
```

and check confirm whether the modified mpsz_sf data set now contains fully valid geometries.

```{r}
length(which(st_is_valid(mpsz_sf) == FALSE))
```

Great! Our geographic data are now cleared of invalid geometries.

#### 2.2.1.3 Handling Missing Values

Next, we will check for missing values in our geographic data.

Let's begin with checking roaddata_sf, we can applying the following code chunk.

```{r}
roaddata_sf[rowSums(is.na(roaddata_sf)) != 0, ]
```

Wow, there is an absurd 1719007 records with missing values?!

Let's investigate what could be wrong using View().

```{r}
View(roaddata_sf)
```

From the above, it seems like it is the "ref" field comprises of so many missing values! Let's try removing it, and check again!

```{r}
roaddata_sf <- roaddata_sf %>% dplyr::select(-ref) 
```

```{r}
roaddata_sf[rowSums(is.na(roaddata_sf)) != 0, ]
```

Oh no, it appears that there are still a lot of missing values, particularly in the "name" field. However, we should not delete those records because they are not necessarily redundant: in Singapore, if a road length is less than 60m, it need not be named. Thus, these records might still represent valid roads that are shorter than 60m!

Then, ignoring the missing values in "name" field, let's check whether there are missing values in the other fields.

```{r}
roaddata_sf[rowSums(is.na(roaddata_sf %>% dplyr::select(-name))) != 0, ]
```

Phew, finally! There are no missing values in other fields. Seems like roaddata_sf is cleared of missing values that requires our attention.

Next, let's check mpsz_sf!

```{r}
mpsz_sf[rowSums(is.na(mpsz_sf)) != 0, ]
```

Thankfully, there's no missing values in mpsz_sf!

Hurray we're done with resolving the missing values\~!

### 2.2.2 Verifying and Transforming CRS

To check the CRS of the data sets, we can use st_crs() from sf package.

```{r}
st_crs(roaddata_sf)
```

```{r}
st_crs(mpsz_sf)
```

From the above, and as also noticed earlier in Section 2.2.1, the data sets are in the WGS84 CRS. However, in Singapore, we should use the SVY21 CRS (with EPSG code: 3414) as it is more appropriate for our analysis.

To change the CRS of the data sets, we can use st_transform() from sf package, inputting the EPSG code for SVY21 (3414) as the second argument of the function.

```{r}
roaddata_sf <- st_transform(roaddata_sf, 3414)
```

```{r}
mpsz_sf <- st_transform(mpsz_sf, 3414)
```

Then, let's confirm that the CRS for the data sets have been correctly modified.

```{r}
st_crs(roaddata_sf)
```

```{r}
st_crs(mpsz_sf)
```

Hooray! Our geospatial data are now in the correct CRS!

### 2.2.3 Extraction of relevant data

After doing some cleaning of our geospatial data, let's roughly visualize how they look like:

```{r}
tmap_mode("plot")

tm_shape(roaddata_sf) + 
  tm_lines()
```

As expected, roaddata_sf contains the visualization of Malaysia, Singapore and Brunei.

```{r}
tm_shape(mpsz_sf) +
  tm_polygons()
```

And for mpsz_sf, since it is supposed to contain data of Singapore's territories, the visualization displays the map for only Singapore.

However, in this exercise, we are interested in the data that includes only Singapore without its outer islands, we will have to remove all outer islands outside of Singapore mainland. Hence, we need to do some manipulation to remove the outer islands.

To begin with, we will remove the outer islands from mpsz_sf first using str_detect() from the **stringr** package and filter() from the **dplyr** package, and name the new data frame mpsz_sgsf

```{r}
mpsz_sgsf <- mpsz_sf %>% filter(!str_detect(Description, "ISLAND"))
```

Let's make a quick plot using qtm() from the **tmap** package to check if we've successfully extracted data of Singapore's data excluding outer islands:

```{r}
tmap_mode("plot")
qtm(mpsz_sgsf)
```

Then, to get Singapore's boundary layer without outer islands,

```{r}
sgboundary_sf <- mpsz_sgsf %>% st_union
```

Let's check:

```{r}
qtm(sgboundary_sf)
```

Yay! We've gotten the Singapore boundary layer that excludes the outer islands!

Next, to derive the road layers that lie within Singapore, we can use st_intersection() from the **sf** package. Here, we shall form a new data set mpszroadsg_sf which should contain geospatial data with road layers within Singapore, without its outer islands.

```{r}
roadsg_sf <- st_intersection(sgboundary_sf, roaddata_sf)
```

Now, let's check that roadsg_sf contains the road layer only within Singapore by plotting a map!

```{r}
qtm(roadsg_sf)
```

True enough, this should be how the road system of Singapore, without its outer islands, looks like. YAY!

# 3 Data Preparation (Aspatial)

Now, it's time to introduce our aspatial data set!

## 3.1 Importing Aspatial Data

To import aspatial data, we will be using **read_parquet()** from arrow package.

```{r}
#|eval: FALSE
grabposisi0 <- read_parquet("data/aspatial/part-00000-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet")
grabposisi1 <- read_parquet("data/aspatial/part-00001-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet")
grabposisi2 <- read_parquet("data/aspatial/part-00002-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet")
grabposisi3 <- read_parquet("data/aspatial/part-00003-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet")
grabposisi4 <- read_parquet("data/aspatial/part-00004-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet")
grabposisi5 <- read_parquet("data/aspatial/part-00005-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet")
grabposisi6 <- read_parquet("data/aspatial/part-00006-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet")
grabposisi7 <- read_parquet("data/aspatial/part-00007-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet")
grabposisi8 <- read_parquet("data/aspatial/part-00008-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet")
grabposisi9 <- read_parquet("data/aspatial/part-00009-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet")
```

Let's take a look at the data frames using glimpse() from

```{r}
#|eval: FALSE
glimpse(grabposisi0)
glimpse(grabposisi1)
glimpse(grabposisi2)
glimpse(grabposisi3)
glimpse(grabposisi4)
glimpse(grabposisi5)
glimpse(grabposisi6)
glimpse(grabposisi7)
glimpse(grabposisi8)
glimpse(grabposisi9)
```

We see that each of the data frames is a tibble data frame, with nine columns, with the same column headers.

To combine all of them into one single data frame named grabposisi, we can use rbind().

```{r}
grabposisi <- rbind(grabposisi0, grabposisi1, grabposisi2, grabposisi3, grabposisi4, grabposisi5, grabposisi6, grabposisi7, grabposisi8, grabposisi9)
```

Tadah! Our grabposisi data sets are consolidated and ready for further preparation.

## 3.1 Data Preparation

Next, let's examine our new consolidated grabposisi data set.

```{r}
glimpse(grabposisi)
```

### 3.1.1 Changing Data Types

We see that the pingtimestamp field is in the integer format, when it is a datetime type of data. To convert it into datetime format, we can use the following code:

```{r}
grabposisi$pingtimestamp <- as_datetime(grabposisi$pingtimestamp)
```

Now, it is in a more appropriate for interpretation and manipulation later!

### 3.1.2 Conversion into sf tibble data frame

Next, we should combined the rawlng and rawlat fields grabposisi as coordinates format, so that we can transform them into geometry format as an sf tibble data frame to combine with our geospatial data.

To do so, we should apply the following code:

Then, we will use the code below to convert the grabposisi data into sf tibble data frame with svy21 projection system.

```{r}
grabposisi_sf <- st_as_sf(grabposisi, coords = c("rawlng", "rawlat"), crs = 3414)
```

```{r}
grabposisi_sf
```

### 3.1.3 Verifying and Transforming CRS

Before we proceed further, let's ensure grabposisi_sf is in the correct CRS.

```{r}
st_crs(grabposisi_sf)
```

It's in svy21 projection system, we're good to move on!

### 3.1.4 Extracting Study Data

Considering the fact that we are only interested in the Grab taxi location points by origins in this study, we can apply the following code:

```{r}
origin_grab_sf <- grabposisi_sf %>%
  group_by(trj_id) %>%
  arrange(pingtimestamp) %>%
  filter(row_number() == 1) %>%
  mutate(weekday = wday(pingtimestamp, 
                        label = TRUE,
                        abbr = TRUE),
         start_hr = factor(hour(pingtimestamp)),
         day = factor(mday(pingtimestamp)))
```

The code might look a bit complicating, but let's go through this step by step through the pipelines:

-   Firstly, we use group_by() from dplyr to group the records according to trajectory IDs, so that records of the same journey are together.

-   Then, we use arrange() from dplyr to arrange the records according to time in ascending order. With that, the first record of each group would be the earliest record of the journey, inferring that it is the origin point.

-   Next, we use filter() from dplyr to filter each group, keeping only their first row. This allows us to keep only the records relating to the origin points of each journey.

-   Lastly, we use mutate() from dplyr, along with wday(), hour() and mday() from lubridate to retrieve the weekdays, start_hr, and day of the week to support our analysis later.

Tadah! We have successfully extracted the Grab taxi location points by origin.

```{r}
glimpse(origin_grab_sf)
```

# 4 Joining Geospatial and Aspatial Data (TO KIV)

Now, we will join our geospatial and aspatial data to conduct further analysis in the later sections!

```{r}
originroadsg_sf <- st_union(mpsz_road_sf, origin_grab_sf)
```

```{r}
originroadsg_sf <- st_join(mpszroadsg_sf, origin_grab_sf)
```

```{r}
tm_shape(originroadsg_sf) +
  tm_polygons() + 
  tm_lines() + 
  tm_dots()
```

```{r}
grab_mpsz_road_sf
```

```{r}
tm_shape(grabmpszroad_sf)
```

# 5 Exploratory Data Analysis (EDA) and Choropleth Mapping

In this section, we will conduct some EDA on our aspatial data to gain some rough insights to lead us into

Let's have a look at how our combined data looks like!

Some questions we might be curious about are: - Which are the peak days of the week to take a Grab ride? - When are the peak times throughout the day to hitch a Grab ride? - Where do many of the Grab rides begin? Are they evenly distributed across Singapore? - If so, when are the peak times and days wh

## 5.1 Days of the Week

To look at the peak days, we can plot a bar chart using ggplot2.

```{r}
ggplot(origin_grab_sf) + 
  geom_bar(aes(x=weekday))
```

## 5.2 Peak Times

```{r}
hours_of_day <- 0:23
ggplot(origin_grab_sf, aes(x= start_hr)) +
 geom_histogram()
```

Let's have a look at how the origin points look like on the Singapore map.

```{r}
tm_shape(origin_grab_sf) +
  tm_dots()
```

## 5.3 Mapping the geospatial data sets

Next, it is also useful for us to create a pin map to show the spatial patterns of our data.

```{r}
tmap_mode("view")
tm_shape(mpsz_sf) + tm_polygons() +
  tm_shape(roaddata_sf) + tm_lines() + 
  tm_shape(origin_grab_sf) + tm_dots()
```

In this interactive mode, we can navigate and zoom around the map freely. Also, we can query the information of each simple feature (i.e the point) by clicking on it.

Thereafter, we switch back to plot mode as the interactive mode will consume a connection.

```{r}
tmap_mode("plot")
```

# 6 First-order Spatial Point Patterns Analysis (SPPA)

## 6.1 Geospatial Data Wrangling for First-order SPPA

To conduct first-order spatial point analysis on our data, we be using the **spatstat** package. However, the spatstat package requires our data to be in sp's Spatial classes. In this section, we will convert the sf data frames to sp's Spatial class.

### 6.1.1 Converting sf data frames to sp's Spatial class

To do so, we would use as_Spatial() from the **sf** package to convert our three geospatial data from sf data frame to sp's Spatial class.

```{r}
sgboundary <- as_Spatial(sgboundary_sf)
```

```{r}
roadsg <- as_Spatial(roadsg_sf)
origin<- as_Spatial(origin_grab_sf)
```

To look into the information of the three new Spatial classes,

```{r}
sgboundary
```

```{r}
roadsg
```

```{r}
origin
```

### 6.1.2 Converting the Spatial class into generic sp format

spatstat requires the analytical sata to be in ppp object form. However, as there is no direct method to convert Spatial classes into ppp objects, we would have to convert the Spatial classes into Spatial objects first.

```{r}
sgboundary_sp <- as(sgboundary, "SpatialPolygons")
```

```{r}
roadsg_sp <- as(roadsg, "SpatialLines")
```

```{r}
origin_sp <- as(origin, "SpatialPoints")
```

### 6.1.3 Converting the generic sp format into spatstat's ppp format

```{r}
origin_ppp <- as(origin_sp, "ppp")
```

```{r}
origin_ppp <- as.ppp(origin_grab_sf)
```

Here, we can plot origin_grab_ppp to examine the difference.

```{r}
plot(origin_ppp)
```

For a quick understanding of the summary statistics of the newly created ppp object, we can apply the summary() from base R.

```{r}
summary(origin_ppp)
```

### 6.1.4 Handling duplicated points

In this case, given the nature of our geospatial data (origin points of journeys), we can expect several duplicated points on the map. However,

We can check for duplication in a ppp object using the following code chunk:

```{r}
any(duplicated(origin_ppp))
```

To find out the number of coincident points, we use multiplicity() from the **spatstat** package.

```{r}
multiplicity(origin_ppp)
```

To find out how many locations have more than one point event, we can use the following code:

```{r}
sum(multiplicity(origin_ppp))
```

To view the location of the duplicate point events, we can plot origin_grab data with the following code chunk:

```{r}
tmap_mode("plot")
tm_shape(origin) + 
  tm_dots(alpha = 0.4, 
          size = 0.05)
```

From the graph above, we can spot the duplicate point events as the duplicate points overlap, resulting in darker points in the map.

To overcome the issue of duplicate points, we can adopt any of the following 3 solutions:

1.  Delete the duplicates (however, this would result in the loss of some useful point events)

2.  Jittering: To add a small perturbation to the duplicate points, so that they do not occupy the exact same space

3.  Make each point "unique", and attach the duplicates of the points to the patterns as **marks**, as attributes to the points. Then, we would require analytical techniques that take into account these marks.

Given the nature of this data set, where there could be many duplicated origin points, wherein these duplicates are also meaningful, we will apply the 2nd solution -- jittering.

```{r}
origin_grab_jit <- rjitter(origin_ppp,
                           retry = TRUE,
                           nsim = 1,
                           drop = TRUE)
```

### 6.1.5 Creating owin object

When analyzing spatial potterns, it is good practice to confine our analysis within a geograhical area. In **spatstat**, an object called owin is designed to represent this polygonal region.

The following code chunk is used to convert sgboundary_sf into owin object of **spatstat**.

```{r}
sgboundary_owin <- as.owin(sgboundary_sf)
```

The output object can be displayed by using the plot() function,

```{r}
plot(sgboundary_owin)
```

and summary() function of BaseR.

```{r}
summary(sgboundary_owin)
```

### 6.1.6 Combining point events object and owin object

In this last step of geospatial data wrangling, we will extract origin locations that are within Singapore using the following code chunk:

```{r}
originSG_ppp <- origin_ppp[sgboundary_owin]
```

The output object combined both the point and polygon feature in one ppp object class as shown below.

```{r}
summary(originSG_ppp)
```

To plot the newly derived originSG_ppp map,

```{r}
plot(originSG_ppp)
```

## 6.2 Deriving Traditional Kernel Density Estimation (KDE) Layers

In this section, we will be performing first-order SPPA using the **spatstat** package. In particular, we will be deriving the kernel density estimation (KDE) layer for visualizing and exploring the intensity of point processes.

### 6.2.1 Computing KDE using automatic bandwidth selection method

The code chunk below computed a Kernel Density byy using the following configurations of density() of **spatstat**.

Automatic bandwidth selection method: bw.ppl() - other methods: bw.CvL(), bw.scott() or bw.diggle()

Smoothing kernel: "gaussian" - other methods: "epanechnikov", "quartic" or "disc"

The intensity estimate is corrected for edge effect bias by using edge = TRUE.

```{r}
kde_originSG_bwppl <- density(originSG_ppp,
                           sigma = bw.ppl,
                           edge = TRUE,
                           kernel = "gaussian")
```

Then, we will plot the derived kernel density.

```{r}
plot(kde_originSG_bwppl)
```

##### The density values of the output range

To retrive the bandwidth used to compute the KDE layer, we use the following code chunk:

```{r}
bw <- bw.ppl(originSG_ppp)
```

### 6.2.2 Rescaling KDE values

In the following code chunk, rescale() is used to convert the unit of measurement from meter to kilometer.

```{r}
originSG_ppp.km <- rescale(originSG_ppp, 1000, km)
```

Now, we can re-run density() using the rescaled data set and plot the output KDE map.

```{r}
kde_originSG_bwppl.km <- density(originSG_ppp.km,
                              sigma = bw.ppl,
                              edge = TRUE, 
                              kernel = "gaussian")
plot(kde_originSG_bwppl.km)
```

Now, we can see that the output image looks identical to the earlier version, but with more intepretable data values in the legend.

### 6.2.3 Comparing different automatic bandwidth methods

Aside from bw.diggle, as mentioned before, there are three other **spatstat** functions (bw.CvL(), bw.scott(), bw.ppl()) that can be used to determine the bandwidth automatically.

Let's take a look at the bandwidth used by each of these automatic bandwidth calculation methods, keeping all our kernel method ("gaussian") constant!

```{r}
bw.CvL(originSG_ppp.km)
```

```{r}
bw.scott(originSG_ppp.km)
```

```{r}
bw.diggle(originSG_ppp.km)
```

The following code chunk is used to compare the difference in output using the different automatic bandwidth methods.

```{r}
kde_originSG_bwCvL <- density(originSG_ppp.km,
                              sigma = bw.CvL,
                              edge = TRUE,
                              kernel = "gaussian")
kde_originSG_bwscott <- density(originSG_ppp.km,
                                sigma = bw.scott,
                                edge = TRUE,
                                kernel = "gaussian")
kde_originSG_bwdiggle <- density(originSG_ppp.km,
                                 sigma = bw.diggle,
                                 edge = TRUE, 
                                 kernel = "gaussian")

par(mrow = c(2,2))
plot(kde_originSG_bwppl.km, main = "bw.ppl")
plot(kde_originSG_bwCvL, main = "bw.CvL")
plot(kde_originSG_bwscott, main = "bw.scott")
plot(kde_originSG_bwdiggle, main = "bw.diggle")
```

### 6.2.4 Comparing different kernel methods

By default, the kernel method used in density() is Gaussian. However, there are three other available method: Epanechnikov, Quartic and Dics

In the following code chunk, we will compare the different kernel methods, using originSG_ppp.km and automatic bandwidth method bw.ppl.

```{r}
par(mfrow=c(2,2))
tmap_mode("view")

qtm(density(originSG_ppp.km,
             sigma = bw.ppl,
             edge = TRUE,
             kernel = "gaussian"),
     title = "gaussian")
qtm(density(originSG_ppp.km,
             sigma = bw.ppl,
             edge = TRUE,
             kernel = "epanechnikov"),
     title = "Epanechnikov")
qtm(density(originSG_ppp.km,
             sigma = bw.ppl,
             edge = TRUE,
             kernel = "quartic"),
     title = "Quartic")
qtm(density(originSG_ppp.km,
             sigma = bw.ppl,
             edge = TRUE,
             kernel = "disc"),
     title = "Disc")
```

### 6.2.5 Fixed and Adaptive KDE

#### 6.2.5.1 Computing KDE by using fixed bandwidth (not sure whethet to do)

Here, we will compute a KDE later by defining a bandwidth of

#### 6.2.5.2 Computing KDE by using adaptive bandwidth

Here, we will derive adaptive KDE using density.adaptive() of **spatstat**.

```{r}
kde_originSG_adaptive <- adaptive.density(originSG_ppp.km,
                                          method = "kernel")
plot(kde_originSG_adaptive)
```

### 6.2.6 Converting KDE output into grid object

```{r}
gridded_kde_originSG_bwppl <- as.SpatialGridDataFrame.im(kde_originSG_bwppl.km)
spplot(gridded_kde_originSG_bwppl)
```

### 6.2.7 Converting gridded output into raster

Next, we will convert the gridded kernel denstiy objects into RasterLayer object using raster() of **raster** package.

```{r}
kde_originSG_bwppl_raster <- raster(gridded_kde_originSG_bwppl)
```

We can look at the properties of this new kde_originSG_bwppl_raster RasterLayer object using the following code chunk:

```{r}
kde_originSG_bwppl_raster
```

### 6.2.8 Assigning projection systems (if necessary)

### 6.2.9 Visualizing the output in tmap

Finally, we will display the raster on openstreetmap of Singapore.

```{r}
tmap_mode("view")
tm_shape(kde_originSG_bwppl_raster) +
  tm_raster("v") + 
  tm_layout(legend.position = c("right", "bottom"),
            frame = FALSE)
```

## 6.3 Deriving Network Kernel Density Estimation (NetKDE) Layers

### 6.3.1 Preparing the lixels objects

Before computing NetKDE, the roadsg_sp SpatialLines object need to be cut into lixels with a specified minimal distance.

To do so, we can use lixelize_lines() from **spNetwork** package.

```{r}
lixels <- lixelize_lines(roadsg_sf,
                         500,
                         mindist = 150)
```

In the code chunk above, we have set:

length of lixel, lx_length = 500m

minimum length of lixel, mindist = 200m

Note: After the cut, if the length of the final lixel is shorter than the minimum distance, it would be added to the previous lixel. On the other hand, if mindist is NULL, then mindist = maxdist/10. Segments that are already shorter than the minimum distance are not modified.

### 6.3.2 Generating line centre points

Next, lines_center() of spNetwork will be used to generate a SpatialPointsDataFrame with line centre points as shown below:

```{r}
centers <- lines_center(lixels)
```

### 6.3.3 Performing NetKDE

Next, we will compute the NetKDE using the code chunk below.

```{r}
densities <- nkde(roadsg_sf,
                  events = origin_grab_sf,
                  w = rep(1, nrow(origin_grab_sf)),
                  samples = centers,
                  kernel_name = "quartic",
                  bw = 100,
                  div = "bw",
                  method = "simple",
                  digits = 1,
                  tol = 1,
                  grid_shape = c(1,1),
                  max_depth = 8, 
                  agg = 5,
                  sparse = TRUE,
                  verbose = FALSE)
```

From the code chunk above:

-   kernel_name argument indicates that quartic kernel is used

    -   Other possible kernel methods supported by spNetwork : triangle, gaussian, scaled gaussian, tricube, cosine ,triweight, epanechnikov or uniform.

-   method argument indicates that simple method is used to calculate the NKDE.

    -   Currently, spNetwork support three popular methods:

        -   method=“simple”

            -   An intuitive solution: The distances between events and sampling points are replaced by network distances, and the formula of the kernel is adapted to calculate the density over a linear unit instead of an areal unit.

        -   method=“discontinuous”.

            -   Equally “divides” the mass density of an event at intersections of lixels.

        -   method=“continuous”

            -   If the “discontinuous” method is unbiased, it leads to a discontinuous kernel function which is a bit counter-intuitive.

            -   This “continuous” method divides the mass of the density at intersection but adjusts the density before the intersection to make the function continuous.

### 6.3.4 Visualizing NetKDE

Before we can visualize the NetKDE values, we will use the code chunk below to insert the computed density values (i.e. densities) into centers and lixels objects as a density field.

```{r}
centers$density <- densities
lixels$density <- densities
```

Since the svy21 projection system is in metres, the computed density values are very small. Hence, we rescale the density values from the number of events per metre to number of events per kilometre using the code chunk below.

```{r}
centers$density <- centers$density*1000
lixels$density <- lixels$density*1000
```

Then, we can prepare an interactive and high cartographic quality map visualization on openstreetmap of Singapore using the follow code:

```{r}
tmap_mode("view")

tm_shape(lixels) + 
  tm_lines(col = "density") +
  tm_shape(origin_grab_sf) +
  tm_dots()
```


