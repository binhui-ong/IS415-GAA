[
  {
    "objectID": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "href": "In-class_Ex/In-class_Ex02/In-class_Ex02.html",
    "title": "In-class Exercise 2: R for Geospatial Analytics",
    "section": "",
    "text": "2.1 Getting Started\nTo begin with, we will use p_load() from pacman package to load the following packages:\n\narrow,\nlubridate,\ntidyverse,\ntmap, and\nsf.\n\n\n\nCode\npacman::p_load(arrow, lubridate, tidyverse, tmap, sf)\n\n\n\n\n2.2 Importing Grab-Posisi Dataset\nImport the parquet file into R and name the data frame as df:\n\n\nCode\ndf &lt;- read_parquet(\"data/GrabPosisi/part-00008-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\n\n\nTo convert pingtimestamp field from integer to datetime data format:\n\n\nCode\ndf$pingtimestamp &lt;- as_datetime(df$pingtimestamp)\n\n\nSave the newly reformatted df data frame as rds for future use:\n\n\nCode\nwrite_rds(df, \"data/rds/part0.rds\")\n\n\nExtracting trip starting locations\n\n\nCode\norigin_df &lt;- df %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(pingtimestamp) %&gt;%\n  filter(row_number() == 1) %&gt;%\n  mutate(weekday = wday(pingtimestamp,\n                         label = TRUE,\n                         abbr = TRUE),\n         start_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\nExtracting trip destination locations\n\n\nCode\ndestination_df &lt;- df %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(desc(pingtimestamp)) %&gt;%\n  filter(row_number() == 1) %&gt;%\n  mutate(weekday = wday(pingtimestamp,\n                         label = TRUE,\n                         abbr = TRUE),\n         end_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\n\nSave the newly created origin_df and destination_df dataframes as rds files for future use:\n\n\nCode\nwrite_rds(origin_df,\"data/rds/origin_df.rds\")\nwrite_rds(destination_df, \"data/rds/destination_df.rds\")\n\n\nTo import the objects the next time I reopen the R project,\n\n\nCode\norigin_df &lt;- read_rds(\"data/rds/origin_df.rds\")\ndestination_df &lt;- read_rds(\"data/rds/destination_df.rds\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#importing-geospatial-data",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#importing-geospatial-data",
    "title": "In-class Exercise 4: Spatial Weights and Applications",
    "section": "1.2.1 Importing Geospatial Data",
    "text": "1.2.1 Importing Geospatial Data\n\nhunan &lt;- st_read(dsn=\"data/geospatial\",\n                 layer = \"Hunan\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#importing-aspatial-data",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#importing-aspatial-data",
    "title": "In-class Exercise 4: Spatial Weights and Applications",
    "section": "1.2.2 Importing Aspatial Data",
    "text": "1.2.2 Importing Aspatial Data\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#joining-geospatial-and-aspatial-data",
    "href": "In-class_Ex/In-class_Ex04/In-class_Ex04.html#joining-geospatial-and-aspatial-data",
    "title": "In-class Exercise 4: Spatial Weights and Applications",
    "section": "1.2.3 Joining Geospatial and Aspatial Data",
    "text": "1.2.3 Joining Geospatial and Aspatial Data\n\nhunan &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/data/geospatial/TAINAN_VILLAGE.html",
    "href": "Take-home_Ex/Take-home_Ex02/data/geospatial/TAINAN_VILLAGE.html",
    "title": "Bin Hui's IS415-GAA Journal",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“TWD97”,DATUM[“Taiwan Datum 1997”,ELLIPSOID[“GRS 1980”,6378137,298.257222101,LENGTHUNIT[“metre”,1]]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“Taiwan, Republic of China - onshore and offshore - Taiwan Island, Penghu (Pescadores) Islands.”],BBOX[17.36,114.32,26.96,123.61]],ID[“EPSG”,3824]] +proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs 27230 3824 EPSG:3824 TWD97 longlat EPSG:7019 true"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02_2.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02_2.html",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 2)",
    "section": "",
    "text": "This webpage is a continuation of Take-home Exercise 2 (Part 1), which can be found here.\nAdditional reference link for Section 7.3:\nMoving onto our next steps…"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02_2.html#computing-lisa",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02_2.html#computing-lisa",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 2)",
    "section": "6.1: Computing LISA",
    "text": "6.1: Computing LISA\nThe first step in performing the local spatial autocorrelation analysis is deriving the Local Indicator of Spatial Association (LISA). The LISA for each observation gives an indication of the extent of significant spatial clustering of similar values around that observation.\nTo compute LISA, we can apply the following code.\n\n\nCode\nlisa &lt;- wm_q %&gt;% mutate(local_moran = local_moran(\n  total_cases, nb, wt, nsim = 79),\n  .before = 1) %&gt;%\n  unnest(local_moran)\n\n\nComponents of code:\n\nmutate() from dplyr package to create a new column consisting of local_moran’s I statistics\n\nbased on\n\nvariable: total cases\nnumber of simulations (79 + 1) = 80\n\n\nunnest() from base R to expand the list-coumn containing the resulting data frame of local moran’s I statistics into columns\n\nOutput of code (columns):\n\nii: local moran statistic\neii: expectation of local moran statistic; for localmoran_permthe permutation sample means\nvar_ii: variance of local moran statistic; for localmoran_permthe permutation sample standard deviations\nz_ii: standard deviate of local moran statistic; for localmoran_perm based on permutation sample means and standard deviations\np_ii: p-value of local moran statistic using pnorm(); for localmoran_perm using standard deviatse based on permutation sample means and standard deviations\np_ii_sim: For localmoran_perm(), rank() and punif() of observed statistic rank for [0, 1] p-values using alternative hypothesis\np_folded_sim: the simulation folded [0, 0.5] range ranked p-value (based on https://github.com/pysal/esda/blob/4a63e0b5df1e754b17b5f1205b cadcbecc5e061/esda/crand.py#L211-L213)\nskewness: For localmoran_perm, the output of e1071::skewness() for the permutation samples underlying the standard deviates\nkurtosis: For localmoran_perm, the output of e1071::kurtosis() for the permutation samples underlying the standard deviates."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02_2.html#visualizing-local-morans-i",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02_2.html#visualizing-local-morans-i",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 2)",
    "section": "6.2: Visualizing local Moran’s I",
    "text": "6.2: Visualizing local Moran’s I\nThen, we can create a choropleth map using tmap functions to visualize the local moran statistic (“ii”) of different villages.\n\n\nCode\ntm_shape(lisa) + \n  tm_fill(\"ii\") + \n  tm_borders(alpha=0.5) +\n  tm_layout(main.title = \"Local Moran's I of Total Cases\", \n            main.title.size = 0.8)\n\n\n\nInterpreting Local Moran’s I (“ii”):\n\nPositive: Cluster (village is associated with relatively high values of surrounding villages)\n\nFrom the choropleth map, we can see quite a number of clusters in the Tainan City, especially more in the northern regions.\n\nNegative: Outlier (village is associated with relatively low values of surrounding villages)\n\nFrom the choropleth map, we can see a small outlier (orange) village that stands out.\n\n\nBut how significant are these clusters and outliers? To discover that, we will have to check their p-values, in the next step!"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02_2.html#visualizing-p-value-of-locals-moran-i",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02_2.html#visualizing-p-value-of-locals-moran-i",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 2)",
    "section": "6.3: Visualizing p-value of local’s Moran I",
    "text": "6.3: Visualizing p-value of local’s Moran I\nHere, we prepare a choropleth map using tmap functions to visualize the p-value of local moran statistic (“p_ii_sim”). of the different villages, to detect which villages are significant clusters/outliers.\n\n\nCode\ntm_shape(lisa) +\n  tm_fill(\"p_ii_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not significant\")) +\n  tm_borders(alpha = 0.5) +\n  tm_layout (main.title = \"p-value of local Moran's I\",\n             main.title.size = 0.8)\n\n\n\nFrom the choropleth map of p-values, we can see that many parts (more than half) of clusters and outliers discovered by the local moran’s I statistics are not significant at 95% confidence level. However, there are still a larger number of villages, particularly those in the northern and eastern regions, that exhibit significant clustering/outliers at 95% confidence level."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02_2.html#visualizing-local-morans-i-and-p-value",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02_2.html#visualizing-local-morans-i-and-p-value",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 2)",
    "section": "6.4: Visualizing local moran’s I and p-value",
    "text": "6.4: Visualizing local moran’s I and p-value\nPutting the local moran’s I value and p-value choropleth maps side by side allows to interpret out results more easily, tracing which villages are significant outliers or significant clusters on the map. To do so, we can use tmap_arrange() from tmap package to put the maps together, as demonstrated in the code chunk below.\n\n\nCode\nlocal_mapi &lt;- tm_shape(lisa) + \n  tm_fill(\"ii\") + \n  tm_borders(alpha=0.5) +\n  tm_layout(main.title = \"Local Moran's I of Total Cases\", \n            main.title.size = 0.8)\n\nlocal_mapp &lt;- tm_shape(lisa) +\n  tm_fill(\"p_ii_sim\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n          labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not significant\")) +\n  tm_borders(alpha = 0.5) +\n  tm_layout (main.title = \"p-value of local Moran's I\",\n             main.title.size = 0.8\n\ntmap_arrange(local_mapi, local_mapp, ncol = 2)\n\n\n\nAn obvious group of regions that exhibit significant cluster patterns (at 95% confidence level) are the north-western regions of Tainan."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02_2.html#visualizing-lisa-map",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02_2.html#visualizing-lisa-map",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 2)",
    "section": "6.5 Visualizing LISA map",
    "text": "6.5 Visualizing LISA map\nLISA map is a categorical map showing outliers and clusters. There are two types of outliers namely: High-Low and Low-High outliers. Likewise, there are two type of clusters namely: High-High and Low-Low clusters. In fact, LISA map is an interpreted map by combining local Moran’s I of geographical areas and their respective p-values.\nIn lisa sf data.frame, we can find three fields contain the LISA categories. They are mean, median and pysal. In general, classification in mean will be used as shown in the code chunk below, highlighting only villages that exhibit significant clustering/outliers at 95% confidence level.\n\n\nCode\nlisa_sig &lt;- lisa %&gt;% filter(p_ii &lt;0.05)\n\ntm_shape(lisa) + \n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_shape(lisa_sig) +\n  tm_fill(\"mean\") +\n  tm_borders(alpha=0.4)\n\n\n\nThe choropleth map tells us that :\n\nThe north-western and north-eastern regions (in purple) of significant clusters make up of “low-low” clusters, forming clusters of low numbers of dengue cases.\nOn the other hand, some villages in the middle regions (red) make up significant clusters of “high-high” values, implying that they are clusters of high numbers of dengue cases.\nThere are a few villages of “low-high” outliers (purple), showing that they are villages with low numbers of dengue cases, surrounded by villages with higher numbers of dengue cases."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02_2.html#preparing-spacetime-data",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02_2.html#preparing-spacetime-data",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 2)",
    "section": "7.1 Preparing spacetime data",
    "text": "7.1 Preparing spacetime data\n\n7.1.1 Data manipulation\nTo begin with, let’s combine our dengue observations based on epidemiology weeks. To do so, we can use group_by() and summarize() from dplyr package, and epiweek() from lubridate package.\n\n\nCode\ndengue_epi_sf &lt;- dengue_sf %&gt;% \n  group_by(VILLENG, TOWNID, epiweek(發病日)) %&gt;% \n  summarize(cases=n())\n\n\nIn the above code,\n\ngroup_by() groups our records according to the fields in the arguments, in ascending order by default, and in order starting with the first field indicated\nsummarize() creates a new field, returning one row of aggregated value for each combination of grouped variables\nepiweek() extrac\n\nTo build a space-time cube later, we can only define one field as the location field. Hence, we will need to create a new location field which is composed of VILLENG and TOWNID, separated by a comma. A new column can be created using mutate() of dplyr package, and the VILLENG and TOWNID fields can be combined using paste() from base R.\n\n\nCode\ndengue_epi_sf &lt;- dengue_epi_sf %&gt;% mutate(location = paste(VILLENG, \",\", TOWNID))\n\n\nOne of the conditions of building a spacetime cube is that given a number of spatial features (which in this case, is our locations) n, and time periods m, a spatio-temporal full grid must have n X m rows. However, as our existing data only captures actual dengue cases that are recorded, days with 0 cases are not recorded. Therefore, we must insert the records where there are 0 cases by ourselves.\nTo do so, we can apply the following steps:\n\nStep 1: Create a full grid\nApplying the code below creates a tibble table that contains our desired complete n x m records.\n\n\nCode\ntaiwanstudy_sf &lt;- readRDS(\"/Users/binhui-ong/IS415-GAA/Take-home_Ex/Take-home_Ex02/data/rds/taiwanstudy_sf.rds\")\n\n\n\n\nCode\ndengue_full_sf &lt;- tibble(VILLENG = rep(taiwanstudy_sf$VILLENG, 20), \n                         TOWNID = rep(taiwanstudy_sf$TOWNID, 20),\n                         geometry = rep(taiwanstudy_sf$geometry, 20),\n                        `epiweek(發病日)` = rep(c(31:50), each = 258)) %&gt;%\n  mutate(location = paste(VILLENG, \",\" , TOWNID))\n\n\nIn the above code:\n\nTo fill in values for each column (i.e. VILLENG, TOWNID, geometry and epiweek(發病日)), we can use rep(), a Base R function.\n\nFor VILLENG, TOWNID and geometry fields: It replicates the set (vector) of values input in the first argument, for as many times as stated in the second argument throughout the records.\nFor epiweek(發病日) field: It replicates each individual value in the vector in the first argument for the number of times indicated in the “each” argument, before replicating the next value in the vector the same way, functioning this way throughout all the records.\n\n\n\n\nStep 2: Join the existing case records to the full grid\nHere, we use left_join() from the dplyr package to insert our dengue case records in dengue_epi_sf into dengue_full_sf.\n\n\nCode\ndengue_full_sf &lt;- dengue_full_sf %&gt;% left_join(dengue_epi_sf, join_by(VILLENG, TOWNID, `epiweek(發病日)`))  \n\n\nSince we do not need the geometry.y column that is redundant and contains missing values, we can remove it using select() from dplyr, and rename geometry.x as geometry since it is our geometry column.\n\n\nCode\ndengue_full_sf &lt;- dengue_full_sf %&gt;% select(-geometry.y, -location.y) %&gt;% rename(geometry = geometry.x, location = location.x)\n\n\n\n\nCode\ndengue_full_sf %&gt;% dplyr::arrange(VILLENG)\n\n\n# A tibble: 5,160 × 6\n   VILLENG     TOWNID                  geometry `epiweek(發病日)` location cases\n   &lt;chr&gt;       &lt;chr&gt;              &lt;POLYGON [°]&gt;             &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;\n 1 Andong Vil. D06    ((120.2164 23.04019, 120…                31 Andong …     1\n 2 Andong Vil. D06    ((120.2164 23.04019, 120…                32 Andong …    NA\n 3 Andong Vil. D06    ((120.2164 23.04019, 120…                33 Andong …     2\n 4 Andong Vil. D06    ((120.2164 23.04019, 120…                34 Andong …    NA\n 5 Andong Vil. D06    ((120.2164 23.04019, 120…                35 Andong …     1\n 6 Andong Vil. D06    ((120.2164 23.04019, 120…                36 Andong …     2\n 7 Andong Vil. D06    ((120.2164 23.04019, 120…                37 Andong …     1\n 8 Andong Vil. D06    ((120.2164 23.04019, 120…                38 Andong …     5\n 9 Andong Vil. D06    ((120.2164 23.04019, 120…                39 Andong …     7\n10 Andong Vil. D06    ((120.2164 23.04019, 120…                40 Andong …    14\n# ℹ 5,150 more rows\n\n\nIn the above code, I have displayed the records by alphabetical order of VILLNAMEs, for easier visualization here.\nWe can see that after the data wrangling, we have 5160 records, which corresponds to our 258 villages x 20 epidemiology weeks of observations. Looking into the table, there are some records with NA cases, which represent 0 cases for those corresponding weeks and villages.\n\n\nStep 3: Insert “0” values\nNext, we should replace the NA values with 0 to prevent further complications later on, and to make those records appear clearer. To do so, we can use replace_na() from the tidyr package.\n\n\nCode\ndengue_full_sf$cases &lt;- replace_na(dengue_full_sf$cases, 0)\n\n\n\n\nCode\ndengue_full_sf\n\n\n# A tibble: 5,160 × 6\n   VILLENG     TOWNID                  geometry `epiweek(發病日)` location cases\n   &lt;chr&gt;       &lt;chr&gt;              &lt;POLYGON [°]&gt;             &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;\n 1 Qingcao Vi… D06    ((120.1176 23.08387, 120…                31 Qingcao…     0\n 2 Bao'an Vil. D32    ((120.2304 22.93544, 120…                31 Bao'an …     1\n 3 Chihkan Vi… D08    ((120.2012 22.99966, 120…                31 Chihkan…     0\n 4 Dacheng Vi… D02    ((120.1985 22.98147, 120…                31 Dacheng…     0\n 5 Chengbei V… D06    ((120.1292 23.06512, 120…                31 Chengbe…     0\n 6 Chengnan V… D06    ((120.1246 23.06904, 120…                31 Chengna…     0\n 7 Fahua Vil.  D08    ((120.2094 22.98452, 120…                31 Fahua V…     0\n 8 Hainan Vil. D06    ((120.175 23.02218, 120.…                31 Hainan …     0\n 9 Guo'an Vil. D06    ((120.1866 23.02766, 120…                31 Guo'an …     0\n10 Xixin Vil.  D06    ((120.1834 23.06086, 120…                31 Xixin V…     0\n# ℹ 5,150 more rows\n\n\n\n\n\n7.1.2 Building space-time cube\nNext, we will be building a space-time cube of our dengue data in the study areas of villages in Tainan City, across an epidemiology week of 31-50 in 2023.\nStep 1: Prepare geometry data\nLet’s create a location column, similar to in dengue_epi_sf, in our geometry data taiwanstudy_sf first:\n\n\nCode\ntaiwanstudy_sf &lt;- taiwanstudy_sf %&gt;% mutate(location = paste(VILLENG, \",\", TOWNID))\n\n\nStep 2: Creating spacetime object\nHere, we can create our spacetime object using spacetime() from sfdep.\n\n\nCode\ndengue_st &lt;- spacetime(dengue_full_sf, taiwanstudy_sf,\n         .loc_col = \"location\",.time_col = \"epiweek(發病日)\")\n\n\n\n\nCode\ndengue_st\n\n\n# A tibble: 5,160 × 6\n   VILLENG     TOWNID                  geometry `epiweek(發病日)` location cases\n * &lt;chr&gt;       &lt;chr&gt;              &lt;POLYGON [°]&gt;             &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;\n 1 Qingcao Vi… D06    ((120.1176 23.08387, 120…                31 Qingcao…     0\n 2 Bao'an Vil. D32    ((120.2304 22.93544, 120…                31 Bao'an …     1\n 3 Chihkan Vi… D08    ((120.2012 22.99966, 120…                31 Chihkan…     0\n 4 Dacheng Vi… D02    ((120.1985 22.98147, 120…                31 Dacheng…     0\n 5 Chengbei V… D06    ((120.1292 23.06512, 120…                31 Chengbe…     0\n 6 Chengnan V… D06    ((120.1246 23.06904, 120…                31 Chengna…     0\n 7 Fahua Vil.  D08    ((120.2094 22.98452, 120…                31 Fahua V…     0\n 8 Hainan Vil. D06    ((120.175 23.02218, 120.…                31 Hainan …     0\n 9 Guo'an Vil. D06    ((120.1866 23.02766, 120…                31 Guo'an …     0\n10 Xixin Vil.  D06    ((120.1834 23.06086, 120…                31 Xixin V…     0\n# ℹ 5,150 more rows\n\n\nTo verify if the result is indeed a spacetime object,\n\n\nCode\nis_spacetime(dengue_st)\n\n\n[1] TRUE\n\n\nAnd whether it is a spacetime cube,\n\n\nCode\nis_spacetime_cube(dengue_st)\n\n\nYAY! We’ve successfully created our spacetime cube, which is a spatio-tempotial full grid."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02_2.html#computing-local-gi-statistics",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02_2.html#computing-local-gi-statistics",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 2)",
    "section": "7.2 Computing local Gi* statistics",
    "text": "7.2 Computing local Gi* statistics\n\n7.2.1 Deriving spatial weight matrix\nBefore we can compute local Gi* statistics, we would need to derive a spatial weight matrix. To do so, we can apply the following code below, using the functions of sfdep and dplyr packages.\n\n\nCode\ndengue_nb &lt;- dengue_st %&gt;% \n  activate(\"geometry\") %&gt;% \n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 50,\n                                  alpha = 1),\n         .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\n\nIn the code above, activate() from dplyr package is used to activate the geometry context\n\nmutate() from dplyr package is usd to create two new columns nb and wt\nset_nbs() and set_wts() are used to activate the data context again and copy the values to each time_slice\nNow, our dataset has neighbours and weights for each time-slice.\n\n\n\nCode\nhead(dengue_nb)\n\n\n# A tibble: 6 × 8\n  VILLENG      TOWNID                  geometry `epiweek(發病日)` location cases\n  &lt;chr&gt;        &lt;chr&gt;              &lt;POLYGON [°]&gt;             &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;\n1 Qingcao Vil. D06    ((120.1176 23.08387, 120…                31 Qingcao…     0\n2 Bao'an Vil.  D32    ((120.2304 22.93544, 120…                31 Bao'an …     1\n3 Chihkan Vil. D08    ((120.2012 22.99966, 120…                31 Chihkan…     0\n4 Dacheng Vil. D02    ((120.1985 22.98147, 120…                31 Dacheng…     0\n5 Chengbei Vi… D06    ((120.1292 23.06512, 120…                31 Chengbe…     0\n6 Chengnan Vi… D06    ((120.1246 23.06904, 120…                31 Chengna…     0\n# ℹ 2 more variables: nb &lt;list&gt;, wt &lt;list&gt;\n\n\n\n\n7.2.2 Computing Gi*\nNext, we used the derived new columns to manually calculate the local Gi* for each location.\n\n\nCode\ngi_stars &lt;- dengue_nb %&gt;%\n  group_by(`epiweek(發病日)`) %&gt;%\n  mutate(gi_star = local_gstar_perm(\n    cases, nb, wt)) %&gt;%\n  unnest(gi_star)\n\n\nIn the above code:\n\ngroup the rows by epiweek(發病日), using group_by() from dplyr package\ncreate a new column with local_gstar_perm() computations from sfdep package\nunnest gi_star column of the newly created gi_stars data.frame, using unnest() from tidyr package\n\nThe result of our code is shown below:\n\n\nCode\ngi_stars\n\n\n# A tibble: 5,160 × 16\n# Groups:   epiweek(發病日) [20]\n   VILLENG     TOWNID                  geometry `epiweek(發病日)` location cases\n   &lt;chr&gt;       &lt;chr&gt;              &lt;POLYGON [°]&gt;             &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt;\n 1 Qingcao Vi… D06    ((120.1176 23.08387, 120…                31 Qingcao…     0\n 2 Bao'an Vil. D32    ((120.2304 22.93544, 120…                31 Bao'an …     1\n 3 Chihkan Vi… D08    ((120.2012 22.99966, 120…                31 Chihkan…     0\n 4 Dacheng Vi… D02    ((120.1985 22.98147, 120…                31 Dacheng…     0\n 5 Chengbei V… D06    ((120.1292 23.06512, 120…                31 Chengbe…     0\n 6 Chengnan V… D06    ((120.1246 23.06904, 120…                31 Chengna…     0\n 7 Fahua Vil.  D08    ((120.2094 22.98452, 120…                31 Fahua V…     0\n 8 Hainan Vil. D06    ((120.175 23.02218, 120.…                31 Hainan …     0\n 9 Guo'an Vil. D06    ((120.1866 23.02766, 120…                31 Guo'an …     0\n10 Xixin Vil.  D06    ((120.1834 23.06086, 120…                31 Xixin V…     0\n# ℹ 5,150 more rows\n# ℹ 10 more variables: nb &lt;list&gt;, wt &lt;list&gt;, gi_star &lt;dbl&gt;, e_gi &lt;dbl&gt;,\n#   var_gi &lt;dbl&gt;, p_value &lt;dbl&gt;, p_sim &lt;dbl&gt;, p_folded_sim &lt;dbl&gt;,\n#   skewness &lt;dbl&gt;, kurtosis &lt;dbl&gt;"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02_2.html#mann-kendall-test",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02_2.html#mann-kendall-test",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 2)",
    "section": "7.3 Mann-Kendall Test",
    "text": "7.3 Mann-Kendall Test\nWith the Gi* measures, we can evaluate trends in each village using the Mann-Kendall test. For instance, to evaluate the trend for location Andong Village which has TOWNID D06 (denoted by location = Andong Vil. , D06) we can apply the code chunk below.\n\n\nCode\nandong_mktest &lt;- gi_stars %&gt;% \n  ungroup() %&gt;%\n  filter(location == \"Andong Vil. , D06\") %&gt;%\n  select(location, `epiweek(發病日)`, gi_star)\n\n\n\n\nCode\nandong_mktest\n\n\n# A tibble: 20 × 3\n   location          `epiweek(發病日)` gi_star\n   &lt;chr&gt;                         &lt;dbl&gt;   &lt;dbl&gt;\n 1 Andong Vil. , D06                31 -0.691 \n 2 Andong Vil. , D06                32 -0.895 \n 3 Andong Vil. , D06                33 -0.583 \n 4 Andong Vil. , D06                34 -0.233 \n 5 Andong Vil. , D06                35 -0.760 \n 6 Andong Vil. , D06                36 -0.0642\n 7 Andong Vil. , D06                37 -0.163 \n 8 Andong Vil. , D06                38  0.179 \n 9 Andong Vil. , D06                39  0.561 \n10 Andong Vil. , D06                40  2.56  \n11 Andong Vil. , D06                41  1.38  \n12 Andong Vil. , D06                42  0.357 \n13 Andong Vil. , D06                43  0.865 \n14 Andong Vil. , D06                44  0.674 \n15 Andong Vil. , D06                45  1.72  \n16 Andong Vil. , D06                46  0.466 \n17 Andong Vil. , D06                47  1.47  \n18 Andong Vil. , D06                48  2.33  \n19 Andong Vil. , D06                49 -0.153 \n20 Andong Vil. , D06                50  0.493 \n\n\nThen, we can plot result using ggplot2 functions to observe the Gi* statistic across different epidemiology weeks.\n\n\nCode\nggplot(data=andong_mktest,\n       aes (x=`epiweek(發病日)`,\n            y= gi_star)) + \n         geom_line(color=\"green\") +\n  geom_line(y = 0) + \n  geom_line(y = 1.96) + \n  geom_line(y = -1.96)\n\n\n\n\n\nIn the above plot, I have also added a reference lines where\n\nGi* = 0,\nGi* = 1.96 and\nGi * = -1.96.\n\nHow should we interpret the Gi* statistics? The Gi* statistic returned for each feature in the dataset is a z-score. Since the value of z-score indicates the significance of the result,\n\nThe more statistically significant positive (very positive value) the Gi*, the more intense the clustering of high values (hot spot).\nThe more statistically significant negative (very negative value) the Gi*, the more intense the clustering of low values (cold spot).\n\nWith that, we will use the positivity or negativity of the Gi* statistic to determine if it is a hotspot or a coldspot. Then, we should check if Gi* &gt; 1.96 or Gi* &lt; -1.96 to determine if the hotspot/coldspot is statistically significant at 95% confidence level.\n\nTherefore, for Andong Village, it appears that weeks 40 and 46 are epidemiology weeks of significant hotspots! However, in general, there is no trend as the results for most of the other weeks are statistically insignificant at 95% confidence level.\n\nAs there are too many villages, I am unable to display the result for every single village in this website. However, I have created a ShinyApp website in which you can select any combination of and compare the trends of different villages in Tainan City, Taiwan here!\nAlternatively, rather than creating static plots, we can create an interactive plot using ggplotly() of plotly package to plot the Gi* results.\n\n\nCode\np &lt;- ggplot(data = andong_mktest,\n            aes(x = `epiweek(發病日)`,\n                y = gi_star)) +\n  geom_line() +\n  geom_line(y = 0) + \n  geom_line(y = 1.96) + \n  geom_line(y = -1.96)\n\nggplotly(p)\n\n\n\n\n\n\nWe can perform the above steps for any village we’re interested in!"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02_2.html#performing-emerging-hotspot-analysis-ehsa",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02_2.html#performing-emerging-hotspot-analysis-ehsa",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 2)",
    "section": "7.4 Performing Emerging Hotspot Analysis (EHSA)",
    "text": "7.4 Performing Emerging Hotspot Analysis (EHSA)\nNow… the exciting part! We would be checking which villages exhibit trends!\n\n7.4.1 Performing EHSA\nTo perform EHSA, we can use emerging_hotspot_analysis() from the sfdep package.\n\n\nCode\nehsa &lt;- emerging_hotspot_analysis(\n  x = dengue_st,\n  .var = \"cases\",\n  k = 1,\n  nsim = 79\n)\n\n\nIn the code above, the arguments:\n\nx is the spacetime object\n.var is the variable we want to test\nk = nsim is the last simulations we want to stop run (we start at the 0th simulation)\n\n\n\nCode\nhead(ehsa)\n\n\n# A tibble: 6 × 4\n  location               tau  p_value classification     \n  &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;              \n1 Qingcao Vil. , D06   0.611 0.000191 no pattern detected\n2 Bao'an Vil. , D32   -0.453 0.00582  no pattern detected\n3 Chihkan Vil. , D08   0.621 0.000147 no pattern detected\n4 Dacheng Vil. , D02   0.358 0.0297   no pattern detected\n5 Chengbei Vil. , D06  0.611 0.000191 no pattern detected\n6 Chengnan Vil. , D06  0.284 0.0855   no pattern detected\n\n\n\n\n7.4.2 Visualizing the distribution of EHSA classes\n\n\nCode\nggplot(data = ehsa,\n       aes(x=classification)) +\n  geom_bar()\n\n\n\nFrom the visualization, it seems like all villages have no pattern detected in terms of distribution. How significant is this result though?\n\n\n7.4.3 Visualizing EHSA\nTo examine the significance of the conclusion “all villages have no significant trend detected in terms of number of dengue cases”, we can visualize the EHSA and it’s p-value.\nFirstly, we derive the EHSA of each village by using left_join() from the dplyr package to insert the EHSA values to each village in our study area.\n\n\nCode\ntaiwanstudy_ehsa &lt;- taiwanstudy_sf %&gt;% left_join(ehsa, join_by (location))\n\n\nThen, we use tmap functions to create the choropleth map.\n\n\nCode\nehsa_sig &lt;- taiwanstudy_ehsa %&gt;%\n  filter(p_value &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(taiwanstudy_ehsa) + \n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\n  tm_shape(ehsa_sig) + \n  tm_fill(\"classification\") +\n  tm_borders(alpha = 0.4)\n\n\n\nIn the above code, we are highlighting only regions that exhibit significance in having “no trend detected”. It seems that there are a large number of villages (in turquoise) with no significant trends detected over the epidemiology weeks of 35-50 in year 2023!\nOn the other hand, the grey villages do not exhibit significance in having “no trend detected” over the epidemiology weeks of 35-50 in year 2023."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Bin Hui",
    "section": "",
    "text": "Hi there!\nI am Ong Bin Hui, a third-year student at the Singapore Management University (SMU), majoring in Finance with a second major in Data Science and Analytics.\nApart from my love for music and the exciting world of Finance and Investments, I am very passionate in understanding and harnessing the power of Data Science and Analytics!\nWhy so?\nI believe Data Science and Analytics has a wonderful ability and potential to bring tremendous value to societies and the world in an uncountable number of aspects like healthcare, housing and many more… all of which can uplift lives!\nWith that, I am actively pursuing knowledge and experience in this area, in hopes I can effectively utilize them to contribute to the greater good. Geospatial Analytics, which is the course I am journaling about on this website, is an example of a skill I believe can help me in moving closer towards my goals.\nI hope you’ll enjoy looking through my course work and potentially be inspired about this skill as well! Should you like to chat with me, you can drop me an email at @binhui.ong.2021@business.smu.edu.sg!\nHave a great day ahead! :D\np.s.: Here’s a feel-good song to make your day better: Get a Guitar by RIIZE"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "",
    "text": "In this exercise, we will delve into the processes of importing, transforming and analyzing geospatial and apatial data in R with the use of sf and tidyverse packages."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#installing-and-loading-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#installing-and-loading-r-packages",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Installing and Loading R Packages",
    "text": "Installing and Loading R Packages\nNext, I will install and load tidyverse and sf packages using p_load() from the pacman package. The pacman package is useful to load multiple packages at once!\n\npacman::p_load(tidyverse, sf)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-geospatial-data-shapefile-format",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-geospatial-data-shapefile-format",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Importing geospatial data: shapefile format",
    "text": "Importing geospatial data: shapefile format\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/binhui-ong/IS415-GAA/Hands-on_Ex/Hands-on_Ex01/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\ncyclingpath &lt;- st_read(dsn = \"data/geospatial\",\n                       layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `/Users/binhui-ong/IS415-GAA/Hands-on_Ex/Hands-on_Ex01/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2558 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-geospatial-data-kml-format",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-geospatial-data-kml-format",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "Importing geospatial data: kml format",
    "text": "Importing geospatial data: kml format\n\npreschool &lt;- st_read(dsn = \"data/geospatial/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `/Users/binhui-ong/IS415-GAA/Hands-on_Ex/Hands-on_Ex01/data/geospatial/PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-st_geometry",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-st_geometry",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.5.1 Working with st_geometry()",
    "text": "1.5.1 Working with st_geometry()\nBesides mpsz$geom and mpsz, we can retrieve the geometry list-column in an sf data.frame using st_geometry().\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nThis would display only basic information of the feature class such as type of geometry, geographic extent of the features, and coordinate system of the data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-glimpse",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-glimpse",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.5.2 Working with glimpse()",
    "text": "1.5.2 Working with glimpse()\nWe can also learn more about the associated attribute information in the data frame using glimpse(0) of dplyr.\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-head",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#working-with-head",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.5.3. Working with head()",
    "text": "1.5.3. Working with head()\nTo find out complete information of a feature object, we can use head() of Base R.\n\nhead(mpsz, n=5)\n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30..."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#assigning-epsg-code-to-a-simple-feature-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#assigning-epsg-code-to-a-simple-feature-data-frame",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.7.1 Assigning EPSG code to a simple feature data frame",
    "text": "1.7.1 Assigning EPSG code to a simple feature data frame\nWe use st_crs() from the sf package to retrieve the coordinate reference system for an object.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nWe see that the EPSG assigned to mpsz is 9001. However, the EPSG for svy21 is actually 3414.\nHence, we have to assign a new EPSG to mpsz. We define the updated mpsz as mpsz3414 using st_set_crs() from the sf package.\n\nmpsz3414 &lt;- st_set_crs(mpsz, 3414)\n\nLet’s check that the EPSG is correct now.\n\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#transforming-the-projection-of-preschool-from-wgs84-to-svy21",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#transforming-the-projection-of-preschool-from-wgs84-to-svy21",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.7.2 Transforming the projection of preschool from wgs84 to svy21",
    "text": "1.7.2 Transforming the projection of preschool from wgs84 to svy21\nIn geospatial analytics, we often transform the original data from geographic coordinate system to projected coordinate system. This facilitates analysis involving distance and/or area measurements.\nAs seen earlier in 1.5.1, the preschool simple feature data frame shows that it is in wgs84 coordinate system.\nTo perform the projection transformation,\n\npreschool3414 &lt;- st_transform(preschool, crs =3414)\n\nThe new preschool3414 data frame is as follows:\n\n\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11810.03 ymin: 25596.33 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n     Name\n1   kml_1\n2   kml_2\n3   kml_3\n4   kml_4\n5   kml_5\n6   kml_6\n7   kml_7\n8   kml_8\n9   kml_9\n10 kml_10\n                                                                                                                                                                                                                                                                                                                                                                                                                  Description\n1                             &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;CENTRE_NAME&lt;/th&gt; &lt;td&gt;CHILDREN'S COVE PRESCHOOL PTE.LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;CENTRE_CODE&lt;/th&gt; &lt;td&gt;PT9390&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;498CC9FE48CC94D4&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201093631&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n2                                      &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;CENTRE_NAME&lt;/th&gt; &lt;td&gt;CHILDREN'S COVE PTE. LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;CENTRE_CODE&lt;/th&gt; &lt;td&gt;PT8675&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;22877550804213FD&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201093631&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n3                         &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;CENTRE_NAME&lt;/th&gt; &lt;td&gt;CHILDREN'S VINEYARD PRESCHOOL PTE. LTD&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;CENTRE_CODE&lt;/th&gt; &lt;td&gt;PT9308&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;B2FE90E44AD494E3&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201093631&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n4                   &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;CENTRE_NAME&lt;/th&gt; &lt;td&gt;CHILDTIME CARE & DEVELOPMENT CENTRE PTE.LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;CENTRE_CODE&lt;/th&gt; &lt;td&gt;PT9122&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;1384CDC0D14B76A1&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201093631&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n5                                                 &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;CENTRE_NAME&lt;/th&gt; &lt;td&gt;CHILTERN HOUSE&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;CENTRE_CODE&lt;/th&gt; &lt;td&gt;PT2070&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;FB24EAA6E73B2723&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201093631&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n6                                      &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;CENTRE_NAME&lt;/th&gt; &lt;td&gt;CHILTERN HOUSE EAST COAST&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;CENTRE_CODE&lt;/th&gt; &lt;td&gt;PT6550&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;B53C79DF64135499&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201093631&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n7                                     &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;CENTRE_NAME&lt;/th&gt; &lt;td&gt;CHILTERN HOUSE MOUNTBATTEN&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;CENTRE_CODE&lt;/th&gt; &lt;td&gt;PT8637&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;B53C79DFBF7AD96F&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201093631&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n8                                       &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;CENTRE_NAME&lt;/th&gt; &lt;td&gt;CHILTERN HOUSE TURF CLUB&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;CENTRE_CODE&lt;/th&gt; &lt;td&gt;PT5400&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;8F2BC6E9BF962BC8&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201093631&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n9                              &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;CENTRE_NAME&lt;/th&gt; &lt;td&gt;CHINESE CHRISTIAN MISSION LIMITED&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;CENTRE_CODE&lt;/th&gt; &lt;td&gt;RC0740&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;CA317E72A442CEB6&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201093631&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n10 &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;CENTRE_NAME&lt;/th&gt; &lt;td&gt;CHOW & CHOWS CHILDCARE & EARLY LEARNING CENTRE (CCK 542) LTD.&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;CENTRE_CODE&lt;/th&gt; &lt;td&gt;RC1775&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;2072C1C4F5E69A9C&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201093631&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt;\n                        geometry\n1  POINT Z (25089.46 31299.16 0)\n2  POINT Z (27189.07 32792.54 0)\n3  POINT Z (28844.56 36773.76 0)\n4  POINT Z (24821.92 46303.16 0)\n5  POINT Z (28637.82 35038.49 0)\n6  POINT Z (33248.74 32260.59 0)\n7  POINT Z (33248.74 32260.59 0)\n8   POINT Z (23591.47 35202.8 0)\n9  POINT Z (18338.28 36619.18 0)\n10 POINT Z (18148.23 41723.46 0)\n\n\nYay! The preschool location data is in svy projected coordinate system now.\nAdditionally, looking at the Bounding box, the values are larger than the 0-360 decimal degree commonly used by most of the geographic coordinate systems."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-aspatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#importing-aspatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.8.1 Importing Aspatial Data",
    "text": "1.8.1 Importing Aspatial Data\nAs the data is in csv format, we will use read_csv() of readr package to import listing.csv. The output R object is called listings and it is a tibble data frame.\n\nlistings &lt;- read_csv(\"data/aspatial/listings.csv\")\n\nTo check if the data file has been imported correctly, we can also use list() of Base R to check the tibble data frame.\n\nlist(listings)\n\n[[1]]\n# A tibble: 3,457 × 18\n       id name      host_id host_name neighbourhood_group neighbourhood latitude\n    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;               &lt;chr&gt;            &lt;dbl&gt;\n 1  71609 Villa in…  367042 Belinda   East Region         Tampines          1.35\n 2  71896 Home in …  367042 Belinda   East Region         Tampines          1.35\n 3  71903 Home in …  367042 Belinda   East Region         Tampines          1.35\n 4 275343 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 5 275344 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 6 289234 Home in …  367042 Belinda   East Region         Tampines          1.34\n 7 294281 Rental u… 1521514 Elizabeth Central Region      Newton            1.31\n 8 324945 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 9 330095 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n10 369141 Place to… 1521514 Elizabeth Central Region      Newton            1.31\n# ℹ 3,447 more rows\n# ℹ 11 more variables: longitude &lt;dbl&gt;, room_type &lt;chr&gt;, price &lt;dbl&gt;,\n#   minimum_nights &lt;dbl&gt;, number_of_reviews &lt;dbl&gt;, last_review &lt;date&gt;,\n#   reviews_per_month &lt;dbl&gt;, calculated_host_listings_count &lt;dbl&gt;,\n#   availability_365 &lt;dbl&gt;, number_of_reviews_ltm &lt;dbl&gt;, license &lt;chr&gt;\n\n\nThe output reveals that listing tibble data frame consists of 3,457 rows and 18 columns.\nTwo useful fields we are going to use in the next phase are latitude and longitude. As they are in the decimal degree format, we could assume that the data is in wgs84 Geographic Coordinate System."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#creating-a-simple-feature-data-frame-from-an-aspatial-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#creating-a-simple-feature-data-frame-from-an-aspatial-data-frame",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.8.2 Creating a simple feature data frame from an aspatial data frame",
    "text": "1.8.2 Creating a simple feature data frame from an aspatial data frame\nThe following code chunk converts listing data frame into a simple feature data frame by using st_as_sf() of sf packages.\n\nlistings_sf &lt;- st_as_sf(listings, \n                        coords = c(\"longitude\", \"latitude\"),\n                        crs=4326) |&gt;\n  st_transform(crs=3414)\n\n\nglimpse(listings_sf)\n\nRows: 3,457\nColumns: 17\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           &lt;chr&gt; \"Villa in Singapore · ★4.44 · 2 bedroom…\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat…\n$ price                          &lt;dbl&gt; 150, 80, 80, 64, 78, 220, 85, 75, 69, 7…\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 92, 60, 60, 92,…\n$ number_of_reviews              &lt;dbl&gt; 19, 24, 46, 20, 16, 12, 131, 17, 5, 81,…\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              &lt;dbl&gt; 0.13, 0.16, 0.30, 0.15, 0.11, 0.09, 0.9…\n$ calculated_host_listings_count &lt;dbl&gt; 5, 5, 5, 51, 51, 5, 7, 51, 51, 7, 7, 1,…\n$ availability_365               &lt;dbl&gt; 55, 91, 91, 183, 183, 54, 365, 183, 183…\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 3, 0, 0, 1, 2, 0, 0, 0, 0, …\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n$ geometry                       &lt;POINT [m]&gt; POINT (41972.5 36390.05), POINT (…"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#buffering",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#buffering",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.9.1 Buffering",
    "text": "1.9.1 Buffering\nCase: The authority is planning to upgrade the existing cycling path. To do so, they need to acquire 5 metres of reserved land on both sides of the current cycling path. You are tassked to determine the extent of land to be acquired and their total area.\nSolution:\nStep 1: Compute the 5-meter buffers around cycling paths with st_buffer() of sf package.\n\nbuffer_cycling &lt;- st_buffer(cyclingpath, dist = 5, nQuadSegs = 30)\n\nStep 2: Calculate the area of the buffers with st_area() of sf package.\n\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\n\nStep 3: Derive the total land involved with sum() of Base R.\n\nsum(buffer_cycling$AREA)\n\n1774367 [m^2]\n\n\nTADAH! We’re done!"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#point-in-polygon-count",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#point-in-polygon-count",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1.9.2 Point-in-polygon count",
    "text": "1.9.2 Point-in-polygon count\nCase: A pre-school service group wants to find out the number of pre-schools in each Planning Subzome.\nSolution:\nStep 1:\n\nmpsz3414$'PreSch Count' &lt;- lengths(st_intersects (mpsz3414, preschool3414))\n\nStep 2:\n\nsummary(mpsz3414$'PreSch Count')\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\nStep 3:\n\ntop_n(mpsz3414, 1, 'PreSch Count')\n\nSimple feature collection with 323 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry PreSch Count\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...            0\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...            6\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...            0\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...            5\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...            3\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...           13\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...            5\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...            1\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...           11\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...            1\n\n\nAdditional Question: Calculate the density of pre-school by planning subzone.\nSolution:\nStep 1: Derive the area of each planning subzone with st_area() from sf package.\n\nmpsz3414$Area &lt;- mpsz3414 |&gt;\n                st_area()\n\nStep 2: Compute the density with mutate() of dplyr package.\n\nmpsz3414 &lt;- mpsz3414 |&gt;\n        mutate(`PreSch Density` = (`PreSch Count`/Area * 1000000))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html",
    "title": "Hands-on Exercise 2: Thematic Mapping and GeoVisualization with R",
    "section": "",
    "text": "In this exercise, I will be demonstrating the use of R packages to create themed maps that are helpful to visualize geospatial data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-geospatial-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-geospatial-data-into-r",
    "title": "Hands-on Exercise 2: Thematic Mapping and GeoVisualization with R",
    "section": "2.3.1 Importing Geospatial Data into R",
    "text": "2.3.1 Importing Geospatial Data into R\nNext, we will import the MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz. Here, we use the st_read() from sf.\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/binhui-ong/IS415-GAA/Hands-on_Ex/Hands-on_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nWe can examine the content of mpsz with the following code chunk.:\n\nmpsz \n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\nThe code chunk allows us to have a quick understanding of the data by displaying only the first 10, out of 323, records."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-attribute-aspatial-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#importing-attribute-aspatial-data-into-r",
    "title": "Hands-on Exercise 2: Thematic Mapping and GeoVisualization with R",
    "section": "2.3.2 Importing Attribute (Aspatial) Data into R",
    "text": "2.3.2 Importing Attribute (Aspatial) Data into R\nNext, we would be importing into the respopagesextod2011to2020.csv file into R.\nWe will use read_csv() from the readr package and save it as a data frame called popdata.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#data-preparation",
    "title": "Hands-on Exercise 2: Thematic Mapping and GeoVisualization with R",
    "section": "2.3.3 Data Preparation",
    "text": "2.3.3 Data Preparation\nIn this exercise, we aim to use the attribute data that only has year 2020 values, with variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY, where\n\nYOUNG: age group 0 to 4 until age group 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group.\n\nIn the following two sub-sections, we will prepare the relevant attribute data, and combine it with the geospatial data to produce a new data frame for our map production.\n\n2.3.3.1. Data wrangling\nWe will use the following data wrangling and transformation functions to extract the relevant data.\n\npivot_wider() of tidyr package, and mutate(), filter(), group_by()\nselect() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[14])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:13])+\nrowSums(.[15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\nNext, before conducting the join, we must convert the values in PA and SZ fields in the attribute data to uppercase. This is because the fields are currently in a mix of upper- and lowercase, while in the geospatial data, SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 &lt;- popdata2020 |&gt;\n  mutate_at(.vars= vars(`PA`, `SZ`), .funs = list(toupper))\n\npopdata2020\n\n# A tibble: 332 × 7\n   PA         SZ                   YOUNG `ECONOMY ACTIVE`  AGED TOTAL DEPENDENCY\n   &lt;chr&gt;      &lt;chr&gt;                &lt;dbl&gt;            &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1 ANG MO KIO ANG MO KIO TOWN CEN…  1290             2760   760  4810      0.743\n 2 ANG MO KIO CHENG SAN             5640            16460  6050 28150      0.710\n 3 ANG MO KIO CHONG BOON            5100            15000  6470 26570      0.771\n 4 ANG MO KIO KEBUN BAHRU           4620            13010  5120 22750      0.749\n 5 ANG MO KIO SEMBAWANG HILLS       1880             3630  1310  6820      0.879\n 6 ANG MO KIO SHANGRI-LA            3330             9050  3610 15990      0.767\n 7 ANG MO KIO TAGORE                1940             4480  1530  7950      0.775\n 8 ANG MO KIO TOWNSVILLE            4190            11950  5100 21240      0.777\n 9 ANG MO KIO YIO CHU KANG             0                0     0     0    NaN    \n10 ANG MO KIO YIO CHU KANG EAST     1110             2410   750  4270      0.772\n# ℹ 322 more rows\n\n\n\n\n2.3.3.2 Joining the geospatial and attribute data\nNow, we are ready to combine the geospatial and attribute data! For this step, we use left_join() from the dplyr package to join the attribute data to the geographical data, using planning subzone name (i.e SUBZONE_N and SZ) as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz,popdata2020, by = c(\"SUBZONE_N\" = \"SZ\"))\n\nAs the output follows the format of the left data frame in left_join, using mpsz as our left data table ensures that the output would also be a simple feature data frame.\nNow that we are almost done with our data preparation, the last step is to save the data as a file in our directory for future use.\nTo do so, we must create a new folder “rds” in our directory. Thereafter, we apply the code chunk below.\n\nwrite_rds(mpsz_pop2020,\"data/rds/mpsz_popdata2020.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#approach-1-plot-a-choropleth-map-quickly-using-qtm",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#approach-1-plot-a-choropleth-map-quickly-using-qtm",
    "title": "Hands-on Exercise 2: Thematic Mapping and GeoVisualization with R",
    "section": "2.4.1 Approach 1: Plot a choropleth map quickly using qtm()",
    "text": "2.4.1 Approach 1: Plot a choropleth map quickly using qtm()\nThis approach uses tmap to draw a choropleth map simply and quickly. It is concise and give a good default visualization in many cases.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, fill = \"DEPENDENCY\")\n\n\n\n\nIn the above code, tmap_mode() with “plot” input produces a static map. To produce an interactive map, we should use “view” input.\nAdditionally, the fill argument is used to map the attribute.\nNonetheless, this approach limits our ability to customize the map. To create a high quality cartographic choropleth map, we should use tmap’s drawing elements shown in the following Approach 2."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#approach-2-creating-a-choropleth-map-using-tmaps-elements",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#approach-2-creating-a-choropleth-map-using-tmaps-elements",
    "title": "Hands-on Exercise 2: Thematic Mapping and GeoVisualization with R",
    "section": "2.4.2 Approach 2: Creating a choropleth map using tmap’s elements",
    "text": "2.4.2 Approach 2: Creating a choropleth map using tmap’s elements\nThe second approach involves adding tmap’s elements layer by layer to create a high quality cartographic choropleth map like the one below. It also allows us to customize various details of the map such as fill and border transparency, and title position through options (such as “alpha=” for fill transparency) within the functions to best suit our aesthetic needs.\nIt is more complicated and lengthy, but definitely rewarding when we want a beautiful graph!\n\n\n\n\n\nSteps\nStep 1: Draw a base map\nTo begin, we would need to input the basic building block of tmap, which is tm_shape(). Thereafter, we add layers of elements such as tm_polygons() (or tm_fills() which will be shown later) to assign the target variable.\n\ntm_shape(mpsz_pop2020) +\n    tm_polygons()\n\n\n\n\nStep 2: Assigning target variable to tm_polygons()\nSince our target variable is “DEPENDENCY”, we would assign it to tm_polygons(). It will create a map with borders within the map and fills to represent the target variable.\n\ntm_shape(mpsz_pop2020) +\n    tm_polygons(\"DEPENDENCY\")\n\n\n\n\nNote: If we want to adjust the transparency of the borders or fills, we can do so with the border.alpha and alpha options respectively.\nAlternative to tm_polygons(): tm_fills() and tm_borders()\nIn a case in which we do not want the borders within the map, we can use tm_fill() instead of tm_polygons.\n\ntm_shape(mpsz_pop2020) +\n    tm_fill(\"DEPENDENCY\")\n\n\n\n\nThen, if we want to include the borders too, we can add a layer with tm_borders().\nThe combination of tm_fill() and tm_borders() would results in the same map as that with tm_polygons() in the first code chunk of this step (shown earlier on).\n\ntm_shape(mpsz_pop2020) +\n    tm_fill(\"DEPENDENCY\") +\n  tm_borders(col= \"tomato\", lwd = 0.2, alpha =0.8, lty = \"dashed\")\n\n\n\n\nThe above is an example of how we made some adjustments to the appearance of the borders:\n\nborder color: “tomato”\nborder line width (lwd): 0.1\nborder transparency: 0.9 (where 0 = completely transparent and 1 = completely opaque)\nborder line type: dashed"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#data-classification-methods-of-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#data-classification-methods-of-tmap",
    "title": "Hands-on Exercise 2: Thematic Mapping and GeoVisualization with R",
    "section": "2.4.3 Data classification methods of tmap",
    "text": "2.4.3 Data classification methods of tmap\n\n2.4.3.1 Plotting choropleth maps using built-in classification methods\nMost choropleth maps use some methods of data classification to group a large number of observations into data ranges or classes.\ntmap provides a total of ten data classification methods for our needs, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, jenks\nTo define a data classification method, the style option in tm_polygons() or tm_fill() can be applied.\nIn the following examples, I will be using tm_polygons() to exhibit how different data classifications and different number of classes can affect the maps produced.\nExample 1: Different data classification with the same number of classes\nData classification with quantile data classification that uses 5 classes\n\ntm_shape(mpsz_pop2020) + \n  tm_polygons(\"DEPENDENCY\", style = \"quantile\", n=5)\n\n\n\n\nData classification with equal data classification that uses 5 classes\n\ntm_shape(mpsz_pop2020) + \n  tm_polygons(\"DEPENDENCY\", style = \"equal\", n=5)\n\n\n\n\nFrom the maps generated by the two different data classification methods, we can see that the quantile data classification method produces a more evenly distribution compared to the equal data classification method, even though they have the same number of classes.\nExample 2: Data classification with the same data classification, using different number of classes\nData classification with equal data classification that uses 4 classes\n\ntm_shape(mpsz_pop2020) + \n  tm_polygons(\"DEPENDENCY\", style = \"equal\", n=4)\n\n\n\n\nData classification with equal data classification that uses 10 classes\n\ntm_shape(mpsz_pop2020) + \n  tm_polygons(\"DEPENDENCY\", style = \"equal\", n=10)\n\n\n\n\nFrom the maps generated by the two different number of classes, we can see that using 4 classes produces a more evenly distribution compared to using 10 classes, even though they employ the same data classification method.\nIn summary, it is important to note that our choice of data classification and classes can significantly affect the distribution in maps, which can in turn affect the reader’s insights drawn from the map, especially at first glance.\n\n\n2.4.3.2 Plotting choropleth maps with customized breaks\nFor all built-in styles, the category breaks are computed internally. To set particular breaks, the breakpoints can be set explicitly using the “breaks=” argument in tm_polygons() or tm_fill().\nIt is important to note that in tmap, the breaks include minimum and maximum points. Hence, for n categories, we must have (n+1) elements in the “breaks=” option, in increasing order.\nBefore we begin, we should attain some descriptive statistics to get an idea of where we should set our break points. To compute the descriptive statistics of “DEPENDENCY” field, we apply the following code.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6519  0.7025  0.7742  0.7645 19.0000      92 \n\n\nFrom the descriptive statistics, we can reasonably set break points at 0.60, 0.70, 0.80 and 0.90, and include our minimum point as 0 and maximum point as 100. The resulting breaks vector is c(0, 0.60, 0.70, 0.80, 0.90).\n\ntm_shape(mpsz_pop2020) + \n  tm_polygons(\"DEPENDENCY\", breaks = c(0, 0.60, 0.70, 0.80, 0.90)) \n\n\n\n\nAs we can see, setting our own breaks has appeared to create a more meaningful representation of the distribution. Hence, when classifying data, we should compare this method with the built-in functions method (section 2.4.3.1) and employ the method that produces a more meaningful result."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#color-scheme",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#color-scheme",
    "title": "Hands-on Exercise 2: Thematic Mapping and GeoVisualization with R",
    "section": "2.4.4 Color Scheme",
    "text": "2.4.4 Color Scheme\ntmap supports color ramps defined by the user or a set of predefined color ramps from the RColorBrewer package.\n\n2.4.4.1 Using ColorBrewer palette\nTo change the color, we assign the preferred color to the palette option of tm_polygons() or tm_fill(). For instance, in the code chunk below, we assign “Blues” to the palette argument tm_polygons.\n\ntm_shape(mpsz_pop2020) + \n  tm_polygons(\"DEPENDENCY\", \n          n=6,\n          style = \"quantile\", \n          palette = \"Blues\")\n\n\n\n\nTo reverse the color shading, we add a “-” prefix to the color.\n\ntm_shape(mpsz_pop2020) + \n  tm_polygons(\"DEPENDENCY\", \n          n=6,\n          style = \"quantile\", \n          palette = \"-Blues\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#map-layouts",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#map-layouts",
    "title": "Hands-on Exercise 2: Thematic Mapping and GeoVisualization with R",
    "section": "2.4.5 Map Layouts",
    "text": "2.4.5 Map Layouts\nMap layout refers to the combination of all map elements into a cohesive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Color settings and data classification methods covered in the previous section related to the palette and break-points are used to affect how the map looks.\n\n2.4.5.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020) + \n  tm_polygons(\"DEPENDENCY\", \n              border.alpha = 0.5, \n              style = \"jenks\", \n              palette = \"Blues\", \n              legend.hist = TRUE, \n              legend.is.portrait = TRUE, \n              legend.hist.z = 0.1) + \n  tm_layout(main.title = \"Distribution of Dependency Ratio by Planning Subzone (Jenks Classification)\", \n            main.title.position = \"center\", \n            main.title.size = 1, \n            legend.height = 0.35,\n            legend.width = 0.35,\n            legend.outside = FALSE, \n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE)\n\n\n\n\n\n\n2.4.5.2 Map Style\ntmap also allows a wide variety of layout settings to be adjusted. To do so, we can add tmap_style() to the code chunk.\nFor example, to use “classic” style in our map, we apply the following code.\n\ntm_shape(mpsz_pop2020) + \n  tm_polygons(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\",\n          border.alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n2.4.5.3 Cartographic Furniture\nIn addition to map style, tmap also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto to choropleth map.\n\ntm_shape(mpsz_pop2020) + \n  tm_polygons(\"DEPENDENCY\", \n              border.alpha = 0.5, \n              style = \"quantile\", \n              palette = \"Blues\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by Planning Subzone (Quantile Classification)\", \n            main.title.position = \"center\", \n            main.title.size = 1, \n            legend.height = 0.35,\n            legend.width = 0.35,\n            legend.outside = FALSE, \n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) + \n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA) and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nTo reset the default style, we can use:\n\ntmap_style(\"white\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#drawing-small-multiple-choropleth-maps",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#drawing-small-multiple-choropleth-maps",
    "title": "Hands-on Exercise 2: Thematic Mapping and GeoVisualization with R",
    "section": "2.4.6 Drawing Small Multiple Choropleth Maps",
    "text": "2.4.6 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arranged side-by-side, and sometimes stacked vertically. They enable the visualization of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nApproach 1: By assigning multiple values to at least one the aesthetic arguments\nApproach 2: By defining a group-by variable in tm_facets(), and\nApproach 3: By creating multiple stand-alone maps with tmap_arrange().\n\n\n2.4.6.1 Approach 1 ; Assign muliple values to at least one of the aesthetic arguments\nExample 1a: In this example, small multiple choropleth maps are created by defining ncols in tm_polygons().\n\ntm_shape(mpsz_pop2020) + \n  tm_polygons(c(\"YOUNG\", \"AGED\"),\n              style = \"equal\",\n              palette = \"Blues\",\n              border.alpha = 0.5) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"),\n            ) + \n  tmap_style(\"white\")\n\n\n\n\nExample 1b: In this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments.\n\ntm_shape(mpsz_pop2020) + \n  tm_polygons(c(\"DEPENDENCY\", \"AGED\"), \n              style = c(\"equal\", \"quantile\"),\n              palette = list(\"Blues\", \"Greens\")) + \n  tm_layout(legend.position=c(\"right\", \"bottom\"))\n\n\n\n\n\n\n2.4.6.2 Approach 2: Define a group-by variable in tm_facets\nExample 2: In this example, multiple small choropleth maps are created using tm_facets().\n\ntm_shape(mpsz_pop2020) + \n  tm_polygons(\"DEPENDENCY\",\n              style = \"quantile\",\n              palette = \"Blues\",\n              thres.poly = 0,\n              border.alpha = 0.5) + \n  tm_facets(by = \"REGION_N\",\n            free.coords = TRUE,\n            drop.units = TRUE) + \n  tm_layout(legend.show = FALSE, \n            title.position = c(\"center\", \"center\"),\n            title.size = 20)\n\n\n\n\n\n\n2.4.6.3 Approach 3: Create multiple stand-alone maps with tmap_arrange()\nIn this approach, we create different maps and combine them together with tmap_arrange(), in which we can customize how we want to arrange them using the ncol option.\n\nyoungmap &lt;- tm_shape(mpsz_pop2020) +\n  tm_polygons(\"YOUNG\", \n              style = \"quantile\",\n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020) +\n  tm_polygons(\"AGED\", \n              style = \"quantile\",\n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp = 1 ,ncol = 2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#mapping-spatial-object-meeting-a-selection-criterion",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#mapping-spatial-object-meeting-a-selection-criterion",
    "title": "Hands-on Exercise 2: Thematic Mapping and GeoVisualization with R",
    "section": "2.4.7 Mapping Spatial Object Meeting a Selection Criterion",
    "text": "2.4.7 Mapping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, you can also use selection function to map spatial objects that meet the selection criterion.\nFor example, if we want to map the “DEPENDENCY” distribution only for the “CENTRAL REGION” of Singapore, we can apply the following code.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\",])+\n  tm_polygons(\"DEPENDENCY\",\n              style = \"quantile\",\n              palette = \"Blues\",\n              legend.hist = TRUE, \n              legend.is.portrait = TRUE,\n              legend.histz = 0.1,\n              border.alpha = 0.5) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.35,\n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#tmap-package",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#tmap-package",
    "title": "Hands-on Exercise 2: Thematic Mapping and GeoVisualization with R",
    "section": "2.5.1 tmap package",
    "text": "2.5.1 tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#geospatial-data-wrangling",
    "title": "Hands-on Exercise 2: Thematic Mapping and GeoVisualization with R",
    "section": "2.5.2 Geospatial data wrangling",
    "text": "2.5.2 Geospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#data-wrangling-1",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Ex02.html#data-wrangling-1",
    "title": "Hands-on Exercise 2: Thematic Mapping and GeoVisualization with R",
    "section": "2.5.3 Data wrangling",
    "text": "2.5.3 Data wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions\n\nWith the guide of Professor Kam Tin Seong."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html",
    "title": "Hands-on Exercise 4: Spatial Weights and Applications",
    "section": "",
    "text": "In this hands-on exercise, we will be learn to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute spatial weights using appropriate functions of spdep package,\nand calculate spatially lagged variables using appropriate functions of spdep package.\n\n\n\nWe will be using two of the following data sets:\n\n\n\n\n\n\n\n\nType\nContent\nFormat\n\n\n\n\nGeospatial\nHunan county boundary layer\nESRI shapefile\n\n\nAspatial\nHunan’s local development indicators in 2012\ncsv"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#the-study-area-and-data",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#the-study-area-and-data",
    "title": "Hands-on Exercise 4: Spatial Weights and Applications",
    "section": "",
    "text": "We will be using two of the following data sets:\n\n\n\n\n\n\n\n\nType\nContent\nFormat\n\n\n\n\nGeospatial\nHunan county boundary layer\nESRI shapefile\n\n\nAspatial\nHunan’s local development indicators in 2012\ncsv"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#installing-and-loading-r-packages",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#installing-and-loading-r-packages",
    "title": "Hands-on Exercise 4: Spatial Weights and Applications",
    "section": "2.1 Installing and Loading R packages",
    "text": "2.1 Installing and Loading R packages\nIn this exercise, we will be using the following packages;\n\nsf\nspdep\ntmap\ntidyverse\nknitr\n\nTo install and load the R packages, we will use p_load() from the pacman package.\n\n\nCode\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#importing-data",
    "title": "Hands-on Exercise 4: Spatial Weights and Applications",
    "section": "2.2 Importing Data",
    "text": "2.2 Importing Data\n\n2.2.1 Importing Geospatial Data\nTo import the Hunan shapefile geospatial data as an sf object, we will use st_read() from sf package.\n\n\nCode\nhunan &lt;- st_read(dsn = \"data/geospatial\",\n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `/Users/binhui-ong/IS415-GAA/Hands-on_Ex/Hands-on_Ex04/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n2.2.2 Importing Aspatial Data\nTo import the Hunan_2012.csv data into a R dataframe class, we use read_csv() of readr package.\n\n\nCode\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\n2.2.3 Performing Relational Join\nNext, we will update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe using left_join() of dplyr package.\n\n\nCode\nhunan &lt;- left_join(hunan, hunan2012)\n\n\n\n\nCode\nhunan &lt;- hunan %&gt;% select(1:4, 7, 15)\n\n\nLet’s explore the updated data table.\n\n\nCode\nhunan\n\n\nSimple feature collection with 88 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734..."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-queen-contiguity-based-neighbours",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-queen-contiguity-based-neighbours",
    "title": "Hands-on Exercise 4: Spatial Weights and Applications",
    "section": "4.1 Computing (QUEEN) contiguity based neighbours",
    "text": "4.1 Computing (QUEEN) contiguity based neighbours\nIn this section, we will learn how to use poly2nb() of spdep package to compute contiguity weight matrices for the study area.\nThis function builds a neighbours list based on regions with contiguous boundaries. If we look at the documentation, we will see that we can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\n\n\nCode\nwm_q &lt;- poly2nb(hunan, queen = TRUE)\nsummary(wm_q)\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. For example, to see the neighbors for the first polygon in the object, we can use the following code.\n\n\nCode\nwm_q[[1]]\n\n\n[1]  2  3  4 57 85\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.\nWe can retrieve the county name of Polygon ID=1 by using the code chunk below:\n\n\nCode\nhunan$County[1]\n\n\n[1] \"Anxiang\"\n\n\nThe output reveals that Polygon ID = 1 is Anxiang county.\nTo reveal the country names of the five neighbouring polygons, we use the following code:\n\n\nCode\nhunan$NAME_3[c(2,3,4,57,85)]\n\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nWe can retrieve the GDPPC of these five countries with the following code:\n\n\nCode\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n\n[1] 20981 34592 24473 21311 22879\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen’s method are 20981, 34592, 24473, 21311 and 22879 respectively.\nWe can display the complete weight matrix by using str().\n\n\nCode\nstr(wm_q)\n\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#creating-rook-contiguity-based-neighbours",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#creating-rook-contiguity-based-neighbours",
    "title": "Hands-on Exercise 4: Spatial Weights and Applications",
    "section": "4.2 Creating (ROOK) contiguity based neighbours",
    "text": "4.2 Creating (ROOK) contiguity based neighbours\n\n\nCode\nwm_r &lt;- poly2nb(hunan, queen = FALSE)\nsummary(wm_r)\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 10 neighbours. There are two units with only one neighbour."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualizing-contiguity-weights",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#visualizing-contiguity-weights",
    "title": "Hands-on Exercise 4: Spatial Weights and Applications",
    "section": "4.3 Visualizing contiguity weights",
    "text": "4.3 Visualizing contiguity weights\nA connectivity graph takes a point and displays a line to each neighboring point. Since, we are working with polygons at the moment, we will need to get points in order to make our connectivity graphs. The most common method for this will be polygon centroids. We will calculate these in the sf package before moving onto the graphs.\n\n4.3.1 Preparing the points for connectivity graph\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid on the sf object: hunan. We need the coordinates in a separate data frame for this to work.\nTo do this, we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of hunan. Our function will be st_centroid(). We will be using map_dbl() variation of map from the purrr package.\nTo get our longitude values, we map the st_centroid() function over the geometry column of hunan and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\n\nCode\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\n\nWe do the same for latitude. However, in this case, we access the second value in each centroid with [[2]].\n\n\nCode\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\n\nNow that we have the latitude and longitude, we use cbind() to bind them together into the same object.\n\n\nCode\ncoords &lt;- cbind(longitude, latitude)\n\n\nThen, let’s check the first few observations to ensure the coordinates are formatted correctly.\n\n\nCode\nhead(coords)\n\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\nThey look correct! Now, we’re ready to move to our next step.\n\n\n4.3.2 Plotting Queen contiguity based neighbours map\n\n\nCode\nplot(hunan$geometry, border = \"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\n\n\n4.3.3 Plotting Rook contiguity based neighbours map\n\n\nCode\nplot(hunan$geometry, border = \"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\n\n\n4.3.4 Plotting both Queen and Rook contiguity based neighbours maps\n\n\nCode\npar(mfrow = c(1,2))\nplot(hunan$geometry, border = \"lightgrey\", main = \"Queen Contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\nplot(hunan$geometry, border = \"lightgrey\", main = \"Rooks Contiguity\")\nplot(wm_r, coords, pch = 10, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#determine-the-cut-off-distance",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#determine-the-cut-off-distance",
    "title": "Hands-on Exercise 4: Spatial Weights and Applications",
    "section": "5.1 Determine the cut-off distance",
    "text": "5.1 Determine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n\nCode\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour is 61.79km, so using this as the upper threshold gives certainty that all units will have at least one neighbour."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-fixed-distance-weight-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-fixed-distance-weight-matrix",
    "title": "Hands-on Exercise 4: Spatial Weights and Applications",
    "section": "5.2 Computing fixed distance weight matrix",
    "text": "5.2 Computing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown below:\n\n\nCode\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nFrom the above, we can see that there are 324 nonzero links, and 4.18% nonzero weights.\nNext, we will use str() to display the content of wm_d62 weight matrix.\n\n\nCode\nstr(wm_d62)\n\n\nList of 88\n $ : int [1:5] 3 4 5 57 64\n $ : int [1:4] 57 58 78 85\n $ : int [1:4] 1 4 5 57\n $ : int [1:3] 1 3 5\n $ : int [1:4] 1 3 4 85\n $ : int 69\n $ : int [1:2] 67 84\n $ : int [1:4] 9 46 47 78\n $ : int [1:4] 8 46 68 84\n $ : int [1:4] 16 22 70 72\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:2] 11 17\n $ : int 13\n $ : int [1:4] 10 17 22 83\n $ : int [1:3] 11 14 16\n $ : int [1:3] 20 22 63\n $ : int [1:5] 20 21 73 74 82\n $ : int [1:5] 18 19 21 22 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:4] 10 16 18 20\n $ : int [1:3] 41 77 82\n $ : int [1:4] 25 28 31 54\n $ : int [1:4] 24 28 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:2] 26 29\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:2] 27 37\n $ : int 33\n $ : int [1:2] 24 36\n $ : int 50\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:5] 31 34 45 56 80\n $ : int [1:2] 29 42\n $ : int [1:3] 44 77 79\n $ : int [1:4] 40 42 43 81\n $ : int [1:3] 39 45 79\n $ : int [1:5] 23 35 45 79 82\n $ : int [1:5] 26 37 39 43 81\n $ : int [1:3] 39 42 44\n $ : int [1:2] 38 43\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:5] 8 9 35 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:4] 48 49 50 52\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:2] 48 55\n $ : int [1:5] 24 28 49 50 52\n $ : int [1:4] 48 50 53 75\n $ : int 36\n $ : int [1:5] 1 2 3 58 64\n $ : int [1:5] 2 57 64 66 68\n $ : int [1:3] 60 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:5] 12 60 62 63 87\n $ : int [1:4] 61 63 77 87\n $ : int [1:5] 12 18 61 62 83\n $ : int [1:4] 1 57 58 76\n $ : int 76\n $ : int [1:5] 58 67 68 76 84\n $ : int [1:2] 7 66\n $ : int [1:4] 9 58 66 84\n $ : int [1:2] 6 75\n $ : int [1:3] 10 72 73\n $ : int [1:2] 73 74\n $ : int [1:3] 10 11 70\n $ : int [1:4] 19 70 71 74\n $ : int [1:5] 19 21 71 73 86\n $ : int [1:2] 55 69\n $ : int [1:3] 64 65 66\n $ : int [1:3] 23 38 62\n $ : int [1:2] 2 8\n $ : int [1:4] 38 40 41 45\n $ : int [1:5] 34 35 36 45 47\n $ : int [1:5] 25 26 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:4] 12 13 16 63\n $ : int [1:4] 7 9 66 68\n $ : int [1:2] 2 5\n $ : int [1:4] 21 46 47 74\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language dnearneigh(x = coords, d1 = 0, d2 = 62, longlat = TRUE)\n - attr(*, \"dnn\")= num [1:2] 0 62\n - attr(*, \"bounds\")= chr [1:2] \"GE\" \"LE\"\n - attr(*, \"nbtype\")= chr \"distance\"\n - attr(*, \"sym\")= logi TRUE\n\n\nAnother way to display the structure of the weight matrix is to combine table() and card() of spdep.\n\n\nCode\ntable(hunan$County, card(wm_d62))\n\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\nFrom the table above, we notice that every county has only one “1” value, with “0” in the rest of their columns. Here, the columns indicate the number of neighbours the county has, where 1 is “true” and 0 is “false”. For instance, for Anxiang, value “1” lies in column “4”, implying that Anxiang has 4 neighbours.\nTo find the number of disjoint connected subgraphs, we can use n.comp.nb() from the spdep package.\n\n\nCode\nn_comp &lt;- n.comp.nb(wm_d62)\nn_comp$nc\n\n\n[1] 1\n\n\nThen, to find the county ID that the coordinates of the disjoint subgraphs belong to, we can use the following code:\n\n\nCode\ntable(n_comp$comp.id)\n\n\n\n 1 \n88 \n\n\n\n5.2.1 Plotting fixed distance weight matrix\n\n\nCode\nplot(hunan$geometry, border = \"lightgrey\")\nplot(wm_d62, coords, add = TRUE)\nplot(k1, coords, add = TRUE, col = \"red\", length = 0.08)\n\n\n\n\n\nThe red lines show the links of 1st nearest neighbours and the black lines show the links of neighbours within the cut-off distance of 62km.\nAlternatively, we can plot both of them side by side for comparison:\n\n\nCode\npar(mfrow=c(1,2))\nplot(hunan$geometry, border = \"lightgrey\", main = \"1st Nearest Neighbours\")\nplot(k1, coords, add = TRUE, col = \"red\", length = 0.08)\nplot(hunan$geometry, border = \"lightgrey\", main = \"Distance Link\")\nplot(wm_d62, coords, add = TRUE, pch = 19, cex = 0.6)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-adaptive-distance-weight-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#computing-adaptive-distance-weight-matrix",
    "title": "Hands-on Exercise 4: Spatial Weights and Applications",
    "section": "5.3 Computing adaptive distance weight matrix",
    "text": "5.3 Computing adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have less neighbours. Having many neighbours smooths the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either by accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\n\nCode\nknn6 &lt;- knn2nb(knearneigh(coords, k = 6))\nknn6\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nSimilarly, we can display the content of the matrix using str().\n\n\nCode\nstr(knn6)\n\n\nList of 88\n $ : int [1:6] 2 3 4 5 57 64\n $ : int [1:6] 1 3 57 58 78 85\n $ : int [1:6] 1 2 4 5 57 85\n $ : int [1:6] 1 3 5 6 69 85\n $ : int [1:6] 1 3 4 6 69 85\n $ : int [1:6] 3 4 5 69 75 85\n $ : int [1:6] 9 66 67 71 74 84\n $ : int [1:6] 9 46 47 78 80 86\n $ : int [1:6] 8 46 66 68 84 86\n $ : int [1:6] 16 19 22 70 72 73\n $ : int [1:6] 10 14 16 17 70 72\n $ : int [1:6] 13 15 60 61 63 83\n $ : int [1:6] 12 15 60 61 63 83\n $ : int [1:6] 11 15 16 17 72 83\n $ : int [1:6] 12 13 14 17 60 83\n $ : int [1:6] 10 11 17 22 72 83\n $ : int [1:6] 10 11 14 16 72 83\n $ : int [1:6] 20 22 23 63 77 83\n $ : int [1:6] 10 20 21 73 74 82\n $ : int [1:6] 18 19 21 22 23 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:6] 10 16 18 19 20 83\n $ : int [1:6] 18 20 41 77 79 82\n $ : int [1:6] 25 28 31 52 54 81\n $ : int [1:6] 24 28 31 33 54 81\n $ : int [1:6] 25 27 29 33 42 81\n $ : int [1:6] 26 29 30 37 42 81\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:6] 26 27 37 42 43 81\n $ : int [1:6] 26 27 28 33 49 81\n $ : int [1:6] 24 25 36 39 40 54\n $ : int [1:6] 24 31 50 54 55 56\n $ : int [1:6] 25 26 28 30 49 81\n $ : int [1:6] 36 40 41 45 56 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:6] 26 27 29 42 43 44\n $ : int [1:6] 23 43 44 62 77 79\n $ : int [1:6] 25 40 42 43 44 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:6] 26 27 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:6] 37 38 39 42 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:6] 8 9 35 47 78 86\n $ : int [1:6] 8 21 35 46 80 86\n $ : int [1:6] 49 50 51 52 53 55\n $ : int [1:6] 28 33 48 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:6] 28 48 49 50 52 54\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:6] 48 50 51 52 55 75\n $ : int [1:6] 24 28 49 50 51 52\n $ : int [1:6] 32 48 50 52 53 75\n $ : int [1:6] 32 34 36 78 80 85\n $ : int [1:6] 1 2 3 58 64 68\n $ : int [1:6] 2 57 64 66 68 78\n $ : int [1:6] 12 13 60 61 87 88\n $ : int [1:6] 12 13 59 61 63 87\n $ : int [1:6] 12 13 60 62 63 87\n $ : int [1:6] 12 38 61 63 77 87\n $ : int [1:6] 12 18 60 61 62 83\n $ : int [1:6] 1 3 57 58 68 76\n $ : int [1:6] 58 64 66 67 68 76\n $ : int [1:6] 9 58 67 68 76 84\n $ : int [1:6] 7 65 66 68 76 84\n $ : int [1:6] 9 57 58 66 78 84\n $ : int [1:6] 4 5 6 32 75 85\n $ : int [1:6] 10 16 19 22 72 73\n $ : int [1:6] 7 19 73 74 84 86\n $ : int [1:6] 10 11 14 16 17 70\n $ : int [1:6] 10 19 21 70 71 74\n $ : int [1:6] 19 21 71 73 84 86\n $ : int [1:6] 6 32 50 53 55 69\n $ : int [1:6] 58 64 65 66 67 68\n $ : int [1:6] 18 23 38 61 62 63\n $ : int [1:6] 2 8 9 46 58 68\n $ : int [1:6] 38 40 41 43 44 45\n $ : int [1:6] 34 35 36 41 45 47\n $ : int [1:6] 25 26 28 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:6] 12 13 15 16 22 63\n $ : int [1:6] 7 9 66 68 71 74\n $ : int [1:6] 2 3 4 5 56 69\n $ : int [1:6] 8 9 21 46 47 74\n $ : int [1:6] 59 60 61 62 63 88\n $ : int [1:6] 59 60 61 62 63 87\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language knearneigh(x = coords, k = 6)\n - attr(*, \"sym\")= logi FALSE\n - attr(*, \"type\")= chr \"knn\"\n - attr(*, \"knn-k\")= num 6\n - attr(*, \"class\")= chr \"nb\"\n\n\nNotice that each county has exactly six neighbours!\n\n5.3.1 Plotting distance based neighbours\n\n\nCode\nplot(hunan$geometry, border = \"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\n#6 Weights based on IDW\nIn this section, we will learn to derive a spatial weight matrix based on Inversed Distance method.\nFirstly, we will compute the distances between areas by using nbdists() of spdep package.\n\n\nCode\ndist &lt;- nbdists(wm_q, coords, longlat = TRUE)\n\n\nThen, to derive the Inversed Distance, we use the following code:\n\n\nCode\nids &lt;- lapply(dist, function(x) 1/(x))\nids\n\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n[[2]]\n[1] 0.01535405 0.01764308 0.01925924 0.02323898 0.01719350\n\n[[3]]\n[1] 0.03916350 0.02822040 0.03695795 0.01395765\n\n[[4]]\n[1] 0.01820896 0.02822040 0.03414741 0.01539065\n\n[[5]]\n[1] 0.03695795 0.03414741 0.01524598 0.01618354\n\n[[6]]\n[1] 0.015390649 0.015245977 0.021748129 0.011883901 0.009810297\n\n[[7]]\n[1] 0.01708612 0.01473997 0.01150924 0.01872915\n\n[[8]]\n[1] 0.02022144 0.03453056 0.02529256 0.01036340 0.02284457 0.01500600 0.01515314\n\n[[9]]\n[1] 0.02022144 0.01574888 0.02109502 0.01508028 0.02902705 0.01502980\n\n[[10]]\n[1] 0.02281552 0.01387777 0.01538326 0.01346650 0.02100510 0.02631658 0.01874863\n[8] 0.01500046\n\n[[11]]\n[1] 0.01882869 0.02243492 0.02247473\n\n[[12]]\n[1] 0.02779227 0.02419652 0.02333385 0.02986130 0.02335429\n\n[[13]]\n[1] 0.02779227 0.02650020 0.02670323 0.01714243\n\n[[14]]\n[1] 0.01882869 0.01233868 0.02098555\n\n[[15]]\n[1] 0.02650020 0.01233868 0.01096284 0.01562226\n\n[[16]]\n[1] 0.02281552 0.02466962 0.02765018 0.01476814 0.01671430\n\n[[17]]\n[1] 0.01387777 0.02243492 0.02098555 0.01096284 0.02466962 0.01593341 0.01437996\n\n[[18]]\n[1] 0.02039779 0.02032767 0.01481665 0.01473691 0.01459380\n\n[[19]]\n[1] 0.01538326 0.01926323 0.02668415 0.02140253 0.01613589 0.01412874\n\n[[20]]\n[1] 0.01346650 0.02039779 0.01926323 0.01723025 0.02153130 0.01469240 0.02327034\n\n[[21]]\n[1] 0.02668415 0.01723025 0.01766299 0.02644986 0.02163800\n\n[[22]]\n[1] 0.02100510 0.02765018 0.02032767 0.02153130 0.01489296\n\n[[23]]\n[1] 0.01481665 0.01469240 0.01401432 0.02246233 0.01880425 0.01530458 0.01849605\n\n[[24]]\n[1] 0.02354598 0.01837201 0.02607264 0.01220154 0.02514180\n\n[[25]]\n[1] 0.02354598 0.02188032 0.01577283 0.01949232 0.02947957\n\n[[26]]\n[1] 0.02155798 0.01745522 0.02212108 0.02220532\n\n[[27]]\n[1] 0.02155798 0.02490625 0.01562326\n\n[[28]]\n[1] 0.01837201 0.02188032 0.02229549 0.03076171 0.02039506\n\n[[29]]\n[1] 0.02490625 0.01686587 0.01395022\n\n[[30]]\n[1] 0.02090587\n\n[[31]]\n[1] 0.02607264 0.01577283 0.01219005 0.01724850 0.01229012 0.01609781 0.01139438\n[8] 0.01150130\n\n[[32]]\n[1] 0.01220154 0.01219005 0.01712515 0.01340413 0.01280928 0.01198216 0.01053374\n[8] 0.01065655\n\n[[33]]\n[1] 0.01949232 0.01745522 0.02229549 0.02090587 0.01979045\n\n[[34]]\n[1] 0.03113041 0.03589551 0.02882915\n\n[[35]]\n[1] 0.01766299 0.02185795 0.02616766 0.02111721 0.02108253 0.01509020\n\n[[36]]\n[1] 0.01724850 0.03113041 0.01571707 0.01860991 0.02073549 0.01680129\n\n[[37]]\n[1] 0.01686587 0.02234793 0.01510990 0.01550676\n\n[[38]]\n[1] 0.01401432 0.02407426 0.02276151 0.01719415\n\n[[39]]\n[1] 0.01229012 0.02172543 0.01711924 0.02629732 0.01896385\n\n[[40]]\n[1] 0.01609781 0.01571707 0.02172543 0.01506473 0.01987922 0.01894207\n\n[[41]]\n[1] 0.02246233 0.02185795 0.02205991 0.01912542 0.01601083 0.01742892\n\n[[42]]\n[1] 0.02212108 0.01562326 0.01395022 0.02234793 0.01711924 0.01836831 0.01683518\n\n[[43]]\n[1] 0.01510990 0.02629732 0.01506473 0.01836831 0.03112027 0.01530782\n\n[[44]]\n[1] 0.01550676 0.02407426 0.03112027 0.01486508\n\n[[45]]\n[1] 0.03589551 0.01860991 0.01987922 0.02205991 0.02107101 0.01982700\n\n[[46]]\n[1] 0.03453056 0.04033752 0.02689769\n\n[[47]]\n[1] 0.02529256 0.02616766 0.04033752 0.01949145 0.02181458\n\n[[48]]\n[1] 0.02313819 0.03370576 0.02289485 0.01630057 0.01818085\n\n[[49]]\n[1] 0.03076171 0.02138091 0.02394529 0.01990000\n\n[[50]]\n[1] 0.01712515 0.02313819 0.02551427 0.02051530 0.02187179\n\n[[51]]\n[1] 0.03370576 0.02138091 0.02873854\n\n[[52]]\n[1] 0.02289485 0.02394529 0.02551427 0.02873854 0.03516672\n\n[[53]]\n[1] 0.01630057 0.01979945 0.01253977\n\n[[54]]\n[1] 0.02514180 0.02039506 0.01340413 0.01990000 0.02051530 0.03516672\n\n[[55]]\n[1] 0.01280928 0.01818085 0.02187179 0.01979945 0.01882298\n\n[[56]]\n[1] 0.01036340 0.01139438 0.01198216 0.02073549 0.01214479 0.01362855 0.01341697\n\n[[57]]\n[1] 0.028079221 0.017643082 0.031423501 0.029114131 0.013520292 0.009903702\n\n[[58]]\n[1] 0.01925924 0.03142350 0.02722997 0.01434859 0.01567192\n\n[[59]]\n[1] 0.01696711 0.01265572 0.01667105 0.01785036\n\n[[60]]\n[1] 0.02419652 0.02670323 0.01696711 0.02343040\n\n[[61]]\n[1] 0.02333385 0.01265572 0.02343040 0.02514093 0.02790764 0.01219751 0.02362452\n\n[[62]]\n[1] 0.02514093 0.02002219 0.02110260\n\n[[63]]\n[1] 0.02986130 0.02790764 0.01407043 0.01805987\n\n[[64]]\n[1] 0.02911413 0.01689892\n\n[[65]]\n[1] 0.02471705\n\n[[66]]\n[1] 0.01574888 0.01726461 0.03068853 0.01954805 0.01810569\n\n[[67]]\n[1] 0.01708612 0.01726461 0.01349843 0.01361172\n\n[[68]]\n[1] 0.02109502 0.02722997 0.03068853 0.01406357 0.01546511\n\n[[69]]\n[1] 0.02174813 0.01645838 0.01419926\n\n[[70]]\n[1] 0.02631658 0.01963168 0.02278487\n\n[[71]]\n[1] 0.01473997 0.01838483 0.03197403\n\n[[72]]\n[1] 0.01874863 0.02247473 0.01476814 0.01593341 0.01963168\n\n[[73]]\n[1] 0.01500046 0.02140253 0.02278487 0.01838483 0.01652709\n\n[[74]]\n[1] 0.01150924 0.01613589 0.03197403 0.01652709 0.01342099 0.02864567\n\n[[75]]\n[1] 0.011883901 0.010533736 0.012539774 0.018822977 0.016458383 0.008217581\n\n[[76]]\n[1] 0.01352029 0.01434859 0.01689892 0.02471705 0.01954805 0.01349843 0.01406357\n\n[[77]]\n[1] 0.014736909 0.018804247 0.022761507 0.012197506 0.020022195 0.014070428\n[7] 0.008440896\n\n[[78]]\n[1] 0.02323898 0.02284457 0.01508028 0.01214479 0.01567192 0.01546511 0.01140779\n\n[[79]]\n[1] 0.01530458 0.01719415 0.01894207 0.01912542 0.01530782 0.01486508 0.02107101\n\n[[80]]\n[1] 0.01500600 0.02882915 0.02111721 0.01680129 0.01601083 0.01982700 0.01949145\n[8] 0.01362855\n\n[[81]]\n[1] 0.02947957 0.02220532 0.01150130 0.01979045 0.01896385 0.01683518\n\n[[82]]\n[1] 0.02327034 0.02644986 0.01849605 0.02108253 0.01742892\n\n[[83]]\n[1] 0.023354289 0.017142433 0.015622258 0.016714303 0.014379961 0.014593799\n[7] 0.014892965 0.018059871 0.008440896\n\n[[84]]\n[1] 0.01872915 0.02902705 0.01810569 0.01361172 0.01342099 0.01297994\n\n[[85]]\n [1] 0.011451133 0.017193502 0.013957649 0.016183544 0.009810297 0.010656545\n [7] 0.013416965 0.009903702 0.014199260 0.008217581 0.011407794\n\n[[86]]\n[1] 0.01515314 0.01502980 0.01412874 0.02163800 0.01509020 0.02689769 0.02181458\n[8] 0.02864567 0.01297994\n\n[[87]]\n[1] 0.01667105 0.02362452 0.02110260 0.02058034\n\n[[88]]\n[1] 0.01785036 0.02058034"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#spatial-lag-with-row-standarized-weights",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#spatial-lag-with-row-standarized-weights",
    "title": "Hands-on Exercise 4: Spatial Weights and Applications",
    "section": "7.1 Spatial lag with row-standarized weights",
    "text": "7.1 Spatial lag with row-standarized weights\nFinally, we’ll compute the average neighbor GDPPC value for each polygon. These values are often referred to as spatially lagged values.\n\n\nCode\nGDPPC.lag &lt;- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nRecall in the previous section, we retrieved the GDPPC of these five countries by using the code chunk below:\n\n\nCode\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n\n[1] 20981 34592 24473 21311 22879\n\n\nWe can append the spatially lagged GDPPC values onto hunan sf data frame by using the code chunk below.\n\n\nCode\nlag.list &lt;- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag GDPPC\")\nhunan &lt;- left_join(hunan, lag.res)\n\n\nThe following table shows the average neighboring income values (stored in the Inc.lag object) for each county.\n\n\nCode\nhead(hunan)\n\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.\n\n\nCode\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_gdppc &lt;- qtm(hunan, \"lag GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, asp = 1, ncol = 2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#spatial-lag-as-a-sum-of-neighbouring-values",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#spatial-lag-as-a-sum-of-neighbouring-values",
    "title": "Hands-on Exercise 4: Spatial Weights and Applications",
    "section": "7.2 Spatial lag as a sum of neighbouring values",
    "text": "7.2 Spatial lag as a sum of neighbouring values\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights. Thereafter, we use glist = in the nb2listw function to explicitly assign these weights.\nWe start by applying a function that will assign a value of 1 per neighbor. This is done with lapply, which we have been using to manipulate the neighbors structure throughout the past notebooks. Basically, it applies a function across each value in the neighbors structure.\n\n\nCode\nb_weights &lt;- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_q, \n                       glist = b_weights,\n                       style = \"B\")\nb_weights2\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith the proper weights assigned, we can use lag.listw to compute a lag variable from our weight structure and GDPPC.\n\n\nCode\nlag_sum &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag_sum)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")\n\n\nFirstly, let’s examine the result with the following code chunk.\n\n\nCode\nlag_sum\n\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 124236 113624  96573 110950 109081 106244 174988 235079 273907 256221\n[11]  98013 104050 102846  92017 133831 158446 141883 119508 150757 153324\n[21] 113593 129594 142149 100119  82884  74668  43184  99244  46549  20518\n[31] 140576 121601  92069  43258 144567 132119  51694  59024  69349  73780\n[41]  94651 100680  69398  52798 140472 118623 180933  82798  83090  97356\n[51]  59482  77334  38777 111463  74715 174391 150558 122144  68012  84575\n[61] 143045  51394  98279  47671  26360 236917 220631 185290  64640  70046\n[71] 126971 144693 129404 284074 112268 203611 145238 251536 108078 238300\n[81] 108870 108085 262835 248182 244850 404456  67608  33860\n\n\nNext, we will append the lag_sum GDPPC field into hunan sf data frame by using the following code chunk.\n\n\nCode\nhunan &lt;- left_join(hunan, lag.res)\n\n\nThen, we can plot both the GDPPC and Spatial Lag Sum GDPPC for comparison using the code below.\n\n\nCode\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_sum_gdppc &lt;- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp = 1, ncol = 2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#spatial-window-average",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#spatial-window-average",
    "title": "Hands-on Exercise 4: Spatial Weights and Applications",
    "section": "7.3 Spatial window average",
    "text": "7.3 Spatial window average\nThe spatial window average uses row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbours structure and add the diagonal element before assigning weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from the spdep package.\n\n\nCode\nwm_qs &lt;- include.self(wm_q)\n\n\n\n\nCode\nwm_qs\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNotice that the number of nonzero links, percentage nonzero weights and average number of links are 536, 6.921488, 6.090909 respectively compared to wm_q of 488, 5.785124, and 5.090909.\nLet us look at the neighbour list of area [1] by using the following code chunk:\n\n\nCode\nwm_qs[[1]]\n\n\n[1]  1  2  3  4 57 85\n\n\nNotice that now [1] has six neighbours instead of five!\nNow, let’s obtain weights with nb2listw().\n\n\nCode\nwm_qs &lt;- nb2listw(wm_qs)\n\n\nLastly, we need to create the lag variable from our weight structure and GDPPC variable.\n\n\nCode\nlag_w_avg_gdppc &lt;- lag.listw(wm_qs,\n                             hunan$GDPPC)\nlag_w_avg_gdppc\n\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\n\nCode\nlag.list.wm_qs &lt;- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nlag_wm_qs.res &lt;- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) &lt;- c(\"NAME_3\", \"lag_window_avg.GDPPC\")\n\n\nNote: The third command line on the code chunk above renames the field names of lag_wm_q1.res object into NAME_3 and lag_window_avg GDPPC respectively.\nNext, the code chunk below will be used to append lag_window_avg GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\n\nCode\nhunan &lt;- left_join(hunan, lag_wm_qs.res)\n\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\n\nCode\nhunan %&gt;% \n  select(\"County\", \"lag GDPPC\", \"lag_window_avg.GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg.GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n43747.00\n47621.20\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n33582.71\n37160.12\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n45651.17\n49224.71\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n32027.62\n29886.89\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n32671.00\n26627.50\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n20810.00\n22690.17\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n25711.50\n25366.40\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n30672.33\n25825.75\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n33457.75\n30329.00\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n31689.20\n32682.83\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n20269.00\n25948.62\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n23901.60\n23987.67\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n25126.17\n25463.14\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n21903.43\n21904.38\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n22718.60\n23127.50\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n25918.80\n25949.83\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n20307.00\n20018.75\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n20023.80\n19524.17\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n16576.80\n18955.00\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n18667.00\n17800.40\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n14394.67\n15883.00\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n19848.80\n18831.33\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n15516.33\n14832.50\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518.00\n17965.00\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n17572.00\n17159.89\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n15200.12\n16199.44\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n18413.80\n18764.50\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n14419.33\n26878.75\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n24094.50\n23188.86\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n22019.83\n20788.14\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n12923.50\n12365.20\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n14756.00\n15985.00\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n13869.80\n13764.83\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n12296.67\n11907.43\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n15775.17\n17128.14\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n14382.86\n14593.62\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n11566.33\n11644.29\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n13199.50\n12706.00\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n23412.00\n21712.29\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n39541.00\n43548.25\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n36186.60\n35049.00\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n16559.60\n16226.83\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n20772.50\n19294.40\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n19471.20\n18156.00\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n19827.33\n19954.75\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n15466.80\n18145.17\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n12925.67\n12132.75\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n18577.17\n18419.29\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n14943.00\n14050.83\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n24913.00\n23619.75\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n25093.00\n24552.71\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n24428.80\n24733.67\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n17003.00\n16762.60\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n21143.75\n20932.60\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n20435.00\n19467.75\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n17131.33\n18334.00\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n24569.75\n22541.00\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n23835.50\n26028.00\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360.00\n29128.50\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n47383.40\n46569.00\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n55157.75\n47576.60\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n37058.00\n36545.50\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n21546.67\n20838.50\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n23348.67\n22531.00\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n42323.67\n42115.50\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n28938.60\n27619.00\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n25880.80\n27611.33\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n47345.67\n44523.29\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n18711.33\n18127.43\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n29087.29\n28746.38\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n20748.29\n20734.50\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n35933.71\n33880.62\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n15439.71\n14716.38\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n29787.50\n28516.22\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n18145.00\n18086.14\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n21617.00\n21244.50\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n29203.89\n29568.80\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n41363.67\n48119.71\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n22259.09\n22310.75\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n44939.56\n43151.60\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n16902.00\n17133.40\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n16930.00\n17009.33\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nLastly, we use qtm() of tmap package to plot the lag_gdppc and w_ave_gdppc maps next to each other for quick comparison.\n\n\nCode\nw_avg_gdppc &lt;- qtm(hunan, \"lag_window_avg.GDPPC\")\ntmap_arrange(lag_gdppc, w_avg_gdppc, asp = 1, ncol = 2)\n\n\n\n\n\nNote: For more effective comparison, we should use the core tmap mapping functions."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#spatial-window-sum",
    "href": "Hands-on_Ex/Hands-on_Ex04/Hands-on_Ex04.html#spatial-window-sum",
    "title": "Hands-on Exercise 4: Spatial Weights and Applications",
    "section": "7.4 Spatial window sum",
    "text": "7.4 Spatial window sum\nThe spatial window sum is the counter part of the window average, but without using row-standardized weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\n\nCode\nwm_qs &lt;- include.self(wm_q)\nwm_qs\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNext, we will assign binary weights to the neighbour structure that includes the diagonal element.\n\n\nCode\nb_weights &lt;- lapply(wm_qs, function(x) 0*x +1)\nb_weights[1]\n\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nNotice that now [1] has six neighbours instead of five.\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\n\n\nCode\nb_weights2 &lt;- nb2listw(wm_qs, \n                       glist = b_weights,\n                       style = \"B\")\nb_weights2\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nWith our new weight structure, we can compute the lag variable with lag.listw().\n\n\nCode\nw_sum_gdppc &lt;- list(hunan$NAME_3, lag.listw((b_weights2), hunan$GDPPC))\nw_sum_gdppc\n\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\nNext, we will convert the lag variable listw object into a data frame by using as.data.frame().\n\n\nCode\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) &lt;- c(\"NAME_3\", \"w_sum GDPPC\")\n\n\nNote: The second command line on the code chunk above renames the field names of w_sum_gdppc.res object into NAME_3 and w_sum GDPPC respectively.\nNext, the code chunk below will be used to append w_sum GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\n\nCode\nhunan &lt;- left_join(hunan, w_sum_gdppc.res)\n\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\n\nCode\nhunan %&gt;%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n174988\n238106\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n235079\n297281\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n273907\n344573\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n256221\n268982\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n98013\n106510\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n104050\n136141\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n102846\n126832\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n92017\n103303\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n133831\n151645\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n158446\n196097\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n141883\n207589\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n119508\n143926\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n150757\n178242\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n153324\n175235\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n113593\n138765\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n129594\n155699\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n142149\n160150\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n100119\n117145\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n82884\n113730\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n74668\n89002\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n43184\n63532\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n99244\n112988\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n46549\n59330\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518\n35930\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n140576\n154439\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n121601\n145795\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n92069\n112587\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n43258\n107515\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n144567\n162322\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n132119\n145517\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n51694\n61826\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n59024\n79925\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n69349\n82589\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n73780\n83352\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n94651\n119897\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n100680\n116749\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n69398\n81510\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n52798\n63530\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n140472\n151986\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n118623\n174193\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n180933\n210294\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n82798\n97361\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n83090\n96472\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n97356\n108936\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n59482\n79819\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n77334\n108871\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n38777\n48531\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n111463\n128935\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n74715\n84305\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n174391\n188958\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n150558\n171869\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n122144\n148402\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n68012\n83813\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n84575\n104663\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n143045\n155742\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n51394\n73336\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n98279\n112705\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n47671\n78084\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360\n58257\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n236917\n279414\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n220631\n237883\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n185290\n219273\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n64640\n83354\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n70046\n90124\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n126971\n168462\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n144693\n165714\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n129404\n165668\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n284074\n311663\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n112268\n126892\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n203611\n229971\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n145238\n165876\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n251536\n271045\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n108078\n117731\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n238300\n256646\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n108870\n126603\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n108085\n127467\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n262835\n295688\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n248182\n336838\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n244850\n267729\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n404456\n431516\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n67608\n85667\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n33860\n51028\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_sum GDPPC and w_sum_gdppc maps next to each other for a quick comparison.\n\n\nCode\nw_sum_gdppc &lt;- qtm(hunan, \"w_sum GDPPC\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp = 1, ncol = 2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#importing-the-spatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#importing-the-spatial-data",
    "title": "Hands-on Exercise 3: 1st Order Spatial Point Pattern Analysis Methods",
    "section": "1.4.1. Importing the spatial data",
    "text": "1.4.1. Importing the spatial data\nIn this section, we will use st_read() from the sf package to import the three geospatial data sets into R.\n\n\nCode\n#|eval: FALSE\nchildcare_sf &lt;- st_read(dsn = \"data/ChildCareServices.geojson\") %&gt;%\n  st_transform(crs=3414)\n\n\nReading layer `ChildCareServices' from data source \n  `/Users/binhui-ong/IS415-GAA/Hands-on_Ex/Hands-on_Ex03/data/ChildCareServices.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\nCode\n#|eval: FALSE\nsg_sf &lt;- st_read(dsn=\"data\", layer = \"MP14_SUBZONE_WEB_PL\") %&gt;% st_union()\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/binhui-ong/IS415-GAA/Hands-on_Ex/Hands-on_Ex03/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\nCode\n#|eval: FALSE\nmpsz_sf &lt;- st_read(dsn=\"data\", layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/binhui-ong/IS415-GAA/Hands-on_Ex/Hands-on_Ex03/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nBefore we use the data sets for analysis, we should check if they have all been correctly projected into the SVY21 coordinate system,\n\n\nCode\nst_crs(childcare_sf)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\nCode\nst_crs(sg_sf)\n\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\n\nCode\nst_crs(mpsz_sf)\n\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nSince the sg_sf and mpsz_sf data sets do not have the correct EPSG code, we can change them using st_set_crs().\n\n\nCode\nsg_sf&lt;- st_set_crs(sg_sf, 3414)\n\n\n\n\nCode\nmpsz_sf &lt;- st_set_crs(mpsz_sf, 3414)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#mapping-the-geospatial-data-sets",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#mapping-the-geospatial-data-sets",
    "title": "Hands-on Exercise 3: 1st Order Spatial Point Pattern Analysis Methods",
    "section": "1.4.2 Mapping the geospatial data sets",
    "text": "1.4.2 Mapping the geospatial data sets\nAfter checking the referencing system of each geospatial data frame, it is also useful for us to plot a map to show their spatial patterns.\n\n\nCode\ntm_shape(sg_sf) + tm_polygons() + \ntm_shape(mpsz_sf) + tm_polygons() + \ntm_shape(childcare_sf) + tm_dots()\n\n\n\n\n\nNotice that all the geospatial layers are within the same map extend. This shows that their referencing system and coordinate values are referred to similar spatial context. This is very important in any geospatial analysis.\nWe can create a pin map by using the following code chunk:\n\n\nCode\ntmap_mode('view') \ntm_shape(childcare_sf) + \n  tm_dots()\n\n\n\n\n\n\n\nIn the interactive mode, tmap is using leaflet for R API. This interactive pin map allows us to navigate and zoom around the map freely.\nAdditionally, we can query the information of each simple feature (i.e. the point) by clicking it.\nLastly, we can change the background of the internet map layer. Currently, three internet map layers are provided. They are: ESRI.WorldGrayCanvas, OpenStreetMap, and ESRI.WorldTopoMap. The default is ESRI.WorldGrayCanvas.\nThereafter, we switch back to plot mode as the interactive mode will consume a connection.\n\n\nCode\ntmap_mode('plot')"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-sf-data-frames-to-sps-spatial-class",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-sf-data-frames-to-sps-spatial-class",
    "title": "Hands-on Exercise 3: 1st Order Spatial Point Pattern Analysis Methods",
    "section": "1.5.1 Converting sf data frames to sp’s Spatial* class",
    "text": "1.5.1 Converting sf data frames to sp’s Spatial* class\nTo convert geospatial data from sf data frame to sp’s Spatial* class, we use as_Spatial() from sf package.\n\n\nCode\nchildcare &lt;- as_Spatial(childcare_sf)\nmpsz &lt;- as_Spatial(mpsz_sf)\nsg &lt;- as_Spatial(sg_sf)\n\n\nTo look into the information of the three new Spatial* classes,\n\n\nCode\nchildcare\n\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1925 \nextent      : 11810.03, 45404.24, 25596.33, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 2\nnames       :    Name,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Description \nmin values  :   kml_1, &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;100044&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;44, TELOK BLANGAH DRIVE, #01 - 19/51, SINGAPORE 100044&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;PCF SPARKLETOTS PRESCHOOL @ TELOK BLANGAH BLK 44 (CC)&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;349C54F201805938&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201093837&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \nmax values  : kml_999,                                            &lt;center&gt;&lt;table&gt;&lt;tr&gt;&lt;th colspan='2' align='center'&gt;&lt;em&gt;Attributes&lt;/em&gt;&lt;/th&gt;&lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSBLOCKHOUSENUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSBUILDINGNAME&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSPOSTALCODE&lt;/th&gt; &lt;td&gt;99982&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSSTREETNAME&lt;/th&gt; &lt;td&gt;35, ALLANBROOKE ROAD, SINGAPORE 099982&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSTYPE&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;DESCRIPTION&lt;/th&gt; &lt;td&gt;Child Care Services&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;HYPERLINK&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;LANDXADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;LANDYADDRESSPOINT&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;NAME&lt;/th&gt; &lt;td&gt;ISLANDER PRE-SCHOOL PTE LTD&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;PHOTOURL&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;ADDRESSFLOORNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;INC_CRC&lt;/th&gt; &lt;td&gt;4F63ACF93EFABE7F&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"\"&gt; &lt;th&gt;FMEL_UPD_D&lt;/th&gt; &lt;td&gt;20211201093837&lt;/td&gt; &lt;/tr&gt;&lt;tr bgcolor=\"#E3E3F3\"&gt; &lt;th&gt;ADDRESSUNITNUMBER&lt;/th&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;&lt;/center&gt; \n\n\n\n\nCode\nmpsz\n\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 323 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 15\nnames       : OBJECTID, SUBZONE_NO, SUBZONE_N, SUBZONE_C, CA_IND, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C,          INC_CRC, FMEL_UPD_D,     X_ADDR,     Y_ADDR,    SHAPE_Leng,    SHAPE_Area \nmin values  :        1,          1, ADMIRALTY,    AMSZ01,      N, ANG MO KIO,         AM, CENTRAL REGION,       CR, 00F5E30B5C9B7AD8,      16409,  5092.8949,  19579.069, 871.554887798, 39437.9352703 \nmax values  :      323,         17,    YUNNAN,    YSSZ09,      Y,     YISHUN,         YS,    WEST REGION,       WR, FFCCF172717C2EAF,      16409, 50424.7923, 49552.7904, 68083.9364708,  69748298.792 \n\n\n\n\nCode\nsg\n\n\nclass       : SpatialPolygons \nfeatures    : 1 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-the-spatial-class-into-generic-sp-format",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-the-spatial-class-into-generic-sp-format",
    "title": "Hands-on Exercise 3: 1st Order Spatial Point Pattern Analysis Methods",
    "section": "1.5.2. Converting the Spatial* class into generic sp format",
    "text": "1.5.2. Converting the Spatial* class into generic sp format\nspatstat requires the analytical data in to be in ppp object form. As there is no direct way to convert a Spatial* classes into ppp object, we would need to convert the Spatial* classes into Spatial object first.\n\n\nCode\nchildcare_sp &lt;- as(childcare, \"SpatialPoints\")\nsg_sp &lt;- as(sg, \"SpatialPolygons\")\n\n\nThen, we should look into the sp objects properties:\n\n\nCode\nchildcare_sp\n\n\nclass       : SpatialPoints \nfeatures    : 1925 \nextent      : 11810.03, 45404.24, 25596.33, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\n\n\nCode\nsg_sp\n\n\nclass       : SpatialPolygons \nfeatures    : 1 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n\n\nWith the conversion from Spatial* class into generic sp format, the spatial data frames are converted into spatial objects."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-the-generic-sp-format-into-spatstats-ppp-format",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#converting-the-generic-sp-format-into-spatstats-ppp-format",
    "title": "Hands-on Exercise 3: 1st Order Spatial Point Pattern Analysis Methods",
    "section": "1.5.3 Converting the generic sp format into spatstat’s ppp format",
    "text": "1.5.3 Converting the generic sp format into spatstat’s ppp format\nNext, we will use as.ppp() from spatstat package to convert the spatial data into spatstat’s ppp object format:\n\n\nCode\nchildcare_ppp &lt;- as(childcare_sp, \"ppp\")\nchildcare_ppp\n\n\nPlanar point pattern: 1925 points\nwindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n\n\nHere, we can plot childcare_ppp to examine the difference.\n\n\nCode\nplot(childcare_ppp)\n\n\n\n\n\nTo have a quick understanding of the summary statistics of the newly created ppp object, we can use the following code:\n\n\nCode\nsummary(childcare_ppp)\n\n\nPlanar point pattern:  1925 points\nAverage intensity 2.417323e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n                    (33590 x 23700 units)\nWindow area = 796335000 square units\n\n\nFrom the above summary, we notice the warning message about duplicates. In spatial point patterns analysis, the presence of duplicates is a significant issue The statistical methodology used for spatial point patterns processes is based largely on the assumption that process are simple, that is, that the points cannot be coincident."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#handling-duplicated-points",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#handling-duplicated-points",
    "title": "Hands-on Exercise 3: 1st Order Spatial Point Pattern Analysis Methods",
    "section": "1.5.4. Handling duplicated points",
    "text": "1.5.4. Handling duplicated points\nWe can check for duplication in a ppp object by using the following code chunk:\n\n\nCode\nany(duplicated(childcare_ppp))\n\n\n[1] TRUE\n\n\nTo find out the number of coincident points, we use the multiplicity() function from spatstat package:\n\n\nCode\nmultiplicity(childcare_ppp)\n\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n   1    2    1    1    1    1    2    1    1    1    1    1    1    3    1    1 \n  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 \n   1    3    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n  33   34   35   36   37   38   39   40   41   42   43   44   45   46   47   48 \n   1    1    1    1    4    1    1    1    1    1    1    1    1    1    1    2 \n  49   50   51   52   53   54   55   56   57   58   59   60   61   62   63   64 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    2    1    1 \n  65   66   67   68   69   70   71   72   73   74   75   76   77   78   79   80 \n   1    3    1    1    1    2    1   10    1    1    1    1    1    1    1    1 \n  81   82   83   84   85   86   87   88   89   90   91   92   93   94   95   96 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n  97   98   99  100  101  102  103  104  105  106  107  108  109  110  111  112 \n   1    1    1    1    1    1    1    2    1    1    3    1    1    1    2    1 \n 113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128 \n   1    2    2    2    1    1    1    1    1    1    1    1    2    1    1    1 \n 129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144 \n   1    1    1    1    1    3    1    1    1    1    1    1    1    1    1    1 \n 145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176 \n   1    1    2    2    2    1    1    1    1    1    2    1    4    1    1    2 \n 177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192 \n   1    1    1    1    1    1    1    1    2    1    1    1    1    1    1    1 \n 193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208 \n   3    1    1    1    1    1    3    1    1    1    1    1    1    1    1    1 \n 209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224 \n   1    1    1    1    1   10    1    1    3    1    1    1    1    1    1    1 \n 225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  240 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n 241  242  243  244  245  246  247  248  249  250  251  252  253  254  255  256 \n   1    1    2    6    1    2    1    1    2    1    1    1    1    1    1    1 \n 257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272 \n   3    2    3    2    1    2    1    1    2    4    1    6    6    1    1    1 \n 273  274  275  276  277  278  279  280  281  282  283  284  285  286  287  288 \n   2    1    1    1    1    2    1    1    1    1    1    1    3    1    1    1 \n 289  290  291  292  293  294  295  296  297  298  299  300  301  302  303  304 \n   1    1    4    1    2    1    1    1    1    1    1    1    1    1    1    1 \n 305  306  307  308  309  310  311  312  313  314  315  316  317  318  319  320 \n   1    1    1    1    1    1    1    1    1    1    1    2    1    1    1    1 \n 321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  336 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 337  338  339  340  341  342  343  344  345  346  347  348  349  350  351  352 \n   1    1    2    1    1    1    2    1    1    1    2    1    1    1    1    1 \n 353  354  355  356  357  358  359  360  361  362  363  364  365  366  367  368 \n   1    1    1    1    2    1    2    2    1    1    1    1    2    1    1    1 \n 369  370  371  372  373  374  375  376  377  378  379  380  381  382  383  384 \n   4    1    1    1    1    2    1    1    1    1    1    1    2    1    1    1 \n 385  386  387  388  389  390  391  392  393  394  395  396  397  398  399  400 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    2 \n 401  402  403  404  405  406  407  408  409  410  411  412  413  414  415  416 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    4 \n 417  418  419  420  421  422  423  424  425  426  427  428  429  430  431  432 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 433  434  435  436  437  438  439  440  441  442  443  444  445  446  447  448 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n 449  450  451  452  453  454  455  456  457  458  459  460  461  462  463  464 \n   1    1    2    1    1    1    1    1    1    1    1    1    2    1    1    1 \n 465  466  467  468  469  470  471  472  473  474  475  476  477  478  479  480 \n   1    1    2    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 481  482  483  484  485  486  487  488  489  490  491  492  493  494  495  496 \n   2    2    1    1    1    1    1   10    1    2    1    1    1    2    1    3 \n 497  498  499  500  501  502  503  504  505  506  507  508  509  510  511  512 \n   1    1    1    1   10   10   10    1    1    1    1    1    1    1    1    1 \n 513  514  515  516  517  518  519  520  521  522  523  524  525  526  527  528 \n   1    1    1    2    1    2    1    1    1    1    3    1    2    1    1    1 \n 529  530  531  532  533  534  535  536  537  538  539  540  541  542  543  544 \n   1    1    1    1    1    1    3    1    1    1    1    1    2    1    1    2 \n 545  546  547  548  549  550  551  552  553  554  555  556  557  558  559  560 \n   1    1    3    1    1    1    1    1    1    1    1    2    2    2    1    1 \n 561  562  563  564  565  566  567  568  569  570  571  572  573  574  575  576 \n   2    3    1    1    1    2    1    1    1    2    2    1    1    1    1    1 \n 577  578  579  580  581  582  583  584  585  586  587  588  589  590  591  592 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    4    1    1 \n 593  594  595  596  597  598  599  600  601  602  603  604  605  606  607  608 \n   1    1    1    1    1    3    1    1    1    1    1    1    1    1    1    1 \n 609  610  611  612  613  614  615  616  617  618  619  620  621  622  623  624 \n   1    1    1    1    1    4    1    1    1    1    1    1    4    1    1    1 \n 625  626  627  628  629  630  631  632  633  634  635  636  637  638  639  640 \n   1    1    1    1    1    2    1    1    1    1    1    1    1    1    1    1 \n 641  642  643  644  645  646  647  648  649  650  651  652  653  654  655  656 \n   1    1    1    1    2    1    1    1    1    1    1    1    1    2    1    1 \n 657  658  659  660  661  662  663  664  665  666  667  668  669  670  671  672 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    3    1    1 \n 673  674  675  676  677  678  679  680  681  682  683  684  685  686  687  688 \n   1    1    1    1    1    1    1    1    1   10    1    1    1    1    1    2 \n 689  690  691  692  693  694  695  696  697  698  699  700  701  702  703  704 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 705  706  707  708  709  710  711  712  713  714  715  716  717  718  719  720 \n   1    1    1    2    1    2    1   10    1    4    1    2    1    1    1    1 \n 721  722  723  724  725  726  727  728  729  730  731  732  733  734  735  736 \n   3    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 737  738  739  740  741  742  743  744  745  746  747  748  749  750  751  752 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 753  754  755  756  757  758  759  760  761  762  763  764  765  766  767  768 \n   1    3    1    1    3    1    1    1    1    2    1    1    1    1    1    1 \n 769  770  771  772  773  774  775  776  777  778  779  780  781  782  783  784 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 785  786  787  788  789  790  791  792  793  794  795  796  797  798  799  800 \n   1    1    1    1    1    1    1    1    1    1    2    1    1    1    1    1 \n 801  802  803  804  805  806  807  808  809  810  811  812  813  814  815  816 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 817  818  819  820  821  822  823  824  825  826  827  828  829  830  831  832 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 833  834  835  836  837  838  839  840  841  842  843  844  845  846  847  848 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 849  850  851  852  853  854  855  856  857  858  859  860  861  862  863  864 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 865  866  867  868  869  870  871  872  873  874  875  876  877  878  879  880 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n 881  882  883  884  885  886  887  888  889  890  891  892  893  894  895  896 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    2 \n 897  898  899  900  901  902  903  904  905  906  907  908  909  910  911  912 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n 913  914  915  916  917  918  919  920  921  922  923  924  925  926  927  928 \n   1    1    2    1    1    1    1    1    2    2    1    1    1    1    2    1 \n 929  930  931  932  933  934  935  936  937  938  939  940  941  942  943  944 \n   1    1    2    1    2    1    1    1    2    1    1    1    2    1    1    1 \n 945  946  947  948  949  950  951  952  953  954  955  956  957  958  959  960 \n   1    1    2    1    1    2    1    1    1    1    1    1    1    1    2    1 \n 961  962  963  964  965  966  967  968  969  970  971  972  973  974  975  976 \n   1    2    2    1    1    1    1    2    1    1    1    1    2    1    1    2 \n 977  978  979  980  981  982  983  984  985  986  987  988  989  990  991  992 \n   1    1    1    1    2    1    1    1    1    1    1    1    1    1    1    1 \n 993  994  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 \n   1    1    1    2    4    1    1    1    1    1    1    2    1    2    2    2 \n1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 \n   2    1    1    1    1    2    1    1    2    2    2    2    1    1    1    1 \n1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 \n   2    1    1    1    2    1    2    1    1    1    1    1    1    1    1    1 \n1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 \n   1    2    2    2    1    1    1    1    1    2    1    1    2    2    2    1 \n1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 \n   1    1    1    1    2    1    1    2    1    1    1    1    1    1    1    1 \n1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 \n   1    3    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 \n   2    1    2    1    2    1    1    1    1    1    1    2    2    1    1    2 \n1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 \n   1    2    1    2    1    2    1    1    1    1    1    2    1    1    1    1 \n1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 \n   1    2    1    2    2    2    2    2    1    1    1    1    1    2    1    1 \n1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 \n   1    1    1    1    1    2    1    1    2    1    1    1    1    2    1    1 \n1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 \n   1    2    1    1    1    1    2    1    1    1    1    1    1    1    1    1 \n1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 \n   1    1    1    1    1    1    1    1    1    2    1    1    1    1    1    1 \n1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 \n   1    1    1    2    1    1    1    3    1    1    1    1    1    1    1   10 \n1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 \n   2    1    3    2    1    2    1    1    2    3    2    1    1    1    1    1 \n1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 \n   1    1    1    1    1    2    1    2    1    1    1    1    1    1    1    1 \n1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 \n   1    1    1    1    1    1    1    1    1    1    4    1    1    1    1    1 \n1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 \n   2    1    1    1    2    1    2    1    1    1    1    1    1    1    1    1 \n1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 \n  10    1    2    4    1    1    1    4    1    4    1    1    1    1    1    1 \n1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 \n   1    1    1    1    1    1    1    1    1    4    2    3    2    1    1    1 \n1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 \n   2    2    1    1    1    1    1    2    2    3    1    1    1    1    1    2 \n1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 \n   1    1    1    2    1    1    1    1    1    1    1    1    1    1    1    1 \n1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 \n   2    2    2    1    1    1    6    1    1    1    1    1    1    1    1    1 \n1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 \n   1    1    1    4    1    1    1    1    1    1    1    1    1    1    1    1 \n1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 \n   1    1    1    1    2    2    1    1    1    1    1    1    1    1    1    1 \n1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 \n   1    1    1    1    2    1    1    1    1    2    1    1    1    1    2    1 \n1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 \n   2    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 \n   2    1    1    1    1    1    1    3    1    1    1    1    1    1    1    1 \n1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 \n   1    1    1    1    1    1    1    1    1    6    1    1    1    1    1    1 \n1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 \n   1    1    1    1    1    1    1    3    1    1    4    1    1    2    1    1 \n1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 \n   2    1    1    1    2    1    4    1    2    1    1    1    1    1    1    1 \n1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 \n   1    1    1    1    1    1    1    1    2    1    1    2    1    1    1    1 \n1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 \n   1    1    1    1    2    1    1    3    1    1    1    2    1    1    1    1 \n1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 \n   2    1    1    1    1    1    1    2    1    1    2    1    1    1    1    1 \n1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 \n   3    1    1    2    1    1    1    1    1    1    1    1    1    2    1    1 \n1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 \n   1    1    1    1    1    1    1    2    1    1    1    1    1    1    1    1 \n1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 \n   1    1    1    4    1    1    1    6    1    1    1    1    1    1    1    1 \n1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 \n   1    1    1    2    1    1    1    2    1    1    1    1    1    2    1    1 \n1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 \n   1    2    1    1    1    1    1    1    1    1    2    2    2    1    1    1 \n1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 \n   2    1    2    1    2    1    2    1    1    2    1    2    2    2    2    1 \n1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 \n   1    1    1    1    1    2    1    1    1    2    1    1    1    1    2    1 \n1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 \n   1    4    1    4    1    4    1    1    2    1    1    1    1    1    3    1 \n1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 \n   1    1    1    2    2    2    2    2    2    2    2    1    1    2    2    2 \n1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 \n   1    2    1    1    1    1    1    2    2    2    1    2    2    2    2    1 \n1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 \n   2    1    1    1    1    1    1    1    2    2    1    2    1    1    1    1 \n1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 \n   1    1    1    1    2    1    2    2    2    2    2    2    1    1    2    1 \n1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 \n   1    1    1    2    2    2    2    2    1    1    1    2    1    1    2    2 \n1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 \n   1    2    1    1    2    1    1    2    2    2    1    2    1    2    1    1 \n1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 \n   1    1    1    1    1    1    2    1    1    1    1    4    1    1    1    1 \n1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 \n   3    1    1    2    1    1    1    2    1    1    1    1    1    2    2    1 \n1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 \n   1    1    2    1    2    2    1    1    1    1    1    2    1    1    2    1 \n1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 \n   1    3    2    2    2    1    2    1    3    1    1    1    1    1    1    1 \n1921 1922 1923 1924 1925 \n   1    1    1    1    3 \n\n\nTo find out how many locations have more than one point event, we can use the code:\n\n\nCode\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n\n[1] 338\n\n\nThe output shows that there are 338 duplicated point events.\nTo view the location of these duplicate point events, we will plot childcare data with the following code chunk:\n\n\nCode\ntmap_mode('view')\ntm_shape(childcare) + \n  tm_dots(alpha=0.4, \n          size = 0.05)\n\n\n\n\n\n\n\nFrom the graph above, we can spot the duplicate point events as the duplicate points overlap, resulting in darker points in the map.\nAfter plotting the graph, we adjust our tmap_mode back to static setting:\n\n\nCode\ntmap_mode('plot')\n\n\nTo overcome the issue of duplicate points, we can use any of the following 3 solutions:\n\nDelete the duplicates (however, this would lead to the loss of some useful point events)\nJittering: To add a small pertubation to the duplicate points, so that they do not occupy the exact same space\nMake each point “unique”, and attach the duplicates of the points to the patterns as marks, as attributes of the points. Then, we would need analytical techniques that take into account these marks.\n\n\n\nCode\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp, \n                             retry=TRUE,\n                             nsim = 1,\n                             drop = TRUE)\n\n\n\n\nCode\nany(duplicated(childcare_ppp_jit))\n\n\n[1] FALSE"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#creating-owin-object",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#creating-owin-object",
    "title": "Hands-on Exercise 3: 1st Order Spatial Point Pattern Analysis Methods",
    "section": "1.5.5 Creating owin object",
    "text": "1.5.5 Creating owin object\nWhen analysing spatial point patterns, it is a good practice to confine the analysis within a geographical area like Singapore’s boundary. In spatstat, an object called owin is designed to represent this polygonal region.\nThe code chunk below is used to convert sg sf object into owin object of spatstat.\n\n\nCode\nsg_owin &lt;- as.owin(sg_sf)\n\n\nThe output object can be displayed by using the plot() function\n\n\nCode\nplot(sg_owin)\n\n\n\n\n\nand summary() function of Base R.\n\n\nCode\nsummary(sg_owin)\n\n\nWindow: polygonal boundary\n80 separate polygons (35 holes)\n                  vertices         area relative.area\npolygon 1            14650  6.97996e+08      8.93e-01\npolygon 2 (hole)         3 -2.21090e+00     -2.83e-09\npolygon 3              285  1.61128e+06      2.06e-03\npolygon 4 (hole)         3 -2.05920e-03     -2.63e-12\npolygon 5 (hole)         3 -8.83647e-03     -1.13e-11\npolygon 6              668  5.40368e+07      6.91e-02\npolygon 7               44  2.26577e+03      2.90e-06\npolygon 8               27  1.50315e+04      1.92e-05\npolygon 9              711  1.28815e+07      1.65e-02\npolygon 10 (hole)       36 -4.01660e+04     -5.14e-05\npolygon 11 (hole)      317 -5.11280e+04     -6.54e-05\npolygon 12 (hole)        3 -3.41405e-01     -4.37e-10\npolygon 13 (hole)        3 -2.89050e-05     -3.70e-14\npolygon 14              77  3.29939e+05      4.22e-04\npolygon 15              30  2.80002e+04      3.58e-05\npolygon 16 (hole)        3 -2.83151e-01     -3.62e-10\npolygon 17              71  8.18750e+03      1.05e-05\npolygon 18 (hole)        3 -1.68316e-04     -2.15e-13\npolygon 19 (hole)       36 -7.79904e+03     -9.97e-06\npolygon 20 (hole)        4 -2.05611e-02     -2.63e-11\npolygon 21 (hole)        3 -2.18000e-06     -2.79e-15\npolygon 22 (hole)        3 -3.65501e-03     -4.67e-12\npolygon 23 (hole)        3 -4.95057e-02     -6.33e-11\npolygon 24 (hole)        3 -3.99521e-02     -5.11e-11\npolygon 25 (hole)        3 -6.62377e-01     -8.47e-10\npolygon 26 (hole)        3 -2.09065e-03     -2.67e-12\npolygon 27              91  1.49663e+04      1.91e-05\npolygon 28 (hole)       26 -1.25665e+03     -1.61e-06\npolygon 29 (hole)      349 -1.21433e+03     -1.55e-06\npolygon 30 (hole)       20 -4.39069e+00     -5.62e-09\npolygon 31 (hole)       48 -1.38338e+02     -1.77e-07\npolygon 32 (hole)       28 -1.99862e+01     -2.56e-08\npolygon 33              40  1.38607e+04      1.77e-05\npolygon 34 (hole)       40 -6.00381e+03     -7.68e-06\npolygon 35 (hole)        7 -1.40545e-01     -1.80e-10\npolygon 36 (hole)       12 -8.36709e+01     -1.07e-07\npolygon 37              45  2.51218e+03      3.21e-06\npolygon 38             142  3.22293e+03      4.12e-06\npolygon 39             148  3.10395e+03      3.97e-06\npolygon 40              75  1.73526e+04      2.22e-05\npolygon 41              83  5.28920e+03      6.76e-06\npolygon 42             211  4.70521e+05      6.02e-04\npolygon 43             106  3.04104e+03      3.89e-06\npolygon 44             266  1.50631e+06      1.93e-03\npolygon 45              71  5.63061e+03      7.20e-06\npolygon 46              10  1.99717e+02      2.55e-07\npolygon 47             478  2.06120e+06      2.64e-03\npolygon 48             155  2.67502e+05      3.42e-04\npolygon 49            1027  1.27782e+06      1.63e-03\npolygon 50 (hole)        3 -1.16959e-03     -1.50e-12\npolygon 51              65  8.42861e+04      1.08e-04\npolygon 52              47  3.82087e+04      4.89e-05\npolygon 53               6  4.50259e+02      5.76e-07\npolygon 54             132  9.53357e+04      1.22e-04\npolygon 55 (hole)        3 -3.23310e-04     -4.13e-13\npolygon 56               4  2.69313e+02      3.44e-07\npolygon 57 (hole)        3 -1.46474e-03     -1.87e-12\npolygon 58            1045  4.44510e+06      5.68e-03\npolygon 59              22  6.74651e+03      8.63e-06\npolygon 60              64  3.43149e+04      4.39e-05\npolygon 61 (hole)        3 -1.98390e-03     -2.54e-12\npolygon 62 (hole)        4 -1.13774e-02     -1.46e-11\npolygon 63              14  5.86546e+03      7.50e-06\npolygon 64              95  5.96187e+04      7.62e-05\npolygon 65 (hole)        4 -1.86410e-02     -2.38e-11\npolygon 66 (hole)        3 -5.12482e-03     -6.55e-12\npolygon 67 (hole)        3 -1.96410e-03     -2.51e-12\npolygon 68 (hole)        3 -5.55856e-03     -7.11e-12\npolygon 69             234  2.08755e+06      2.67e-03\npolygon 70              10  4.90942e+02      6.28e-07\npolygon 71             234  4.72886e+05      6.05e-04\npolygon 72 (hole)       13 -3.91907e+02     -5.01e-07\npolygon 73              15  4.03300e+04      5.16e-05\npolygon 74             227  1.10308e+06      1.41e-03\npolygon 75              10  6.60195e+03      8.44e-06\npolygon 76              19  3.09221e+04      3.95e-05\npolygon 77             145  9.61782e+05      1.23e-03\npolygon 78              30  4.28933e+03      5.49e-06\npolygon 79              37  1.29481e+04      1.66e-05\npolygon 80               4  9.47108e+01      1.21e-07\nenclosing rectangle: [2667.54, 56396.44] x [15748.72, 50256.33] units\n                     (53730 x 34510 units)\nWindow area = 781945000 square units\nFraction of frame area: 0.422"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#combining-point-events-object-and-owin-object",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#combining-point-events-object-and-owin-object",
    "title": "Hands-on Exercise 3: 1st Order Spatial Point Pattern Analysis Methods",
    "section": "1.5.6 Combining point events object and owin object",
    "text": "1.5.6 Combining point events object and owin object\nIn this last step of geospatial data wrangling, we will extract childcare events that are located within Singapore by using the code chunk below.\n\n\nCode\nchildcareSG_ppp &lt;- childcare_ppp[sg_owin]\n\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\n\nCode\nsummary(childcareSG_ppp)\n\n\nPlanar point pattern:  1925 points\nAverage intensity 2.461811e-06 points per square unit\n\n*Pattern contains duplicated points*\n\nCoordinates are given to 3 decimal places\ni.e. rounded to the nearest multiple of 0.001 units\n\nWindow: polygonal boundary\n80 separate polygons (35 holes)\n                  vertices         area relative.area\npolygon 1            14650  6.97996e+08      8.93e-01\npolygon 2 (hole)         3 -2.21090e+00     -2.83e-09\npolygon 3              285  1.61128e+06      2.06e-03\npolygon 4 (hole)         3 -2.05920e-03     -2.63e-12\npolygon 5 (hole)         3 -8.83647e-03     -1.13e-11\npolygon 6              668  5.40368e+07      6.91e-02\npolygon 7               44  2.26577e+03      2.90e-06\npolygon 8               27  1.50315e+04      1.92e-05\npolygon 9              711  1.28815e+07      1.65e-02\npolygon 10 (hole)       36 -4.01660e+04     -5.14e-05\npolygon 11 (hole)      317 -5.11280e+04     -6.54e-05\npolygon 12 (hole)        3 -3.41405e-01     -4.37e-10\npolygon 13 (hole)        3 -2.89050e-05     -3.70e-14\npolygon 14              77  3.29939e+05      4.22e-04\npolygon 15              30  2.80002e+04      3.58e-05\npolygon 16 (hole)        3 -2.83151e-01     -3.62e-10\npolygon 17              71  8.18750e+03      1.05e-05\npolygon 18 (hole)        3 -1.68316e-04     -2.15e-13\npolygon 19 (hole)       36 -7.79904e+03     -9.97e-06\npolygon 20 (hole)        4 -2.05611e-02     -2.63e-11\npolygon 21 (hole)        3 -2.18000e-06     -2.79e-15\npolygon 22 (hole)        3 -3.65501e-03     -4.67e-12\npolygon 23 (hole)        3 -4.95057e-02     -6.33e-11\npolygon 24 (hole)        3 -3.99521e-02     -5.11e-11\npolygon 25 (hole)        3 -6.62377e-01     -8.47e-10\npolygon 26 (hole)        3 -2.09065e-03     -2.67e-12\npolygon 27              91  1.49663e+04      1.91e-05\npolygon 28 (hole)       26 -1.25665e+03     -1.61e-06\npolygon 29 (hole)      349 -1.21433e+03     -1.55e-06\npolygon 30 (hole)       20 -4.39069e+00     -5.62e-09\npolygon 31 (hole)       48 -1.38338e+02     -1.77e-07\npolygon 32 (hole)       28 -1.99862e+01     -2.56e-08\npolygon 33              40  1.38607e+04      1.77e-05\npolygon 34 (hole)       40 -6.00381e+03     -7.68e-06\npolygon 35 (hole)        7 -1.40545e-01     -1.80e-10\npolygon 36 (hole)       12 -8.36709e+01     -1.07e-07\npolygon 37              45  2.51218e+03      3.21e-06\npolygon 38             142  3.22293e+03      4.12e-06\npolygon 39             148  3.10395e+03      3.97e-06\npolygon 40              75  1.73526e+04      2.22e-05\npolygon 41              83  5.28920e+03      6.76e-06\npolygon 42             211  4.70521e+05      6.02e-04\npolygon 43             106  3.04104e+03      3.89e-06\npolygon 44             266  1.50631e+06      1.93e-03\npolygon 45              71  5.63061e+03      7.20e-06\npolygon 46              10  1.99717e+02      2.55e-07\npolygon 47             478  2.06120e+06      2.64e-03\npolygon 48             155  2.67502e+05      3.42e-04\npolygon 49            1027  1.27782e+06      1.63e-03\npolygon 50 (hole)        3 -1.16959e-03     -1.50e-12\npolygon 51              65  8.42861e+04      1.08e-04\npolygon 52              47  3.82087e+04      4.89e-05\npolygon 53               6  4.50259e+02      5.76e-07\npolygon 54             132  9.53357e+04      1.22e-04\npolygon 55 (hole)        3 -3.23310e-04     -4.13e-13\npolygon 56               4  2.69313e+02      3.44e-07\npolygon 57 (hole)        3 -1.46474e-03     -1.87e-12\npolygon 58            1045  4.44510e+06      5.68e-03\npolygon 59              22  6.74651e+03      8.63e-06\npolygon 60              64  3.43149e+04      4.39e-05\npolygon 61 (hole)        3 -1.98390e-03     -2.54e-12\npolygon 62 (hole)        4 -1.13774e-02     -1.46e-11\npolygon 63              14  5.86546e+03      7.50e-06\npolygon 64              95  5.96187e+04      7.62e-05\npolygon 65 (hole)        4 -1.86410e-02     -2.38e-11\npolygon 66 (hole)        3 -5.12482e-03     -6.55e-12\npolygon 67 (hole)        3 -1.96410e-03     -2.51e-12\npolygon 68 (hole)        3 -5.55856e-03     -7.11e-12\npolygon 69             234  2.08755e+06      2.67e-03\npolygon 70              10  4.90942e+02      6.28e-07\npolygon 71             234  4.72886e+05      6.05e-04\npolygon 72 (hole)       13 -3.91907e+02     -5.01e-07\npolygon 73              15  4.03300e+04      5.16e-05\npolygon 74             227  1.10308e+06      1.41e-03\npolygon 75              10  6.60195e+03      8.44e-06\npolygon 76              19  3.09221e+04      3.95e-05\npolygon 77             145  9.61782e+05      1.23e-03\npolygon 78              30  4.28933e+03      5.49e-06\npolygon 79              37  1.29481e+04      1.66e-05\npolygon 80               4  9.47108e+01      1.21e-07\nenclosing rectangle: [2667.54, 56396.44] x [15748.72, 50256.33] units\n                     (53730 x 34510 units)\nWindow area = 781945000 square units\nFraction of frame area: 0.422\n\n\nTo plot the newly derived childcareSG_ppp map,\n\n\nCode\nplot(childcareSG_ppp)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#kernel-density-estimation-kde",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#kernel-density-estimation-kde",
    "title": "Hands-on Exercise 3: 1st Order Spatial Point Pattern Analysis Methods",
    "section": "1.6.1 Kernel Density Estimation (KDE)",
    "text": "1.6.1 Kernel Density Estimation (KDE)\nIn this section, we will learn how to compute the kernel density estimation (KDE) of childcare services in Singapore.\n\n1.6.1.1 Computing KDE using automatic bandwidth selection method\nThe code chunk below computes a Kernel Density by using the following configurations of density() of spatstat:\n\nAutomatic bandwidth selection method: bw.diggle()\n\nOther recommended methods: bw.CvL(), bw.scott() or bw.ppl()\n\nSmoothing kernel: “gaussian”\n\nOther smoothing methods: “epanechnikov”, “quartic” or “disc”\n\nThe intensity estimate is corrected for edge effect bias by using method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\n\n\nCode\nkde_childcareSG_bw &lt;- density(childcareSG_ppp,\n                              sigma = bw.diggle,\n                              edge = TRUE,\n                              kernel = \"gaussian\")\n\n\nThe plot() function of Base R is then used to display the kernel density derived.\n\n\nCode\nplot(kde_childcareSG_bw)\n\n\n\n\n\nThe density values of the output range from 0 to 0.00003 which is way too small to comprehend. This is because the default unit of measurement of svy21 is in meter. As a result, the density values computed is in “number of points per square meter”.\nBefore we move on to next section, it is good to know that you can retrieve the bandwidth used to compute the kde layer by using the code chunk below.\n\n\nCode\nbw &lt;- bw.diggle(childcareSG_ppp)\nbw\n\n\n   sigma \n295.4419 \n\n\n\n\n1.6.1.2 Rescaling KDE values\nIn the following code chunk, rescale() is used to convert the unit of measurement from meter to kilometer.\n\n\nCode\nchildcareSG_ppp.km &lt;- rescale (childcareSG_ppp, 1000, \"km\")\n\n\nNow, we can re-run density() using the rescaled data set and plot the output kde map.\n\n\nCode\nkde_childcareSG.bw.km &lt;- density(childcareSG_ppp.km,\n                                 sigma = bw.diggle,\n                                 edge = TRUE,\n                                 kernel = \"gaussian\")\nplot(kde_childcareSG.bw.km)\n\n\n\n\n\nNow, we can see that the output image looks identical to the earlier version, but with more intepretable data values in the legend.\n\n\n1.6.1.3 Working with different automatic bandwidth methods\nBeside bw.diggle(), I mentioned that there are three other spatstat functions that can be used to determine the bandwidth, which are: bw.CvL(), bw.scott(), and bw.ppl().\nLet us take a look at the bandwidth return by these automatic bandwidth calculation methods by using the code chunk below.\n\n\nCode\nbw.CvL(childcareSG_ppp.km)\n\n\n  sigma \n4.54311 \n\n\n\n\nCode\nbw.scott(childcareSG_ppp.km)\n\n\n sigma.x  sigma.y \n2.159749 1.396455 \n\n\n\n\nCode\nbw.ppl(childcareSG_ppp.km)\n\n\n    sigma \n0.3897017 \n\n\nBaddeley et. (2016) suggested the use of the bw.ppl() algorithm because based on their experience, it tends to produce more appropriate values when the pattern consists predominantly of tight clusters.\nHowever, they also insist that if the purpose of a study is to detect a single tight cluster in the midst of random noise, the bw.diggle() method seems to work best.\nThe following code chunk is used to compare the difference in output using bw.diggle and bw.ppl() methods.\n\n\nCode\nkde_childcareSG.ppl &lt;- density(childcareSG_ppp.km,\n                               sigma = bw.ppl,\n                               edge = TRUE,\n                               kernel = \"gaussian\")\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw.km, main = \"bw.diggle\")\nplot(kde_childcareSG.ppl, main = \"bw.ppl\")\n\n\n\n\n\n\n\n1.6.1.4 Working with different kernel methods\nBy default, the kernel method used in density.ppp() is Gaussian. However, there are three other options, namely: Epanechnikov, Quartic and Dics.\nThe code chunk below will be used to compute 3 more kernel density estimations by using these 3 kernel functions.\n\n\nCode\npar(mfrow=c(2,2))\nplot(density(childcareSG_ppp.km,\n             sigma = bw.ppl,\n             edge = TRUE,\n             kernel = \"gaussian\"),\n     main = \"gaussian\")\nplot(density(childcareSG_ppp.km,\n             sigma = bw.ppl,\n             edge = TRUE,\n             kernel = \"epanechnikov\"),\n     main = \"Epanechnikov\")\nplot(density(childcareSG_ppp.km,\n             sigma = bw.ppl,\n             edge = TRUE,\n             kernel = \"quartic\"),\n     main = \"quartic\")\nplot(density(childcareSG_ppp.km,\n             sigma = bw.ppl,\n             edge = TRUE,\n             kernel = \"disc\"),\n     main = \"disc\")\n\n\n\n\n\n\n\n1.6.1.5 Fixed and Adaptive KDE\n\n1.6.1.5.1 Computing KDE by using fixed bandwidth\nNext, you will compute a KDE layer by defining a bandwidth of 600 meter.\nNotice that in the code chunk below, the sigma value used is 0.6. This is because the unit of measurement of childcareSG_ppp.km object is in kilometer, hence the 600m is 0.6km.\n\n\nCode\nkde_childcareSG_600 &lt;- density(childcareSG_ppp.km,\n                               sigma = 0.6, \n                               edge = TRUE,\n                               kernel = \"gaussian\")\nplot (kde_childcareSG_600)\n\n\n\n\n\n\n\n1.6.1.5.2 Computing KDE by using adaptive bandwidth\nFixed bandwidth method is very sensitive to highly skewed distribution of spatial point patterns over geographical units (for example, urban versus rural). One way to overcome this problem is by using adaptive bandwidth instead.\nIn this section, we will learn how to derive adaptive kernel density estimation by using density.adaptive() of spatstat.\n\n\nCode\nkde_childcareSG_adaptive &lt;- adaptive.density(childcareSG_ppp.km,\n                                             method = \"kernel\")\nplot(kde_childcareSG_adaptive)\n\n\n\n\n\nWe can compare the fixed and adaptive kernel density estimation outputs by using the code chunk below:\n\n\nCode\npar(mfrow=c(1,2))\nplot(kde_childcareSG.bw.km, main = \"Fixed bandwidth\")\nplot(kde_childcareSG_adaptive, main = \"Adaptive bandwidth\")\n\n\n\n\n\n\n\n\n1.6.1.6 Converting KDE output into grid object\nThe result is the same, but we convert it so that it is suitable for mapping purposes.\n\n\nCode\ngridded_kde_childcareSG_bw &lt;- as.SpatialGridDataFrame.im(kde_childcareSG.bw.km)\nspplot(gridded_kde_childcareSG_bw)\n\n\n\n\n\n\n\n1.6.1.7 Converting gridded output into raster\nNext, we will convert the gridded kernel density objects into RasterLayer object by using raster() of raster package.\n\n\nCode\nkde_childcareSG_bw_raster &lt;- raster(gridded_kde_childcareSG_bw)\n\n\nLet us take a look at the properties of kde_childcareSG_bw_raster RasterLayer object.\n\n\nCode\nkde_childcareSG_bw_raster\n\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.419757, 0.2695907  (x, y)\nextent     : 2.667538, 56.39644, 15.74872, 50.25633  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : v \nvalues     : -1.293897e-14, 37.27443  (min, max)\n\n\nNotice that the crs property is NA.\n\n\n1.6.1.8 Assigning projection systems\nSince the crs property of kde_childcareSG_bw_raster RasterLayer object is NA, we will include CRS information for it using the code below.\n\n\nCode\nprojection(kde_childcareSG_bw_raster) &lt;- CRS(\"+init=EPSG:3414\")\nkde_childcareSG_bw_raster\n\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 0.419757, 0.2695907  (x, y)\nextent     : 2.667538, 56.39644, 15.74872, 50.25633  (xmin, xmax, ymin, ymax)\ncrs        : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +units=m +no_defs \nsource     : memory\nnames      : v \nvalues     : -1.293897e-14, 37.27443  (min, max)\n\n\nNow, we see that the crs property is complete.\n\n\n1.6.1.9 Visualising the output in tmap\nFinally, we will display the raster in cartographic quality map using tmap package.\n\n\nCode\ntmap_mode(\"plot\")\ntm_shape (kde_childcareSG_bw_raster) + \n  tm_raster(\"v\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), \n            frame = FALSE)\n\n\n\n\n\nNotice that the raster values are encoded explicitly onto the raster pixel using the values in the “v” field.\n\n\n1.6.1.10 Comparing Spatial Point Patterns using KDE\nIn this section, we will learn to compare KDE of childcare at Punggol, Tampines, Choa Chu Kang and Jurong West planning areas.\n\n1.6.1.10.1 Extracting study area\nThe code chunk below will be used to extract the target planning areas.\n\n\nCode\npg &lt;- mpsz[mpsz@data$PLN_AREA_N == \"PUNGGOL\", ] \ntm &lt;- mpsz[mpsz@data$PLN_AREA_N == \"TAMPINES\", ]\nck &lt;- mpsz[mpsz@data$PLN_AREA_N == \"CHOA CHU KANG\", ] \njw &lt;- mpsz[mpsz@data$PLN_AREA_N == \"JURONG WEST\", ] \n\n\nPlotting target planning areas:\n\n\nCode\npar(mfrow = c(2,2))\nplot(pg, main = \"Punggol\")\nplot(tm, main = \"Tampines\")\nplot(ck, main= \"Choa Chu Kang\")\nplot(jw, main = \"Jurong West\")\n\n\n\n\n\n\n\n1.6.1.10.2 Converting the spatial point data frame into generic sp format\nNext, we will convert these SpatialPolygonsDataFrame layers into generic spatialpolygons layers.\n\n\nCode\npg_sp &lt;- as(pg,\"SpatialPolygons\")\ntm_sp &lt;-  as(tm,\"SpatialPolygons\")\nck_sp &lt;- as(ck, \"SpatialPolygons\")\njw_sp &lt;-  as(jw,\"SpatialPolygons\")\n\n\n\n\n1.6.1.10.3 Creating owin object\nNow, we will convert these SpatialPolygons objects into owin objects that is required by spatstat.\n\n\nCode\npg_owin &lt;- as(pg_sp, \"owin\")\ntm_owin &lt;- as(tm_sp, \"owin\")\nck_owin &lt;- as(ck_sp, \"owin\")\njw_owin &lt;- as(jw_sp, \"owin\")\n\n\n\n\n1.6.1.10.4 Combining childcare points and the study area\nBy using the code chunk below, we can extract the childcare centres that are within specific regions to conduct out analysis later on.\n\n\nCode\nchildcare_pg_ppp &lt;- childcare_ppp_jit[pg_owin]\nchildcare_tm_ppp &lt;- childcare_ppp_jit[tm_owin]\nchildcare_ck_ppp &lt;- childcare_ppp_jit[ck_owin]\nchildcare_jw_ppp &lt;- childcare_ppp_jit[jw_owin]\n\n\nNext, we use rescale() to transform the unit of measurement from metre to kilometre.\n\n\nCode\nchildcare_pg_ppp.km &lt;- rescale(childcare_pg_ppp, 1000, \"km\")\nchildcare_tm_ppp.km &lt;- rescale(childcare_tm_ppp, 1000, \"km\")\nchildcare_ck_ppp.km &lt;- rescale(childcare_ck_ppp, 1000, \"km\")\nchildcare_jw_ppp.km &lt;- rescale(childcare_jw_ppp, 1000, \"km\")\n\n\nThe code chunk below is used to plot these four study areas and the location of the childcare centres.\n\n\nCode\npar(mfrow=c(2,2))\nplot(childcare_pg_ppp.km, main = \"Punggol\")\nplot(childcare_tm_ppp.km, main = \"Tampines\")\nplot(childcare_ck_ppp.km, main = \"Choa Chu Kang\")\nplot(childcare_jw_ppp.km, main = \"Jurong West\")\n\n\n\n\n\n\n\n1.6.1.10.5 Computing KDE\nThe code chunk below will then be used to compute the KDE of these four planning areas. bw.diggle method is used to derive the bandwidth of each.\n\n\nCode\npar(mfrow=c(2,2))\nplot(density(childcare_pg_ppp.km,\n             sigma = bw.diggle,\n             edge = TRUE,\n             kernel = \"gaussian\"),\n     main = \"Punggol\")\nplot(density(childcare_tm_ppp.km,\n             sigma = bw.diggle,\n             edge = TRUE,\n             kernel = \"gaussian\"),\n     main = \"Tampines\")\nplot(density(childcare_ck_ppp.km,\n             sigma = bw.diggle,\n             edge = TRUE,\n             kernel = \"gaussian\"),\n     main = \"Choa Chu Kang\")\nplot(density(childcare_jw_ppp.km,\n             sigma = bw.diggle,\n             edge = TRUE,\n             kernel = \"gaussian\"),\n     main = \"Jurong West\")\n\n\n\n\n\n\n\n1.6.1.10.6 Computing fixed bandwidth KDE\nFor comparison purposes, we will use 250m as the bandwidth.\n\n\nCode\npar(mfrow=c(2,2))\nplot(density(childcare_pg_ppp.km,\n             sigma = 0.25,\n             edge = TRUE,\n             kernel = \"gaussian\"),\n     main = \"Punggol\")\nplot(density(childcare_tm_ppp.km,\n             sigma = 0.25,\n             edge = TRUE,\n             kernel = \"gaussian\"),\n     main = \"Tampines\")\nplot(density(childcare_ck_ppp.km,\n             sigma = 0.25,\n             edge = TRUE,\n             kernel = \"gaussian\"),\n     main = \"Choa Chu Kang\")\nplot(density(childcare_jw_ppp.km,\n             sigma = 0.25,\n             edge = TRUE,\n             kernel = \"gaussian\"),\n     main = \"Jurong West\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#nearest-neighbour-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#nearest-neighbour-analysis",
    "title": "Hands-on Exercise 3: 1st Order Spatial Point Pattern Analysis Methods",
    "section": "1.6.2 Nearest Neighbour Analysis",
    "text": "1.6.2 Nearest Neighbour Analysis\nIn this section, we will perform the Clark-Evans test of aggregation for a spatial point pattern by using clarkevans.test() of statspat.\nThe test hypotheses are:\n\nHo = The distribution of childcare services are randomly distributed.\nH1= The distribution of childcare services are not randomly distributed.\nThe 95% confident interval will be used.\n\n\n1.6.2.1 Testing spatial point patterns using Clark and Evans Test\n\n\nCode\nclarkevans.test(childcare_ppp,\n                correction = \"none\",\n                clipregion = \"sg_owin\",\n                alternative = c(\"clustered\"),\n                nsim = 99)\n\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_ppp\nR = 0.49084, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\nSince the p-value &lt; 0.05, we have sufficient evidence at 95% confidence level that the childcare services in Singapore are not randomly distributed.\n\n1.6.2.1.1 Clark and Evans Test: Choa Chu Kang Planning Area\nIn the code chunk below, clarkevans.test() of spatstat is used to performs Clark-Evans test of aggregation for childcare centre in Choa Chu Kang planning area.\n\n\nCode\nclarkevans.test(childcare_ck_ppp,\n                correction = \"none\",\n                clipregion = NULL,\n                alternative = c(\"two.sided\"),\n                nsim = 999)\n\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_ck_ppp\nR = 0.85587, p-value = 0.01769\nalternative hypothesis: two-sided\n\n\nSince the p-value &gt; 0.05, we do not have sufficient evidence at 95% confidence level that the childcare services in Choa Chu Kang are not randomly distributed.\n\n\n1.6.2.1.2 Clark and Evans Test: Tampines Planning Area\n\n\nCode\nclarkevans.test(childcare_tm_ppp,\n                correction = \"none\",\n                clipregion = NULL,\n                alternative = c(\"two.sided\"),\n                nsim = 999)\n\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcare_tm_ppp\nR = 0.69963, p-value = 5.118e-10\nalternative hypothesis: two-sided\n\n\nSince p-value &lt; 0.05, we have sufficient evidence at 95% confidence level that the childcare services in Tampines are not randomly distributed."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analyzing-spatial-point-process-using-g-function",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analyzing-spatial-point-process-using-g-function",
    "title": "Hands-on Exercise 3: 1st Order Spatial Point Pattern Analysis Methods",
    "section": "1.7.1 Analyzing Spatial Point Process using G-function",
    "text": "1.7.1 Analyzing Spatial Point Process using G-function\nThe G function measures the distribution of the distances from an arbitrary event to its nearest event.\nIn this section, we will learn how to compute G-function estimation by using Gest() of spatstat package. We will also learn how to perform Monta Carlo simulation test of Complete Spatial Randomness (CSR) using envelope() of spatstat package.\n\n1.7.1.1 Choa Chu Kang Planning Area\n\n1.7.1.1.1 Computing G-function estimation\nThe code chunk below is used to compute G-function using Gest() of spatstat package.\n\n\nCode\nG_CK &lt;- Gest(childcare_ck_ppp, \n             correction = \"border\")\nplot(G_CK, xlim=c(0,500))\n\n\n\n\n\n\n\n1.7.1.1.2 Performing Complete Spatial Randomness (CSR) test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted.\nThe hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with G-function\n\n\n\nCode\nG_CK.csr &lt;- envelope(childcare_ck_ppp, \n                     Gest, \n                     nsim = 999)\n\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\n\nCode\nplot(G_CK.csr)\n\n\n\n\n\nFrom the above plot, we can see that the estimated G function lies within the envelope. Hence, it is not statistically significant and we do not have sufficient evidence that the childcare services in Choa Chu Kang are not randomly distributed.\n\n\n\n1.7.1.2 Tampines Planning Area\n\n1.7.1.2.1 Computing G-function estimation\n\n\nCode\nG_TM &lt;- Gest(childcare_tm_ppp, \n             correction = \"best\")\nplot(G_TM)\n\n\n\n\n\n\n\n1.7.1.2.2 Performing Complete Spatial Randomness (CSR) test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted.\nThe hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with G-function\n\n\n\nCode\nG_TM.csr &lt;- envelope(childcare_tm_ppp, Gest,\n                     correction = \"all\",\n                     nsim = 999)\n\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\n\nCode\nplot(G_TM.csr)\n\n\n\n\n\nFrom the above plot, we can see that the some parts of the estimated G function lies outside the envelope. Hence, it is statistically significant and we have sufficient evidence that the childcare services in Tampines are not randomly distributed."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analyzing-spatial-point-process-using-f-function",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analyzing-spatial-point-process-using-f-function",
    "title": "Hands-on Exercise 3: 1st Order Spatial Point Pattern Analysis Methods",
    "section": "1.7.2 Analyzing Spatial Point Process using F-function",
    "text": "1.7.2 Analyzing Spatial Point Process using F-function\nThe F function estimates the empty space function F(r) or its hazard rate h(r) from a point pattern in a window of arbitrary shape. In this section, we will learn how to compute F-function estimation by using Fest() of spatstat package. We will also learn how to perform Monta Carlo simulation test using envelope() of spatstat package.\n\n1.7.2.1 Choa Chu Kang Planning Area\n\n1.7.2.1.1 Computing F-function estimation\n\n\nCode\nF_CK &lt;- Fest(childcare_ck_ppp)\nplot(F_CK)\n\n\n\n\n\n\n\n1.7.2.1.2 Performing Complete Spatial Randomness (CSR) test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted.\nThe hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with F-function\n\n\n\nCode\nF_CK.csr &lt;- envelope(childcare_ck_ppp, Fest, nsim = 999)\n\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\n\nCode\nplot(F_CK.csr)\n\n\n\n\n\nFrom the above plot, we can see that the estimated F function lies within the envelope. Hence, it is not statistically significant and we do not have sufficient evidence that the childcare services in Choa Chu Kang are not randomly distributed.\n\n\n\n1.7.2.2 Tampines Planning Area\n\n1.7.2.2.1 Computing F-function estimation\n\n\nCode\nF_TM &lt;- Fest(childcare_tm_ppp, \n                 correction = \"best\")\nplot(F_TM)\n\n\n\n\n\n\n\n1.7.2.2.2 Performing Complete Spatial Randomness (CSR) test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted.\nThe hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with F-function\n\n\n\nCode\nF_TM.csr &lt;- envelope(childcare_tm_ppp, Fest, nsim = 999)\n\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60..\n.......70.........80.........90.........100.........110.........120.........130\n.........140.........150.........160.........170.........180.........190........\n.200.........210.........220.........230.........240.........250.........260......\n...270.........280.........290.........300.........310.........320.........330....\n.....340.........350.........360.........370.........380.........390.........400..\n.......410.........420.........430.........440.........450.........460.........470\n.........480.........490.........500.........510.........520.........530........\n.540.........550.........560.........570.........580.........590.........600......\n...610.........620.........630.........640.........650.........660.........670....\n.....680.........690.........700.........710.........720.........730.........740..\n.......750.........760.........770.........780.........790.........800.........810\n.........820.........830.........840.........850.........860.........870........\n.880.........890.........900.........910.........920.........930.........940......\n...950.........960.........970.........980.........990........\n999.\n\nDone.\n\n\n\n\nCode\nplot(F_TM.csr)\n\n\n\n\n\nFrom the above plot, we can see that the some parts of the estimated F function lies outside the envelope. Hence, it is statistically significant and we have sufficient evidence that the childcare services in Tampines are not randomly distributed."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analying-spatial-point-process-using-k-function",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analying-spatial-point-process-using-k-function",
    "title": "Hands-on Exercise 3: 1st Order Spatial Point Pattern Analysis Methods",
    "section": "1.7.3 Analying Spatial Point Process using K-function",
    "text": "1.7.3 Analying Spatial Point Process using K-function\nK-function measures the number of events found up to a given distance of any particular event.\nIn this section, we will learn how to compute K-function estimates by using Kest() of spatstat package. We will also learn how to perform Monta Carlo simulation test using envelope() of spatstat package.\n\n1.7.3.1 Choa Chu Kang Planning Area\n\n1.7.3.1.1 Computing K-function estimate\n\n\nCode\nK_CK &lt;- Kest(childcare_ck_ppp, \n             correction = \"Ripley\")\nplot(K_CK, .-r~r, ylab = \"K(d) - r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n1.7.3.1.2 Performing Complete Spatial Randomness (CSR) test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted.\nThe hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\n\nCode\nK_CK.csr &lt;- envelope(childcare_ck_ppp, Kest, nsim = 99, rank =1, glocal = TRUE)\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\n\nCode\nplot(K_CK.csr, .-r~r, xlab=\"d\", ylab=\"K(d)-r\")\n\n\n\n\n\n\n\n\n1.7.3.2 Tampines Planning Area\n\n1.7.3.2.1 Computing K-function estimates\n\n\nCode\nK_TM &lt;- Kest(childcare_tm_ppp, correction = \"Ripley\")\nplot(K_TM, . -r~r,\n     ylab = \"K(d) -r\",\n     xlab = \"d(m)\",\n     xlim = c(0,1000))\n\n\n\n\n\n\n\n1.7.3.2.2 Performing Complete Spatial Randomness (CSR) test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted.\nThe hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\n\nCode\nK_TM.csr &lt;- envelope(childcare_tm_ppp, Kest, nsim = 99, rank = 1, glocal = TRUE)\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\n\nCode\nplot(K_TM.csr, .-r~r, xlab= \"d\", ylab = \"K(d)-r\", xlim = c(0,500))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analyzing-spatial-point-process-using-l-function",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#analyzing-spatial-point-process-using-l-function",
    "title": "Hands-on Exercise 3: 1st Order Spatial Point Pattern Analysis Methods",
    "section": "1.7.4 Analyzing Spatial Point Process using L-function",
    "text": "1.7.4 Analyzing Spatial Point Process using L-function\nIn this section, we will learn how to compute L-function estimation by using Lest() of spatstat package. We will also learn how to perform Monta Carlo simulation test using envelope() of spatstat package.\n\n1.7.4.1 Choa Chu Kang Planning Area\n\n1.7.4.1.1 Computing L-function estimation\n\n\nCode\nL_CK &lt;- Lest(childcare_ck_ppp, correction = \"Ripley\")\nplot(L_CK, .-r~r, \n     ylab=\"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\n\n\n1.7.4.1.2 Performing Complete Spatial Randomness (CSR) test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted.\nThe hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Choa Chu Kang are randomly distributed.\nH1= The distribution of childcare services at Choa Chu Kang are not randomly distributed.\nThe null hypothesis will be rejected if p-value if smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\n\nCode\nL_CK.csr &lt;- envelope(childcare_ck_ppp, Lest, nsim = 99, rank =1, glocal = TRUE)\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\n\nCode\nplot(L_CK.csr, .-r~r, \n     xlab = \"d\", ylab = \"L(d)-r\")\n\n\n\n\n\n\n\n\n1.7.4.2 Tampines Planning Area\n\n1.7.4.2.1 Computing L-function estimate\n\n\nCode\nL_TM &lt;- Lest(childcare_tm_ppp, correction = \"Ripley\")\nplot(L_TM, .-r~r, \n     ylab = \"L(d)-r\", xlab = \"d(m)\",\n     xlim = c(0,1000))\n\n\n\n\n\n\n\n1.7.4.2.2 Performing Complete Spatial Randomness (CSR) test\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted.\nThe hypothesis and test are as follows:\n\nHo = The distribution of childcare services at Tampines are randomly distributed.\nH1= The distribution of childcare services at Tampines are not randomly distributed.\nThe null hypothesis will be rejected if p-value if smaller than alpha value of 0.001.\n\nThe code chunk below is used to perform the hypothesis testing.\n\n\nCode\nL_TM.csr &lt;- envelope(childcare_tm_ppp, Lest, nsim = 99, rank = 1, glocal = TRUE)\n\n\nGenerating 99 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n99.\n\nDone.\n\n\n\n\nCode\nplot(L_TM.csr, .-r~r, \n     xlab = \"d\", ylab = \"L(d)-r\", xlim = c(0,500))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#preparing-the-lixels-objects",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#preparing-the-lixels-objects",
    "title": "Hands-on Exercise 3: 1st Order Spatial Point Pattern Analysis Methods",
    "section": "2.6.1 Preparing the lixels objects",
    "text": "2.6.1 Preparing the lixels objects\nBefore computing NetKDE, the SpatialLines object need to be cut into lixels with a specified minimal distance.\nThis task can be performed by using with lixelize_lines() of spNetwork as shown in the code chunk below.\n\n\nCode\nlixels &lt;- lixelize_lines(network,\n                         700, \n                         mindist = 350)\n\n\nFrom the code chunk above, we see that:\n\nThe length of a lixel, lx_length is set to 700m, and\nThe minimum length of a lixel, mindist is set to 350m.\n\nNote:\n\nAfter cut, if the length of the final lixel is shorter than the minimum distance, then it is added to the previous lixel. If mindist is NULL, then mindist = maxdist/10. The segments that are already shorter than the minimum distance are not modified."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#generating-line-centre-points",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#generating-line-centre-points",
    "title": "Hands-on Exercise 3: 1st Order Spatial Point Pattern Analysis Methods",
    "section": "2.6.2 Generating line centre points",
    "text": "2.6.2 Generating line centre points\nNext, lines_center() of spNetwork will be used to generate a SpatialPointsDataFrame (i.e. samples) with line centre points as shown in the code chunk below.\n\n\nCode\nsamples &lt;- lines_center(lixels)\n\n\nThe points are located at the center of the line based on the length of the line."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#performing-netkde",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#performing-netkde",
    "title": "Hands-on Exercise 3: 1st Order Spatial Point Pattern Analysis Methods",
    "section": "2.6.3 Performing NetKDE",
    "text": "2.6.3 Performing NetKDE\nNext, we will compute the NetKDE with the code chunk below:\n\n\nCode\ndensities &lt;- nkde(network,\n                  events = pg_childcare,\n                  w = rep(1,nrow(pg_childcare)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300,\n                  div = \"bw\",\n                  method = \"simple\",\n                  digits = 1,\n                  tol = 1,\n                  grid_shape = c(1,1),\n                  max_depth = 8,\n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\nFrom the code chunk above:\n\nkernel_name argument indicates that quartic kernel is used\n\nOther possible kernel methods supported by spNetwork : triangle, gaussian, scaled gaussian, tricube, cosine ,triweight, epanechnikov or uniform.\n\nmethod argument indicates that simple method is used to calculate the NKDE.\n\nCurrently, spNetwork support three popular methods:\n\nmethod=“simple”\n\nAn intuitive solution: The distances between events and sampling points are replaced by network distances, and the formula of the kernel is adapted to calculate the density over a linear unit instead of an areal unit.\n\nmethod=“discontinuous”.\n\nEqually “divides” the mass density of an event at intersections of lixels.\n\nmethod=“continuous”\n\nIf the “discontinuous” method is unbiased, it leads to a discontinuous kernel function which is a bit counter-intuitive.\nThis “continuous” method divides the mass of the density at intersection but adjusts the density before the intersection to make the function continuous."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#visualising-netkde",
    "href": "Hands-on_Ex/Hands-on_Ex03/Hands-on_Ex03.html#visualising-netkde",
    "title": "Hands-on Exercise 3: 1st Order Spatial Point Pattern Analysis Methods",
    "section": "2.6.3.1 Visualising NetKDE",
    "text": "2.6.3.1 Visualising NetKDE\nBefore we can visualise the NetKDE values, we will use the code chunk below to insert the computed density values (i.e. densities) into samples and lixels objects as a density field.\n\n\nCode\nsamples$density &lt;- densities\nlixels$density &lt;- densities\n\n\nSince the svy21 projection system is in metres, the computed density values are very small i.e. 0.0000005.\nHence, we will rescale the density values from number of events per metre to number of events per kilometre using the code chunk below.\n\n\nCode\nsamples$density &lt;- samples$density*1000\nlixels$density &lt;- lixels$density*1000\n\n\nThen, we can prepare an interactive and high cartographic quality map visualization using the following code, with appropriate functions of the tmap package.\n\n\nCode\ntmap_mode('view')\n\ntm_shape(lixels) + \n  tm_lines(col = \"density\") + \n  tm_shape(pg_childcare) +\n  tm_dots()\n\n\n\n\n\n\n\nThe interactive map above effectively reveals road segments with relatively higher density of childcare centres (darker color) than road segments with relatively lower density of childcare centres (lighter color)."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrection",
    "section": "",
    "text": "In this exercise, we will learning how to compute Global Measures of Spatial Autocorrelection (GMSA) and Local Measures of Spatial Autocorrelation using the spdep package.\nBy the end to this hands-on exercise, we will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\nGMSA\n\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package\nprovide statistically correct interpretation of GSA statistics.\n\nLMSA\n\ncompute Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions spdep package;\ncompute Getis-Ord’s Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of spdep package;\nto visualise the analysis output by using tmap package."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-study-area-and-data",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#the-study-area-and-data",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrection",
    "section": "2.1 The Study Area and Data",
    "text": "2.1 The Study Area and Data\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#setting-the-analytical-tools",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#setting-the-analytical-tools",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrection",
    "section": "2.2 Setting the Analytical Tools",
    "text": "2.2 Setting the Analytical Tools\nBefore we get started, we need to ensure that spdep, sf, tmap and tidyverse packages of R are currently installed in your R.\n\nsf is use for importing and handling geospatial data in R,\ntidyverse is mainly use for wrangling attribute data in R,\nspdep will be used to compute spatial weights, global and local spatial autocorrelation statistics, and\ntmap will be used to prepare cartographic quality chropleth map.\n\nThe code chunk below is used to perform the following tasks:\n\ncreating a package list containing the necessary R packages,\nchecking if the R packages in the package list have been installed in R, if they have yet to be installed, RStudio will installed the missing packages,\nlaunching the packages into R environment.\n\n\npacman:: p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#importing-data-into-r",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#importing-data-into-r",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrection",
    "section": "2.3 Importing Data into R",
    "text": "2.3 Importing Data into R\nIn this section, we will learn how to bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n2.3.1 Import shapefile into R environment\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\",\n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/binhui-ong/IS415-GAA/Hands-on_Ex/Hands-on_Ex05/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n2.3.2 Import csv into R environment\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R data frame class.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n2.3.3 Performing Relational Join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, hunan2012) %&gt;% \n  select(1:4, 7, 15)\n\n\n\n2.3.4 Visualizing Regional Development Indicator\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nequal &lt;- tm_shape(hunan) + \n  tm_fill(\"GDPPC\", \n          n=5,\n          style = \"equal\") + \n  tm_borders(alpha=0.5) + \n  tm_layout(main.title = \"Equal interval classification\", main.title.size = 0.8)\n\nquantile &lt;- tm_shape(hunan) + \n  tm_fill(\"GDPPC\",\n          n = 5, \n          style = \"quantile\") + \n  tm_borders (alpha = 0.5) + \n  tm_layout(main.title = \"Equal quantile classfication\", main.title.size = 0.8)\n\ntmap_arrange(equal, quantile,\n             asp = 1,\n             ncol = 2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-contiguity-spatial-weights",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-contiguity-spatial-weights",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrection",
    "section": "3.1 Computing Contiguity Spatial Weights",
    "text": "3.1 Computing Contiguity Spatial Weights\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries.\nIf we refer to the documentation, we will see that we can pass a “queen” argument that takes TRUE or FALSE as options. If we do not specify this argument, the default is set to TRUE, that is, if we don’t specify queen = FALSE, this function will return a list of first order neighbours using the Queen criteria.\nMore specifically, the code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q &lt;- poly2nb(hunan, queen = TRUE)\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#row-standarized-weights-matrix",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#row-standarized-weights-matrix",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrection",
    "section": "3.2 Row-standarized weights matrix",
    "text": "3.2 Row-standarized weights matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is achieved by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summarise the neighbors’ values, it has one drawback: polygons along the edges of the study area will base their lagged values on fewer polygons, potentially over- or under-estimating the true nature of the spatial autocorrelation in the data.\nFor this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style = \"W\",\n                   zero.policy = TRUE)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#gmsa-morans-i",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#gmsa-morans-i",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrection",
    "section": "3.3 GMSA: Moran’s I",
    "text": "3.3 GMSA: Moran’s I\nIn this section, we will learn how to perform Moran’s I statistics testing by using moran.test() of spdep.\n\n3.3.1 Moran’s I test\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep.\n\nmoran.test(hunan$GDPPC,\n           listw=rswm_q,\n           zero.policy = TRUE,\n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nFrom the output above, we can see that p-value = 1.095e-06 &lt; 0.01. This means that we have sufficient evidence that GDPPC in hunan is not randomly distributed, and is spatially autocorrelated at 99% confidence level.\n\n\n3.3.2 Computing Monte Carlo Moran’s I\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 100 simulation will be performed.\n\nset.seed(123)\nbperm = moran.mc(hunan$GDPPC,\n                 listw=rswm_q,\n                 nsim = 120,\n                 zero.policy = TRUE, \n                 na.action = na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 121 \n\nstatistic = 0.30075, observed rank = 121, p-value = 0.008264\nalternative hypothesis: greater\n\n\nFrom the output above, we can see that p-value = 0.008264, which is less than 0.01. This means that we have sufficient evidence that there is significant positive spatial autocorrelation in different counties at 99% confidence level.\n\n\n3.3.3 Visualizing Monte Carlo Moran’s I\nIt is always a good practice for us the examine the simulated Moran’s I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using the code chunk below.\nIn the code chunk below hist() and abline() of R Graphics are used.\n\nmean(bperm$res[1:99])\n\n[1] -0.005334666\n\n\n\nvar(bperm$res[1:99])\n\n[1] 0.004018902\n\n\n\nsummary(bperm$res[1:99])\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-0.133564 -0.042255 -0.012144 -0.005335  0.029011  0.217831 \n\n\n\nhist(bperm$res,\n     freq = TRUE,\n     breaks = 20,\n     xlab = \"Simulated Moran's I\")\nabline(v=0,\n       col = \"red\")\n\n\n\n\nFrom the output above, we can see that the Simulated Moran’s I has a slightly right skewed distribution."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#gmsa-gearys-c",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#gmsa-gearys-c",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrection",
    "section": "3.4 GMSA: Geary’s C",
    "text": "3.4 GMSA: Geary’s C\nIn this section, we will learn how to perform Geary’s C statistics testing by using appropriate functions of spdep package.\n\n3.4.1 Geary’s C test\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\ngeary.test(hunan$GDPPC, listw = rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\nFrom the output above, we can see that p-value = 0.0001526 &lt; 0.01. This means that we have sufficient evidence that there is significant negatively spatial autocorrelation at 99% confidence level in our data.\n\n\n3.4.2 Computing Monte Carlo Geary’s C\nThe code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep.\n\nset.seed(1234)\nbperm &lt;- geary.mc(hunan$GDPPC,\n                 listw = rswm_q,\n                 nsim = 127)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 128 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.007812\nalternative hypothesis: greater\n\n\nFrom the output above, we can see that p-value = 0.007812 , which is less than 0.01. This means that we have sufficient evidence that there is statistically significant negative spatial autocorrelation in our data.\n\n\n3.4.3 Visualizing the Monte Carlo Geary’s C\nNext, we will plot a histogram to reveal the distribution of the simulated values by using the code chunk below.\n\nmean(bperm$res[1:99])\n\n[1] 1.00126\n\n\n\nvar(bperm$res[1:99])\n\n[1] 0.006247721\n\n\n\nsummary(bperm$res[1:99])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.8289  0.9563  0.9971  1.0013  1.0413  1.2385 \n\n\n\nhist(bperm$res,\n     freq = TRUE,\n     breaks = 20, \n     xlab = \"Simulated Geary C\")\nabline(v=1, col = \"red\")\n\n\n\n\nFrom the output above, we can the distribution of simulated Geary C is slightly left skewed. This makes sense considering the fact that Geary C is the opposite direction of Moran’s I."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#spatial-correlogram",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#spatial-correlogram",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrection",
    "section": "3.5 Spatial Correlogram",
    "text": "3.5 Spatial Correlogram\nSpatial correlograms are great to examine patterns of spatial autocorrelation in our data or model residuals. They show how correlated pairs of spatial observations are when we increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance. Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\n\n3.5.1 Compute Moran’s I correlogram\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran’s I. The plot() of base Graph is then used to plot the output.\n\nMI_corr &lt;- sp.correlogram(wm_q,\n                          hunan$GDPPC,\n                          order = 6,\n                          method = \"I\",\n                          style = \"W\")\nplot(MI_corr)\n\n\n\n\nPlotting the output alone might not provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFrom the plot, we can see that there are significant differences in Moran’s I values across different lags.\n\n\n3.5.2 Compute Geary’s C correlogram and plot\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary’s C. The plot() of base Graph is then used to plot the output.\n\nGC_corr &lt;- sp.correlogram(wm_q,\n                          hunan$GDPPC,\n                          order = 6, \n                          method = \"C\",\n                          style = \"W\")\nplot(GC_corr)\n\n\n\n\nSimilar to the previous step, we will print out the analysis report by using the code chunk below.\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#cluster-and-outlier-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#cluster-and-outlier-analysis",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrection",
    "section": "3.6 Cluster and Outlier Analysis",
    "text": "3.6 Cluster and Outlier Analysis\nLocal Indicators of Spatial Association (LISA) are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable. For instance, if we are studying cancer rates among census tracts in a given city, local clusters in the rates mean that there are areas that have higher or lower rates than is to be expected by chance alone; that is, the values occurring are above or below those of a random distribution in space.\nIn this section, we will learn how to apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran’I to detect cluster and/or outlier from GDP per capita 2012 of Hunan Province, PRC.\n\n3.6.1 Computing local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\nThe code chunks below are used to compute local Moran’s I of GDPPC2012 at the county level.\n\nfips &lt;- order(hunan$County)\nlocalMI &lt;- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic Pr(): the p-value of local moran statistic\n\nThe code chunk below uses printCoefmat() to list the content of the local Moran matrix derived.\n\nprintCoefmat(data.frame(\n  localMI[fips,],\n  row.names= hunan$County[fips]),\n  check.names = FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830\n\n\n\n\n3.6.1.1 Mapping the local Moran’s I\nBefore mapping the local Moran’s I map, it is wise to append the local Moran’s I dataframe (i.e. localMI) onto hunan SpatialPolygonDataFrame. The code chunks below can be used to perform the task. The output SpatialPolygonDataFrame is called hunan.localMI.\n\nhunan.localMI &lt;- cbind(hunan, localMI) %&gt;% \n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\n\n3.6.1.2 Mapping local Moran’s I values\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chunks below.\n\ntm_shape(hunan.localMI) + \n  tm_fill(col = \"Ii\",\n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") + \n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n3.6.1.3 Mapping local Moran’s I p-values\nThe choropleth shows there is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values, as consider above.\nThe code chunks below produce a choropleth map of Moran’s I p-values by using functions of tmap package.\n\ntm_shape(hunan.localMI) + \n  tm_fill(col=\"Pr.Ii\",\n          breaks = c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette = \"-Blues\",\n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha=0.5)\n\n\n\n\n\n\n3.6.1.4 Mapping both local Moran’s I values and p-values\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\nlocalMI.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-a-lisa-cluster-map",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#creating-a-lisa-cluster-map",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrection",
    "section": "3.7 Creating a LISA Cluster Map",
    "text": "3.7 Creating a LISA Cluster Map\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\n\n3.7.1 Plotting Moran scatterplot\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code chunk below plots the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\nnci &lt;- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County),\n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\nNotice that the plot is split in 4 quadrants. The top right corner belongs to areas that have high GDPPC and are surrounded by other areas that have the average level of GDPPC. This are the high-high locations in the lesson slide.\n\n\n3.7.2 Plotting Moran scatterplot with standardised variable\nFirst we will use scale() to centers and scales the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.\n\nhunan$Z.GDPPC &lt;- scale(hunan$GDPPC) %&gt;% \n  as.vector \n\nThe as.vector() added to the end is to make sure that the data type we get out of this is a vector, that map neatly into out dataframe.\nNow, we are ready to plot the Moran scatterplot again by using the code chunk below.\n\nnci2 &lt;- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")\n\n\n\n\n\n\n3.7.3 Preparing LISA map classes\nThe code chunks below show the steps to prepare a LISA cluster map.\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\n\nNext, we derive the spatially lagged variable of interest (i.e. GDPPC) and centers the spatially lagged variable around its mean.\n\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \n\nThen, we center the local Moran’s around the mean.\n\nLM_I &lt;- localMI[,1] - mean(localMI[,1])    \n\nNext, we will set a statistical significance level for the local Moran.\n\nsignif &lt;- 0.05\n\nThese four command lines define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\n\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4      \n\nLastly, place non-significant Moran in the category 0.\n\nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\nIn fact, we can combine all the steps into one single code chunk as shown below:\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \nLM_I &lt;- localMI[,1]   \nsignif &lt;- 0.05       \nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4    \nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\n\n\n3.7.4 Plotting LISA map\nNow, we can build the LISA map by using the code chunks below.\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation:\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)\n\n\n\n\nWe can also include the local Moran’s I map and p-value map as shown below for easy comparison.\n\nFrom the LISA map above, we can tell that there is a very large cluster of high GDPPC counties on the Eastern side of Hunan. There are also some counties with low to high GDPPC. Closer to mid-west, there are two counties with significally low GDPPC."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#hot-spot-and-cold-spot-area-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#hot-spot-and-cold-spot-area-analysis",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrection",
    "section": "3.8 Hot Spot and Cold Spot Area Analysis",
    "text": "3.8 Hot Spot and Cold Spot Area Analysis\nBeside detecting cluster and outliers, localised spatial statistics can be also used to detect hot spot and/or cold spot areas.\nThe term ‘hot spot’ has been used generically across disciplines to describe a region or value that is higher relative to its surroundings (Lepers et al 2005, Aben et al 2012, Isobe et al 2015).\n\n3.8.1 Getis and Ord’s G-Statistics\nAn alternative spatial statistics to detect spatial anomalies is the Getis and Ord’s G-statistics (Getis and Ord, 1972; Ord and Getis, 1995). It looks at neighbours within a defined proximity to identify where either high or low values clutser spatially. Here, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values too.\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\n\n\n\n3.8.2 Deriving distance-based weight matrix\nFirst, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two type of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix.\n\n\n3.8.2.1 Deriving the centroid\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid() on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid(). We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid() function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\n\n\n3.8.2.2 Determine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n3.8.2.3 Computing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nwm62_lw &lt;- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\nThe output spatial weights object is called wm62_lw.\n\n\n\n3.8.3 Computing adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-gi-statistics",
    "href": "Hands-on_Ex/Hands-on_Ex05/Hands-on_Ex05.html#computing-gi-statistics",
    "title": "Hands-on Exercise 5: Global and Local Measures of Spatial Autocorrection",
    "section": "3.9 Computing Gi statistics",
    "text": "3.9 Computing Gi statistics\n\n3.9.1 Gi statistics using fixed distance\n\nfips &lt;- order(hunan$County)\ngi.fixed &lt;- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)        Z(Gi) Pr(z != E(Gi))\n [1,] 0.064192949 0.05747126 2.375922e-04  0.436075843   6.627817e-01\n [2,] 0.042300020 0.04597701 1.917951e-04 -0.265505650   7.906200e-01\n [3,] 0.044961480 0.04597701 1.933486e-04 -0.073033665   9.417793e-01\n [4,] 0.039475779 0.03448276 1.461473e-04  0.413017033   6.795941e-01\n [5,] 0.049767939 0.04597701 1.927263e-04  0.273070579   7.847990e-01\n [6,] 0.008825335 0.01149425 4.998177e-05 -0.377510776   7.057941e-01\n [7,] 0.050807266 0.02298851 9.435398e-05  2.863898821   4.184617e-03\n [8,] 0.083966739 0.04597701 1.848292e-04  2.794350420   5.200409e-03\n [9,] 0.115751554 0.04597701 1.789361e-04  5.216125401   1.827045e-07\n[10,] 0.049115587 0.04597701 1.891013e-04  0.228236603   8.194623e-01\n[11,] 0.045819180 0.03448276 1.420884e-04  0.951035346   3.415864e-01\n[12,] 0.049183846 0.05747126 2.387633e-04 -0.536334231   5.917276e-01\n[13,] 0.048429181 0.04597701 1.924532e-04  0.176761556   8.596957e-01\n[14,] 0.034733752 0.02298851 9.651140e-05  1.195564020   2.318667e-01\n[15,] 0.011262043 0.01149425 4.945294e-05 -0.033020610   9.736582e-01\n[16,] 0.065131196 0.04597701 1.931870e-04  1.378081093   1.681783e-01\n[17,] 0.027587075 0.03448276 1.385862e-04 -0.585756761   5.580390e-01\n[18,] 0.029409313 0.03448276 1.461397e-04 -0.419680565   6.747188e-01\n[19,] 0.061466754 0.05747126 2.383385e-04  0.258805141   7.957856e-01\n[20,] 0.057656917 0.05747126 2.371303e-04  0.012056111   9.903808e-01\n[21,] 0.066518379 0.06896552 2.820326e-04 -0.145716531   8.841452e-01\n[22,] 0.045599896 0.04597701 1.928108e-04 -0.027158687   9.783332e-01\n[23,] 0.030646753 0.03448276 1.449523e-04 -0.318615290   7.500183e-01\n[24,] 0.035635552 0.04597701 1.906613e-04 -0.748946051   4.538897e-01\n[25,] 0.032606647 0.04597701 1.932888e-04 -0.961700582   3.362000e-01\n[26,] 0.035001352 0.04597701 1.897172e-04 -0.796851342   4.255374e-01\n[27,] 0.012746354 0.02298851 9.812587e-05 -1.033949773   3.011596e-01\n[28,] 0.061287917 0.06896552 2.773884e-04 -0.460979158   6.448136e-01\n[29,] 0.014277403 0.02298851 9.683314e-05 -0.885240161   3.760271e-01\n[30,] 0.009622875 0.01149425 4.924586e-05 -0.266671512   7.897221e-01\n[31,] 0.014258398 0.02298851 9.705244e-05 -0.886168613   3.755267e-01\n[32,] 0.005453443 0.01149425 4.986245e-05 -0.855476971   3.922871e-01\n[33,] 0.043283712 0.05747126 2.367109e-04 -0.922143185   3.564539e-01\n[34,] 0.020763514 0.03448276 1.393165e-04 -1.162328599   2.451020e-01\n[35,] 0.081261843 0.06896552 2.794398e-04  0.735582222   4.619850e-01\n[36,] 0.057419907 0.05747126 2.338437e-04 -0.003358489   9.973203e-01\n[37,] 0.013497133 0.02298851 9.624821e-05 -0.967459309   3.333145e-01\n[38,] 0.019289310 0.03448276 1.455643e-04 -1.259299080   2.079223e-01\n[39,] 0.025996272 0.04597701 1.892938e-04 -1.452256513   1.464303e-01\n[40,] 0.016092694 0.03448276 1.424776e-04 -1.540671121   1.233968e-01\n[41,] 0.035952614 0.05747126 2.379439e-04 -1.395011407   1.630124e-01\n[42,] 0.031690963 0.05747126 2.350604e-04 -1.681505286   9.266481e-02\n[43,] 0.018750079 0.03448276 1.433314e-04 -1.314110709   1.888090e-01\n[44,] 0.015449080 0.02298851 9.638666e-05 -0.767944457   4.425202e-01\n[45,] 0.065760689 0.06896552 2.760533e-04 -0.192889342   8.470456e-01\n[46,] 0.098966900 0.05747126 2.326002e-04  2.720804542   6.512325e-03\n[47,] 0.085415780 0.05747126 2.385746e-04  1.809191360   7.042128e-02\n[48,] 0.038816536 0.05747126 2.343951e-04 -1.218469473   2.230456e-01\n[49,] 0.038931873 0.04597701 1.893501e-04 -0.511984469   6.086619e-01\n[50,] 0.055098610 0.06896552 2.760948e-04 -0.834546363   4.039732e-01\n[51,] 0.033405005 0.04597701 1.916312e-04 -0.908179070   3.637836e-01\n[52,] 0.043040784 0.06896552 2.829941e-04 -1.541081516   1.232969e-01\n[53,] 0.011297699 0.02298851 9.615920e-05 -1.192199867   2.331829e-01\n[54,] 0.040968457 0.05747126 2.356318e-04 -1.075080164   2.823388e-01\n[55,] 0.023629663 0.04597701 1.877170e-04 -1.631075961   1.028743e-01\n[56,] 0.006281129 0.01149425 4.916619e-05 -0.743472246   4.571958e-01\n[57,] 0.063918654 0.05747126 2.369553e-04  0.418842387   6.753313e-01\n[58,] 0.070325003 0.05747126 2.381374e-04  0.832943753   4.048765e-01\n[59,] 0.025947288 0.03448276 1.444058e-04 -0.710289083   4.775249e-01\n[60,] 0.039752578 0.04597701 1.915656e-04 -0.449718820   6.529132e-01\n[61,] 0.049934283 0.05747126 2.334965e-04 -0.493238743   6.218439e-01\n[62,] 0.030964195 0.04597701 1.920248e-04 -1.083386776   2.786368e-01\n[63,] 0.058129184 0.05747126 2.343319e-04  0.042979051   9.657182e-01\n[64,] 0.046096514 0.04597701 1.932637e-04  0.008596093   9.931414e-01\n[65,] 0.012459080 0.01149425 5.008051e-05  0.136337469   8.915545e-01\n[66,] 0.091447733 0.05747126 2.377744e-04  2.203411744   2.756574e-02\n[67,] 0.049575872 0.02298851 9.766513e-05  2.690329952   7.138140e-03\n[68,] 0.107907212 0.04597701 1.933581e-04  4.453703219   8.440175e-06\n[69,] 0.019616151 0.02298851 9.789454e-05 -0.340842743   7.332220e-01\n[70,] 0.032923393 0.03448276 1.454032e-04 -0.129318589   8.971056e-01\n[71,] 0.030317663 0.02298851 9.867859e-05  0.737806634   4.606320e-01\n[72,] 0.019437582 0.03448276 1.455870e-04 -1.246912658   2.124295e-01\n[73,] 0.055245460 0.04597701 1.932838e-04  0.666667559   5.049845e-01\n[74,] 0.074278054 0.05747126 2.383538e-04  1.088613505   2.763244e-01\n[75,] 0.013269580 0.02298851 9.719982e-05 -0.985792573   3.242349e-01\n[76,] 0.049407829 0.03448276 1.463785e-04  1.233609606   2.173484e-01\n[77,] 0.028605749 0.03448276 1.455139e-04 -0.487196415   6.261191e-01\n[78,] 0.039087662 0.02298851 9.801040e-05  1.626174042   1.039126e-01\n[79,] 0.031447120 0.04597701 1.877464e-04 -1.060416797   2.889550e-01\n[80,] 0.064005294 0.05747126 2.359641e-04  0.425361422   6.705732e-01\n[81,] 0.044606529 0.05747126 2.357330e-04 -0.837897118   4.020885e-01\n[82,] 0.063700493 0.06896552 2.801427e-04 -0.314565243   7.530918e-01\n[83,] 0.051142205 0.04597701 1.933560e-04  0.371456331   7.102977e-01\n[84,] 0.102121112 0.04597701 1.610278e-04  4.424392623   9.671399e-06\n[85,] 0.021901462 0.02298851 9.843172e-05 -0.109566928   9.127528e-01\n[86,] 0.064931813 0.04597701 1.929430e-04  1.364597995   1.723794e-01\n[87,] 0.031747344 0.04597701 1.909867e-04 -1.029658605   3.031703e-01\n[88,] 0.015893319 0.02298851 9.765131e-05 -0.718000620   4.727569e-01\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nThe output of localG() is a vector of G or Gstar values, with attributes “gstari” set to TRUE or FALSE, “call” set to the function call, and class “localG”.\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\nNext, we will join the Gi values to their corresponding hunan sf data frame by using the code chunk below.\n\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\nIn fact, the code chunk above performs three tasks. First, it convert the output vector (i.e. gi.fixed) into r matrix object by using as.matrix(). Next, cbind() is used to join hunan@data and gi.fixed matrix to produce a new SpatialPolygonDataFrame called hunan.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename().\n\n\n3.9.2 Mapping Gi values with fixed distance weights\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nGimap &lt;-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)\n\n\n\n\nWe can see that there are provinces with significant clustering of similar values (high GDPPC counties are surrounded by high GDPPC counties, low GDPPC counties surrounded by low GDPPC counties) on the Eastern part of Hunan, and some less significant clustering of dissimilar values in the Western part of Hunan (low GDPPC counties surrounded by high GDPPC counties and vice versa). There is also a majority of counties, mostly in the central, northern and southern regions with no significant clustering.\n\n\n3.9.3 Gi statistics using adaptive distance\nThe code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e knb_lw).\n\nfips &lt;- order(hunan$County)\ngi.adaptive &lt;- localG(hunan$GDPPC, knn_lw)\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n\n3.9.4 Mapping Gi values with adaptive distance weights\nIt is time for us to visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc&lt;- qtm(hunan, \"GDPPC\")\n\nGimap &lt;- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, \n             Gimap, \n             asp=1, \n             ncol=2)\n\n\n\n\nWe can see that there are provinces with significant clustering of similar values (high GDPPC counties are surrounded by high GDPPC counties, low GDPPC counties surrounded by low GDPPC counties) on the Eastern part of Hunan, and some clustering of dissimilar values in the Western part of Hunan (low GDPPC counties surrounded by high GDPPC counties and vice versa). There is also a majority of counties, mostly in the central, northern and southern regions with no significant clustering."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bin Hui’s IS415-Geospatial Analytics and Applications Journal",
    "section": "",
    "text": "Hello, welcome to Bin Hui’s IS415 (Geospatial Analytics and Applications) Journal!\nI am Ong Bin Hui, a penultimate student from the Singapore Management University (SMU)\nThis is the course website of IS415, which I study this term. Please feel free to explore my course work on this website with the navigation pane at the top of this webpage.\nMoreover, to find out more about me, head over to the “About Me” page.\nHave a good day! :)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "",
    "text": "Human mobility, the movement of human beings in space and time, reflects the spatial-temporal characteristics of human behavior. With the advancement Information and Communication Technologies (ICT) especially in smartphones, a large volume of data related to human mobility have been collected. By using appropriate GIS analysis methods, these data are potentially useful in supporting smart city planning and management.\nIn 2020, a very interesting human mobility data set called Grab Posisi was released by GRAB, one of the largest shared taxi operator in South-east Asia. This provides an opportunity for us to explore the geographical and spatio-temporal distribution of Grab hailing services locations in Singapore.\n\n\n\nIn this exercise, we will be exploring the geographical and spatio-network distribution of Grab hailing services locations in Singapore with the use of spatial point patterns analysis techniques.\n\n\n\n\nUse appropriate functions of sf and tidyverse to prepare the following geospatial data layer in sf tibble data.frames:\n\nGrab taxi location points either by origins or destinations.\nRoad layer within Singapore excluding outer islands.\nSingapore boundary layer excluding outer islands\n\nUse the extracted data to derive traditional Kernel Density Estimation layers.\nUse the extracted data to derive either Network Kernel Density Estimation (NKDE) or Temporal Network Kernel Density Estimation (TNKDE)\nUse appropriate tmap functions to display the kernel density layers on openstreetmap of Singapore.\nDescribe the spatial patterns revealed by the kernel density maps.\n\n\n\n\nTo address the above questions, we would be using the following data sets:\n\n\n\nType\nContent\nSource\n\n\n\n\nGeospatial\nRoad data set of Malaysia, Singapore and Brunei\nOpenStreetMap of Geofabrik download server\n\n\nGeospatial\nMaster Plan 2019 Subzone Boundary (No Sea)\ndata.gov.sg\n\n\nAspatial\nGrab-Posisi of Singapore\nengineering.grab.com\n\n\n\n\n\n\nIn this exercise, the following R packages will be used:\n\ntidyverse: to read, manipulate and create tidy data, and to create data graphics\nsf: to provide simple features access to represent and work with spatial vector data such as points and polygons\nspatstat: to perform statistical analysis of spatial data\nraster: to read, write, manipulate, analyze and model spatial data\nmaptools: tools for handling spatial objects\ntmap: to create thematic and high-quality cartographic maps\narrow: improve the performance of data analysis methods, and to increase the efficiency of moving data from one system or programming language to another\nspNetwork: to perform Spatial Point Patterns Analysis such as kernel density estimation (KDE) and K-function on network. It also can be used to build spatial matrices (‘listw’ objects like in ‘spdep’ package) to conduct any kind of traditional spatial analysis with spatial weights based on reticular distances.\nclassInt: provides a uniform interface to finding class intervals for continuous numerical variables\nlubridate: to parse and manipulate dates\nggplot2: to create data visualizations\n\nTo install and load the packages, we will use p_load() from the pacman package:\n\npacman::p_load(tidyverse, sf, spatstat, raster, maptools, tmap, arrow, spNetwork, classInt, lubridate, ggplot2)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#background",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#background",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "",
    "text": "Human mobility, the movement of human beings in space and time, reflects the spatial-temporal characteristics of human behavior. With the advancement Information and Communication Technologies (ICT) especially in smartphones, a large volume of data related to human mobility have been collected. By using appropriate GIS analysis methods, these data are potentially useful in supporting smart city planning and management.\nIn 2020, a very interesting human mobility data set called Grab Posisi was released by GRAB, one of the largest shared taxi operator in South-east Asia. This provides an opportunity for us to explore the geographical and spatio-temporal distribution of Grab hailing services locations in Singapore."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#our-objectives",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#our-objectives",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "",
    "text": "In this exercise, we will be exploring the geographical and spatio-network distribution of Grab hailing services locations in Singapore with the use of spatial point patterns analysis techniques."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#our-task",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#our-task",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "",
    "text": "Use appropriate functions of sf and tidyverse to prepare the following geospatial data layer in sf tibble data.frames:\n\nGrab taxi location points either by origins or destinations.\nRoad layer within Singapore excluding outer islands.\nSingapore boundary layer excluding outer islands\n\nUse the extracted data to derive traditional Kernel Density Estimation layers.\nUse the extracted data to derive either Network Kernel Density Estimation (NKDE) or Temporal Network Kernel Density Estimation (TNKDE)\nUse appropriate tmap functions to display the kernel density layers on openstreetmap of Singapore.\nDescribe the spatial patterns revealed by the kernel density maps."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-acquisition",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-acquisition",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "",
    "text": "To address the above questions, we would be using the following data sets:\n\n\n\nType\nContent\nSource\n\n\n\n\nGeospatial\nRoad data set of Malaysia, Singapore and Brunei\nOpenStreetMap of Geofabrik download server\n\n\nGeospatial\nMaster Plan 2019 Subzone Boundary (No Sea)\ndata.gov.sg\n\n\nAspatial\nGrab-Posisi of Singapore\nengineering.grab.com"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#install-and-load-r-packages",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#install-and-load-r-packages",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "",
    "text": "In this exercise, the following R packages will be used:\n\ntidyverse: to read, manipulate and create tidy data, and to create data graphics\nsf: to provide simple features access to represent and work with spatial vector data such as points and polygons\nspatstat: to perform statistical analysis of spatial data\nraster: to read, write, manipulate, analyze and model spatial data\nmaptools: tools for handling spatial objects\ntmap: to create thematic and high-quality cartographic maps\narrow: improve the performance of data analysis methods, and to increase the efficiency of moving data from one system or programming language to another\nspNetwork: to perform Spatial Point Patterns Analysis such as kernel density estimation (KDE) and K-function on network. It also can be used to build spatial matrices (‘listw’ objects like in ‘spdep’ package) to conduct any kind of traditional spatial analysis with spatial weights based on reticular distances.\nclassInt: provides a uniform interface to finding class intervals for continuous numerical variables\nlubridate: to parse and manipulate dates\nggplot2: to create data visualizations\n\nTo install and load the packages, we will use p_load() from the pacman package:\n\npacman::p_load(tidyverse, sf, spatstat, raster, maptools, tmap, arrow, spNetwork, classInt, lubridate, ggplot2)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-import",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-import",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "2.1 Data Import",
    "text": "2.1 Data Import\nTo import geospatial data, we will be using st_read() from the sf package.\nRoad data set from OSM (shapefile format):\n\nroaddata_sf &lt;- st_read(dsn = \"data/geospatial\", \n                       layer = \"gis_osm_roads_free_1\")\n\nMaster Plan 2019 Subzone Boundary (No Sea) (geojson format):\n\nmpsz_sf &lt;- st_read(\"data/geospatial/MasterPlan2019SubzoneBoundaryNoSeaGEOJSON.geojson\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-preparation",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#data-preparation",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "2.2 Data Preparation",
    "text": "2.2 Data Preparation\nIt is important to ensure our geospatial data is clean, in the correct coordinate reference system (CRS) and extracted to contain only relevant data to prevent complications later on.\nIn this section, we will go through the procedures to prepare our geospatial data.\n\n2.2.1 Data Pre-Processing\nTo begin with, let’s examine the data sets to understand their features.\n\nroaddata_sf\n\nFrom the above, we can see that roaddata_sf is an sf object, with linestring geometry type and dimension XY.\nWe also notice that it is in WGS84 geodetic CRS, which is not our desired coordinate reference system (svy 21). Hence, we would have to reproject it later (Section 2.2.2).\n\nmpsz_sf\n\nFor mpsz_sf, we see that it is an sf object with multipolygon geometry type. It comprises of records with XYZ coordinates, indicating a Z-dimension, quite redundant to us.\nHere, we notice that mpsz_sf has geodetic CRS of WGS84 as well. Hence, we will need to fix the CRS for mpsz_sf later (Section 2.2.2) as well.\n\n2.2.1.1 Dropping Z-dimension\nAfter having an understanding of our data sets, we will start to modify them into our desired dimensions and systems.\nIn this step, we will remove the Z-dimension in mpsz_sf, with the use of st_zm(). st_zm() is a function used to drop or add Z and/or M dimensions, from sf package.\n\nmpsz_sf &lt;- st_zm(mpsz_sf)\n\n\nmpsz_sf\n\nWith that, we can see that the mpsz_sf has become two-dimensional (XY).\n\n\n2.2.1.2 Invalid Geometries\nTo check whether our data sets contain invalid geometries, we can apply the following code chunks:\n\nlength(which(st_is_valid(roaddata_sf) == FALSE))\n\n\nlength(which(st_is_valid(mpsz_sf) == FALSE))\n\nWe see that roaddata_sf has no invalid geometry, while mpsz_sf has 6 invalid geometries.\nTo correct the invalid geometries in mpsz_sf, we can use st_make_valid() from sf package,\n\nmpsz_sf &lt;- st_make_valid(mpsz_sf)\n\nand check confirm whether the modified mpsz_sf data set now contains fully valid geometries.\n\nlength(which(st_is_valid(mpsz_sf) == FALSE))\n\nGreat! Our geographic data are now cleared of invalid geometries.\n\n\n2.2.1.3 Handling Missing Values\nNext, we will check for missing values in our geographic data.\nLet’s begin with checking roaddata_sf, we can applying the following code chunk.\n\nroaddata_sf[rowSums(is.na(roaddata_sf)) != 0, ]\n\nWow, there is an absurd 1719007 records with missing values?!\nWe can investigate what could be wrong using View(roaddata_sf) (not shown here).\nFrom the result of View(), it seems like it is the “ref” field comprises of many missing values! Let’s try removing it, and check again!\n\nroaddata_sf &lt;- roaddata_sf %&gt;% dplyr::select(-ref) \n\n\nroaddata_sf[rowSums(is.na(roaddata_sf)) != 0, ]\n\nFrom the code above, which I did not display the result, it would appear that there are still a lot of missing values, particularly in the “name” field. However, we should not delete those records because they are not necessarily redundant: in Singapore, if a road length is less than 60m, it need not be named. Thus, these records might still represent valid roads that are shorter than 60m!\nThen, ignoring the missing values in “name” field, let’s check whether there are missing values in the other fields.\n\nroaddata_sf[rowSums(is.na(roaddata_sf %&gt;% dplyr::select(-name))) != 0, ]\n\nPhew, finally! There are no missing values in other fields. Seems like roaddata_sf is cleared of missing values that requires our attention.\nNext, let’s check mpsz_sf!\n\nmpsz_sf[rowSums(is.na(mpsz_sf)) != 0, ]\n\nThankfully, there’s no missing values in mpsz_sf!\nHurray, we’re done with resolving the missing values!\n\n\n\n2.2.2 Verifying and Transforming CRS\nTo check the CRS of the data sets, we can use st_crs() from sf package.\n\nst_crs(roaddata_sf)\n\n\nst_crs(mpsz_sf)\n\nFrom the above codes, and as also noticed earlier in Section 2.2.1, we would see that the data sets are in the WGS84 CRS. However, in Singapore, we should use the SVY21 CRS (with EPSG code: 3414) as it is more appropriate for our analysis.\nTo change the CRS of the data sets, we can use st_transform() from sf package, inputting the EPSG code for SVY21 (3414) as the second argument of the function.\n\nroaddata_sf &lt;- st_transform(roaddata_sf, 3414)\n\n\nmpsz_sf &lt;- st_transform(mpsz_sf, 3414)\n\nThen, let’s confirm that the CRS for the data sets have been correctly modified.\n\nst_crs(roaddata_sf)\n\n\nst_crs(mpsz_sf)\n\nHooray! Our geospatial data are now in the correct CRS!\n\n\n2.2.3 Extraction of relevant data\nAfter doing some cleaning of our geospatial data, let’s roughly visualize how they look like:\n\ntmap_mode(\"plot\")\n\ntm_shape(roaddata_sf) + \n  tm_lines()\n\n\nAs expected, roaddata_sf contains the visualization of Malaysia, Singapore and Brunei.\n\ntm_shape(mpsz_sf) +\n  tm_polygons()\n\n\nAnd for mpsz_sf, since it is supposed to contain data of Singapore’s territories, the visualization displays the map as such.\nHowever, in this exercise, we are interested in the data that includes only Singapore without its outer islands, we will have to remove all outer islands outside of Singapore mainland. Hence, we need to do some manipulation to remove the outer islands.\nTo begin with, we will remove the outer islands from mpsz_sf first using str_detect() from the stringr package and filter() from the dplyr package, and name the new data frame mpsz_sgsf\n\nmpsz_sgsf &lt;- mpsz_sf %&gt;% filter(!str_detect(Description, \"ISLAND\"))\n\nThen, to resolve internal boundaries,\n\nsgboundary_sf &lt;- mpsz_sgsf %&gt;% st_union\n\n\nsgboundary_sf\n\nLet’s make a quick plot using qtm() from the tmap package to check if we’ve successfully extracted data of Singapore’s boundary layer excluding outer islands:\n\ntmap_mode(\"plot\")\nqtm(sgboundary_sf)\n\n=\nYay! We’ve gotten the Singapore boundary layer that excludes the outer islands!\nNext, to derive the road layers that lie within Singapore, we can use st_contains() from the sf package. Here, we shall form a new data set road_sf which should contain geospatial data with road layers within Singapore, without its outer islands.\n\nroad_sf &lt;- roaddata_sf[st_contains(sgboundary_sf, roaddata_sf, sparse = FALSE),]\n\nLet’s break down the code above:\n\nThe first argument of st_contains() is the boundary which we want our result data to contain, and the second argument is the road data that we want to extract.\nThen, with sparse = FALSE, we ensure a complete containment check for each feature.\nHowever, it is good to note that this option could result potentially longer running time for large datasets.\nThen, we use the square brackets to extract the data for which st_contains is TRUE, that is, road networks that fall within the Singapore boundary.\n\nNow, let’s check that roadsg_sf contains the road layer only within Singapore by plotting a map!\n\nqtm(road_sf)\n\n\nTrue enough, this should be how the road system of Singapore, without its outer islands, looks like. YAY!"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#importing-aspatial-data",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#importing-aspatial-data",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "3.1 Importing Aspatial Data",
    "text": "3.1 Importing Aspatial Data\nTo import aspatial data, we will be using read_parquet() from arrow package.\n\ngrabposisi0 &lt;- read_parquet(\"data/aspatial/part-00000-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\ngrabposisi1 &lt;- read_parquet(\"data/aspatial/part-00001-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\ngrabposisi2 &lt;- read_parquet(\"data/aspatial/part-00002-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\ngrabposisi3 &lt;- read_parquet(\"data/aspatial/part-00003-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\ngrabposisi4 &lt;- read_parquet(\"data/aspatial/part-00004-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\ngrabposisi5 &lt;- read_parquet(\"data/aspatial/part-00005-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\ngrabposisi6 &lt;- read_parquet(\"data/aspatial/part-00006-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\ngrabposisi7 &lt;- read_parquet(\"data/aspatial/part-00007-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\ngrabposisi8 &lt;- read_parquet(\"data/aspatial/part-00008-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\ngrabposisi9 &lt;- read_parquet(\"data/aspatial/part-00009-8bbff892-97d2-4011-9961-703e38972569.c000.snappy.parquet\")\n\nTo combine all of them into one single data frame named grabposisi, we can use rbind().\n\ngrabposisi &lt;- rbind(grabposisi0, grabposisi1, grabposisi2, grabposisi3, grabposisi4, grabposisi5, grabposisi6, grabposisi7, grabposisi8, grabposisi9)\n\nTadah! Our grabposisi data sets are consolidated and ready for further preparation.\ngrab## 3.1 Data Preparation\nNext, let’s examine our new consolidated grabposisi data set.\n\nglimpse(grabposisi)\n\n\n3.1.1 Changing Data Types\nFrom running the code above, we would see that the pingtimestamp field is in the integer format, when it is supposed to be a datetime type of data. To convert it into datetime format, we can use the following code:\n\ngrabposisi$pingtimestamp &lt;- as_datetime(grabposisi$pingtimestamp)\n\nNow, it is in a more appropriate for interpretation and manipulation later!\n\n\n3.1.2 Conversion into sf tibble data frame\nNext, we will use the code below to convert the grabposisi data into sf tibble data frame. Since the data we are drawing the geometry from are in longitude (rawlat field) and latitude (rawlng field) (i.e. WGS84; EPSG = 4326) format, we have to use EPSG: 4326 to retrieve the data.\n\ngrabposisi_sf &lt;- st_as_sf(grabposisi, coords = c(\"rawlng\", \"rawlat\"), crs = 4326) \n\nTo check our transformed data frame, we apply the code below:\n\ngrabposisi_sf\n\n\n\n3.1.3 Verifying and Transforming CRS\nBefore we proceed further, let’s reproject our grabposisi_sf into our desired svy21 projection system (EPSG:3414) using st_transform().\n\ngrabposisi_sf &lt;- grabposisi_sf %&gt;% st_transform(3414)\n\n\nst_crs(grabposisi_sf)\n\nIt’s now in svy21 projection system, we’re good to move on!\n\n\n3.1.4 Extracting Study Data\nConsidering the fact that we are only interested in the Grab taxi location points by origins in this study, we can apply the following code:\n\norigin_grab_sf &lt;- grabposisi_sf %&gt;%\n  group_by(trj_id) %&gt;%\n  arrange(pingtimestamp) %&gt;%\n  filter(row_number() == 1) %&gt;%\n  mutate(weekday = wday(pingtimestamp, \n                        label = TRUE,\n                        abbr = TRUE),\n         start_hr = factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n\nThe code might look a bit complicating, but let’s go through this step by step through the pipelines:\n\nFirstly, we use group_by() from dplyr to group the records according to trajectory IDs, so that records of the same journey are together.\nThen, we use arrange() from dplyr to arrange the records according to time in ascending order. With that, the first record of each group would be the earliest record of the journey, inferring that it is the origin point.\nNext, we use filter() from dplyr to filter each group, keeping only their first row. This allows us to keep only the records relating to the origin points of each journey.\nLastly, we use mutate() from dplyr, along with wday(), hour() and mday() from lubridate to retrieve the weekdays, start_hr, and day of the week to support our analysis later.\n\nTadah! We have successfully extracted the Grab taxi location points by origin.\nNext, we should check again whether it is in the correct CRS.\n\nst_crs(origin_grab_sf)\n\nIt appears to be right! Let’s have a glimpse of the result origin_grab_sf data set before we move on.\n\nglimpse(origin_grab_sf)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#days-of-the-week",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#days-of-the-week",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "4.1 Days of the Week",
    "text": "4.1 Days of the Week\nTo look at the peak days, we can plot a bar chart using ggplot2.\n\nggplot(origin_grab_sf) + \n  geom_bar(aes(x=weekday))\n\n\nAt a glance, we see that the number of grab rides taken across different days do not appear drastically different."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#peak-times",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#peak-times",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "4.2 Peak Times",
    "text": "4.2 Peak Times\nTo plot the number of rides across different timings of the day, we can use the following code:\n\nhours_of_day &lt;- 0:23\n\norigin_grab_sf$start_hr &lt;- factor(origin_grab_sf$start_hr, levels = hours_of_day)\n\nggplot(origin_grab_sf, aes(x = start_hr)) +\n  geom_bar(stat = \"count\") +\n  labs(x = \"Start Hour\", y = \"Count of Rides\") +\n  scale_x_discrete(limits = hours_of_day) \n\n\nIt appears that past 12am, and around 10am to 11am are the peak periods of ride hailing services! This could be due to most public buses and trains not operating past midnight, resulting in a higher demand for the most popular alternative in Singapore: Grab ride services. Also, rides could be peaking around 10am and 11am as people are travelling to work."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#mapping-the-geospatial-data-sets",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#mapping-the-geospatial-data-sets",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "4.3 Mapping the geospatial data sets",
    "text": "4.3 Mapping the geospatial data sets\nNext, it is also useful for us to create a pin map to show the spatial patterns of our data.\n\ntmap_mode(\"plot\")\ntm_shape(sgboundary_sf) + tm_polygons() +\n  tm_shape(roadsg_sf) + tm_lines() + \n  tm_shape(origin_grab_sf) + tm_dots()\n\nIn this exercise, we use static mode with “plot” argument in tmap_mode() to ease the process of uploading this exercise.\nHowever, we can choose to use “view” argument instead if we want to navigate and zoom around the map freely. Using “view”, we can also query the information of each simple feature (i.e the point) by clicking on it."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#geospatial-data-wrangling-for-traditional-kernel-density-estimate-kde",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#geospatial-data-wrangling-for-traditional-kernel-density-estimate-kde",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "5.1 Geospatial Data Wrangling for Traditional Kernel Density Estimate (KDE)",
    "text": "5.1 Geospatial Data Wrangling for Traditional Kernel Density Estimate (KDE)\nTo conduct first-order spatial point analysis on our data, we will be using the spatstat package. However, the spatstat package requires our data to be in sp’s Spatial classes. In this section, we will convert the sf data frames to sp’s Spatial class, then to Spatial objects, and finally ppp and owin for our analysis.\n\n5.1.1 Converting sf data frames to sp’s Spatial class\nTo do so, we would use as_Spatial() from the sf package to convert our three geospatial data from sf data frame to sp’s Spatial class.\n\nsgboundary &lt;- as_Spatial(sgboundary_sf)\n\n\norigin&lt;- as_Spatial(origin_grab_sf)\n\n\n\n5.1.2 Converting the Spatial class into generic sp format\nspatstat requires the analytical data to be in ppp object form. However, as there is no direct method to convert Spatial classes into ppp objects, we would have to convert the Spatial classes into Spatial objects first.\n\nsgboundary_sp &lt;- as(sgboundary, \"SpatialPolygons\")\n\n\norigin_sp &lt;- as(origin, \"SpatialPoints\")\n\n\n\n5.1.3 Converting the generic sp format into spatstat’s ppp format\n\norigin_ppp &lt;- as(origin_sp, \"ppp\")\n\nHere, we can plot origin_ppp to examine the difference.\n\nplot(origin_ppp)\n\n\nFor a quick understanding of the summary statistics of the newly created ppp object, we can apply the summary() from base R.\n\nsummary(origin_ppp)\n\n\n\n5.1.4 Handling duplicated points\nWe can check for duplication in a ppp object using the following code chunk:\n\nany(duplicated(origin_ppp))\n\nYay, we have no duplicate points, so we don’t have to do anything here.\n\n\n5.1.5 Creating owin object\nWhen analyzing spatial potterns, it is good practice to confine our analysis within a geograhical area. In spatstat, an object called owin is designed to represent this polygonal region.\nThe following code chunk is used to convert sgboundary_sf into owin object of spatstat.\n\nsgboundary_owin &lt;- as.owin(sgboundary_sf)\n\nThe output object can be displayed by using the plot() function,\n\nplot(sgboundary_owin)\n\n\nand summary() function of BaseR.\n\nsummary(sgboundary_owin)\n\n\n\n5.1.6 Combining point events object and owin object\nIn this last step of geospatial data wrangling, we will extract origin locations that are within Singapore using the following code chunk:\n\noriginSG_ppp &lt;- origin_ppp[sgboundary_owin]\n\nThe output object combined both the point and polygon feature in one ppp object class as shown below.\n\nsummary(originSG_ppp)\n\nTo plot the newly derived originSG_ppp map,\n\nplot(originSG_ppp)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#deriving-traditional-kernel-density-estimation-kde-layers",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#deriving-traditional-kernel-density-estimation-kde-layers",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "5.2 Deriving Traditional Kernel Density Estimation (KDE) Layers",
    "text": "5.2 Deriving Traditional Kernel Density Estimation (KDE) Layers\nIn this section, we will be performing first-order SPPA using the spatstat package. In particular, we will be deriving the kernel density estimation (KDE) layer for visualizing and exploring the intensity of point processes (origin points of Grab rides).\n\n5.2.1 Computing KDE using automatic bandwidth selection method\nThe code chunk below computed a Kernel Density byy using the following configurations of density() of spatstat.\nAutomatic bandwidth selection method: bw.ppl() - other methods: bw.CvL(), bw.scott() or bw.diggle()\nSmoothing kernel: “gaussian” - other methods: “epanechnikov”, “quartic” or “disc”\nThe intensity estimate is corrected for edge effect bias by using edge = TRUE.\n\nkde_originSG_bwppl &lt;- density(originSG_ppp,\n                           sigma = bw.ppl,\n                           edge = TRUE,\n                           kernel = \"gaussian\")\n\nThen, we will plot the derived kernel density.\n\nplot(kde_originSG_bwppl)\n\nTo retrieve the bandwidth used to compute the KDE layer, we use the following code chunk:\n\nbw &lt;- bw.ppl(originSG_ppp)\n\n\nbw\n\n\n\n5.2.2 Rescaling KDE values\nIn the following code chunk, rescale() is used to convert the unit of measurement from meter to kilometer.\n\noriginSG_ppp.km &lt;- rescale(originSG_ppp, 1000, \"km\")\n\nNow, we can re-run density() using the rescaled data set and plot the output KDE map.\n\nkde_originSG_bwppl.km &lt;- density(originSG_ppp.km,\n                              sigma = bw.ppl,\n                              edge = TRUE, \n                              kernel = \"gaussian\")\n\n\nplot(kde_originSG_bwppl.km)\n\n\nNow, we can see that the output image looks identical to the earlier version, but with more interpretable data values in the legend.\n\n\n5.2.3 Comparing different automatic bandwidth methods\nAside from bw.diggle, as mentioned before, there are three other spatstat functions (bw.CvL(), bw.scott(), bw.ppl()) that can be used to determine the bandwidth automatically.\nLet’s take a look at the bandwidth used by each of these automatic bandwidth calculation methods, keeping all our kernel method (“gaussian”) constant!\n\nbw.CvL(originSG_ppp.km)\n\n\nbw.scott(originSG_ppp.km)\n\n\nbw.diggle(originSG_ppp.km)\n\nThe following code chunk is used to compare the difference in output using the different automatic bandwidth methods.\n\nkde_originSG_bwCvL &lt;- density(originSG_ppp.km,\n                              sigma = bw.CvL,\n                              edge = TRUE,\n                              kernel = \"gaussian\")\nkde_originSG_bwscott &lt;- density(originSG_ppp.km,\n                                sigma = bw.scott,\n                                edge = TRUE,\n                                kernel = \"gaussian\")\nkde_originSG_bwdiggle &lt;- density(originSG_ppp.km,\n                                 sigma = bw.diggle,\n                                 edge = TRUE, \n                                 kernel = \"gaussian\")\n\n\npar(mfrow = c(2,2))\n\nplot(kde_originSG_bwppl.km, main = \"bw.ppl\")\nplot(kde_originSG_bwCvL, main = \"bw.CvL\")\nplot(kde_originSG_bwscott, main = \"bw.scott\")\nplot(kde_originSG_bwdiggle, main = \"bw.diggle\")\n\n\n\n\n5.2.4 Comparing different kernel methods\nBy default, the kernel method used in density() is Gaussian. However, there are three other available method: Epanechnikov, Quartic and Dics\nIn the following code chunk, we will compare the different kernel methods, using originSG_ppp.km and automatic bandwidth method bw.ppl.\n\npar(mfrow=c(2,2))\n\nplot(density(originSG_ppp.km,\n             sigma = bw.ppl,\n             edge = TRUE,\n             kernel = \"gaussian\"),\n     main = \"gaussian\")\nplot(density(originSG_ppp.km,\n             sigma = bw.ppl,\n             edge = TRUE,\n             kernel = \"epanechnikov\"),\n     main = \"Epanechnikov\")\nplot(density(originSG_ppp.km,\n             sigma = bw.ppl,\n             edge = TRUE,\n             kernel = \"quartic\"),\n     main = \"Quartic\")\nplot(density(originSG_ppp.km,\n             sigma = bw.ppl,\n             edge = TRUE,\n             kernel = \"disc\"),\n     main = \"Disc\")\n\n\n\n\n5.2.5 Fixed and Adaptive KDE\n\n5.2.5.1 Computing KDE by using fixed bandwidth\nHere, we will compute a KDE later by defining a bandwidth of 50m.\n\nkde_originSG_50 &lt;- density(originSG_ppp.km,\n                           sigma = 0.05,\n                           edge = TRUE,\n                           kernel = \"gaussian\")\n\nHere, we use sigma = 0.05 as sigma is expressed in kilometers. Hence, sigma = 0.05 would denote 50m.\n\n\n5.2.5.2 Computing KDE by using adaptive bandwidth\nHere, we will derive adaptive KDE using density.adaptive() of spatstat.\n\nkde_originSG_adaptive &lt;- adaptive.density(originSG_ppp.km,\n                                          method = \"kernel\")\n\n\nplot(kde_originSG_adaptive)\n\n\n\n\n\n5.2.6 Converting KDE output into grid object\n\ngridded_kde_originSG_bwppl &lt;- as.SpatialGridDataFrame.im(kde_originSG_bwppl.km)\n\n\nspplot(gridded_kde_originSG_bwppl)\n\n\n\n\n5.2.7 Converting gridded output into raster\nNext, we will convert the gridded kernel denstiy objects into RasterLayer object using raster() of raster package.\n\nkde_originSG_bwppl_raster &lt;- raster(gridded_kde_originSG_bwppl)\n\nWe can look at the properties of this new kde_originSG_bwppl_raster RasterLayer object using the following code chunk:\n\nkde_originSG_bwppl_raster\n\n\n\n5.2.8 Assigning projection systems\nSince the crs property of kde_originSG_bwppl_raster RasterLayer object is NA, we will include CRS information for it using the code below.\n\nprojection(kde_originSG_bwppl_raster) &lt;- CRS(\"+init=EPSG:3414\")\n\n\nkde_originSG_bwppl_raster\n\nNow, the crs property is complete!\n\n\n5.2.9 Visualizing the KDE on OpenStreetMap\nFinally, we will display the raster on OpenStreetMap of Singapore. To do so, we need to apply the following two steps.\n\n5.2.9.1 Reprojecting raster object\nFrom earlier, we established that our raster object is in SVY21 (EPSG:3414). However, OpenStreetMap works with WGS84 projection system (EPSG:4326). Hence, to lay our KDE raster layer on OpenStreetMap, we must reproject its coordinate system.\nTo do so, we can use projectRaster from the raster package to apply the following code, with the second argument as the EPSG (4326) of WGS84, creating a new object kde_originSG_bwppl_raster_reproj which is the WGS84 version of our raster object.\n\nkde_originSG_bwppl_raster_reproj &lt;- projectRaster(kde_originSG_bwppl_raster, crs = 4326)\n\n\n\n5.2.9.2 Creating the visualization\nThen, we can use the following code, with tmap functions to display the KDE layer on OpenStreetMap. The “view” argument in tmap_mode() should give an interactive map. However, I will only display a snapshot of the result here.\n\ntmap_mode(\"view\")\ntm_shape(kde_originSG_bwppl_raster_reproj) +\n  tm_raster(\"v\") + \n  tm_layout(legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_basemap(\"OpenStreetMap\")\n\n\nTadah, we’re done with displaying the KDE layer on OpenStreetMap of Singapore!"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#geospatial-data-preparation-for-network-kernel-density-estimation-netkde",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#geospatial-data-preparation-for-network-kernel-density-estimation-netkde",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "5.3 Geospatial Data Preparation for Network Kernel Density Estimation (NetKDE)",
    "text": "5.3 Geospatial Data Preparation for Network Kernel Density Estimation (NetKDE)\nNetwork Constrained Spatial Point Patterns Analysis is a collection of spatial point patterns analysis methods specially developed to analyse spatial point event occurrences on or alongside networks. In our case, our event of analysis would be origin points of Grab ride journeys at 1-hour peak hour time frames. In this study, I will look into the 10am and 12am 1-hour peak travel hour in Tampines, a large and highly populated residential town for our analysis.\n\n5.3.1 Extracting study area\nTo retrieve boundary of Tampines, we apply the following code, similar to what we did in Section 2.2.3.\n\ntampines_sg_sf &lt;- mpsz_sf %&gt;% filter(str_detect(Description, \"TAMPINES\")) %&gt;% st_union()\n\n\norchard_sg_sf &lt;- mpsz_sf %&gt;% filter(str_detect(Description, \"ORCHARD\")) %&gt;% st_union()\n\nTo retrieve the road layer of Tampines and Orchard, we apply the code:\n\ntampines_road_sf &lt;- road_sf[st_contains(tampines_sg_sf, road_sf, sparse = FALSE),]\n\n\norchard_road_sf &lt;- road_sf[st_contains(orchard_sg_sf, road_sf, sparse = FALSE),]\n\nAnd to retrieve the origin points of Grab rides in Tampines and Orchard,\n\ntampines_origin_sf &lt;- origin_grab_sf[unlist(st_contains(tampines_sg_sf, origin_grab_sf)),]\n\n\norchard_origin_sf &lt;- origin_grab_sf[unlist(st_contains(orchard_sg_sf, origin_grab_sf)),]\n\n\n\n5.3.2 Preparing the lixels objects\nBefore computing NetKDE, our road_sf linestring data need to be cut into lixels with a specified minimal distance.\nTo do so, we can use lixelize_lines() from spNetwork package.\n\ntampines_lixels &lt;- lixelize_lines(tampines_road_sf,\n                         500,\n                         mindist = 150)\n\n\norchard_lixels &lt;- lixelize_lines(orchard_road_sf,\n                         500,\n                         mindist = 150)\n\nIn the code chunk above, we have set:\n\nlength of lixel, lx_length = 500m\nminimum length of lixel, mindist = 150m; as Singapore is small with a presumably large number of origin points all over the island\n\nNote: After the cut, if the length of the final lixel is shorter than the minimum distance, it would be added to the previous lixel. On the other hand, if mindist is NULL, then mindist = maxdist/10. Segments that are already shorter than the minimum distance are not modified.\n\n\n5.3.3 Generating line centre points\nNext, lines_center() of spNetwork will be used to generate a SpatialPointsDataFrame with line centre points as shown below:\n\ntampines_centers &lt;- lines_center(tampines_lixels)\n\n\norchard_centers &lt;- lines_center(orchard_lixels)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#performing-network-kde",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home_Ex01.html#performing-network-kde",
    "title": "Take-home Exercise 1: Application of Spatial Point Patterns Analysis to discover the geographical distribution of Grab hailing services in Singapore",
    "section": "5.4 Performing Network KDE",
    "text": "5.4 Performing Network KDE\nIn this section, we will perform the NetKDE. To begin with, let’s filter our data set so that we only have data for that of Tampines and Orchard areas, in order to ease to focus and reduce the computational complication in our analysis.\nNext, we will compute their respective NetKDE using the code chunks below, using a bandwidth of 500.\n\ntampines_densities &lt;- nkde(tampines_road_sf,\n                  events = tampines_origin_sf,\n                  w = rep(1, nrow(tampines_origin_sf)),\n                  samples = tampines_centers,\n                  kernel_name = \"quartic\",\n                  bw = 500,\n                  div = \"bw\",\n                  method = \"simple\",\n                  digits = 1,\n                  tol = 1,\n                  grid_shape = c(1,1),\n                  max_depth = 8, \n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\norchard_densities &lt;- nkde(orchard_road_sf,\n                  events = orchard_origin_sf,\n                  w = rep(1, nrow(orchard_origin_sf)),\n                  samples = orchard_centers,\n                  kernel_name = \"quartic\",\n                  bw = 500,\n                  div = \"bw\",\n                  method = \"simple\",\n                  digits = 1,\n                  tol = 1,\n                  grid_shape = c(1,1),\n                  max_depth = 8, \n                  agg = 5,\n                  sparse = TRUE,\n                  verbose = FALSE)\n\nFrom the code chunk above:\n\nw is a vector representing the weight of each event\nsamples is the points representing the locations for which the densities will be estimated\nkernel_name argument indicates that quartic kernel is used\n\nOther possible kernel methods supported by spNetwork : triangle, gaussian, scaled gaussian, tricube, cosine ,triweight, epanechnikov or uniform.\n\nmethod argument indicates that simple method is used to calculate the NKDE.\n\nCurrently, spNetwork support three popular methods:\n\nmethod=“simple”\n\nAn intuitive solution: The distances between events and sampling points are replaced by network distances, and the formula of the kernel is adapted to calculate the density over a linear unit instead of an areal unit.\n\nmethod=“discontinuous”.\n\nEqually “divides” the mass density of an event at intersections of lixels.\n\nmethod=“continuous”\n\nIf the “discontinuous” method is unbiased, it leads to a discontinuous kernel function which is a bit counter-intuitive.\nThis “continuous” method divides the mass of the density at intersection but adjusts the density before the intersection to make the function continuous.\n\n\n\nbw denotes the bandwidth used\nagg indicates if the events must be aggregated within a distance. If NULL, the events are aggregated only by rounding the coordinates\n\n\n5.3.4 Visualizing NetKDE\nBefore we can visualize the NetKDE values, we will use the code chunk below to insert the computed density values (i.e. densities) into centers and lixels objects as a density field.\n\ntampines_centers$density &lt;- tampines_densities\ntampines_lixels$density&lt;- tampines_densities\n\n\norchard_centers$density &lt;- orchard_densities\norchard_lixels$density&lt;- orchard_densities\n\nSince the svy21 projection system is in metres, the computed density values are very small. Hence, we rescale the density values from the number of events per metre to number of events per kilometre using the code chunk below.\n\ntampines_centers$density&lt;- tampines_centers$density*1000\ntampines_lixels$density &lt;- tampines_lixels$density*1000\n\n\norchard_centers$density&lt;- orchard_centers$density*1000\norchard_lixels$density &lt;- orchard_lixels$density*1000\n\nFinally, we can prepare a high cartographic quality map of the NKDE visualization for Tampines and Orchard on openstreetmap of Singapore using the follow codes. However, I will not display the actual interactive map here, and will show only a snapshot of it.\n\ntmap_mode(\"view\")\n\ntm_shape(tampines_lixels) + \n  tm_lines(col = \"density\") +\n  tm_shape(tampines_origin_sf) +\n  tm_dots() +\n  tm_basemap(\"OpenStreetMap\")\n\n\n\ntmap_mode(\"view\")\n\ntm_shape(orchard_lixels) + \n  tm_lines(col = \"density\") +\n  tm_shape(orchard_origin_sf) +\n  tm_dots() +\n  tm_basemap(\"OpenStreetMap\")\n\nFrom running the above codes, we can observe the NKDE of Grab journey origin points in the Tampines and Orchard road networks!"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 1)",
    "section": "",
    "text": "Dengue Hemorrhagic Fever (in short dengue fever) is one of the most widespread mosquito-borne diseases in the most tropical and subtropical regions. It is an acute disease caused by dengue virus infection which is transmitted by female Aedes aegypti and Aedes albopictus mosquitoes. In 2015, Taiwan had recorded the most severe dengue fever outbreak with more than 43,000 dengue cases and 228 deaths. Since then, the annual reported dengue fever cases were maintained at the level of not more than 200 cases. However, in 2023, Taiwan recorded 26703 dengue fever cases.\n\n\n\nIn this exercise, we would like to investigate:\n\nIf the distribution of dengue fever outbreak at Tainan City, Taiwan are independent from space and space and time.\nIf the outbreak is indeed spatial and spatio-temporal dependent,\n\nwhere the clusters and outliers are, and\nwhere the emerging hot spot/cold spot areas are.\n\n\n\n\n\nIn this exercise, we will be fulfilling the following tasks:\n\nUse appropriate functions of sf and tidyverse to prepare the following geospatial data layer:\n\na study area layer in sf polygon features. It must be at village level and confined to the D01, D02, D04, D06, D07, D08, D32 and D39 counties of Tainan City, Taiwan.\na dengue fever layer within the study area in sf point features. The dengue fever cases should be confined to epidemiology week 31-50, 2023.\na derived dengue fever layer in spacetime s3 class of sfdep. It should contain, among many other useful information, a data field showing number of dengue fever cases by village and by epidemiology week.\n\nUse the extracted data to perform global spatial autocorrelation analysis by using sfdep methods.\nUse the extracted data to perform local spatial autocorrelation analysis by using sfdep methods.\nUse the extracted data to perform emerging hotspot analysis by using sfdep methods.\nDescribe the spatial patterns revealed by the analysis.\n\n\n\n\nFor this take-home exercise, we will be using the two data sets:\n\n\n\n\n\n\n\n\n\nType\nContent\nFormat\nSource\n\n\n\n\nGeospatial\nTAIWAN_VILLAGE_2020\n\nVillage boundary of Taiwan\nIn Taiwan Geographic Coordinate System\n\nESRI shapefile\nHistorical map data of the village boundary: TWD97 longitude and latitude\n\n\nAspatial\nDengue_Daily.csv\n\nReported dengue cases in Taiwan since 1998\nSelected fields for study:\n\n發病日: Onset date 最小統計區中心點X: x-coordinate\n最小統計區中心點Y: y-coordinate\n\n\ncsv\nDengue Daily Confirmed Cases Since 1998\n\n\n\n\n\n\nIn this exercise, the following R packages would be used:\nThen, to install and/or load the R packages, we can use p_load() from the pacman package.\n\n\nCode\npacman::p_load(tidyverse, sf, sfdep, tmap, lubridate, plotly)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#background",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#background",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 1)",
    "section": "",
    "text": "Dengue Hemorrhagic Fever (in short dengue fever) is one of the most widespread mosquito-borne diseases in the most tropical and subtropical regions. It is an acute disease caused by dengue virus infection which is transmitted by female Aedes aegypti and Aedes albopictus mosquitoes. In 2015, Taiwan had recorded the most severe dengue fever outbreak with more than 43,000 dengue cases and 228 deaths. Since then, the annual reported dengue fever cases were maintained at the level of not more than 200 cases. However, in 2023, Taiwan recorded 26703 dengue fever cases."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#objectives",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#objectives",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 1)",
    "section": "",
    "text": "In this exercise, we would like to investigate:\n\nIf the distribution of dengue fever outbreak at Tainan City, Taiwan are independent from space and space and time.\nIf the outbreak is indeed spatial and spatio-temporal dependent,\n\nwhere the clusters and outliers are, and\nwhere the emerging hot spot/cold spot areas are."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#the-task",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#the-task",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 1)",
    "section": "",
    "text": "In this exercise, we will be fulfilling the following tasks:\n\nUse appropriate functions of sf and tidyverse to prepare the following geospatial data layer:\n\na study area layer in sf polygon features. It must be at village level and confined to the D01, D02, D04, D06, D07, D08, D32 and D39 counties of Tainan City, Taiwan.\na dengue fever layer within the study area in sf point features. The dengue fever cases should be confined to epidemiology week 31-50, 2023.\na derived dengue fever layer in spacetime s3 class of sfdep. It should contain, among many other useful information, a data field showing number of dengue fever cases by village and by epidemiology week.\n\nUse the extracted data to perform global spatial autocorrelation analysis by using sfdep methods.\nUse the extracted data to perform local spatial autocorrelation analysis by using sfdep methods.\nUse the extracted data to perform emerging hotspot analysis by using sfdep methods.\nDescribe the spatial patterns revealed by the analysis."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#the-data",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#the-data",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 1)",
    "section": "",
    "text": "For this take-home exercise, we will be using the two data sets:\n\n\n\n\n\n\n\n\n\nType\nContent\nFormat\nSource\n\n\n\n\nGeospatial\nTAIWAN_VILLAGE_2020\n\nVillage boundary of Taiwan\nIn Taiwan Geographic Coordinate System\n\nESRI shapefile\nHistorical map data of the village boundary: TWD97 longitude and latitude\n\n\nAspatial\nDengue_Daily.csv\n\nReported dengue cases in Taiwan since 1998\nSelected fields for study:\n\n發病日: Onset date 最小統計區中心點X: x-coordinate\n最小統計區中心點Y: y-coordinate\n\n\ncsv\nDengue Daily Confirmed Cases Since 1998"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#installing-and-loading-r-packages",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#installing-and-loading-r-packages",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 1)",
    "section": "",
    "text": "In this exercise, the following R packages would be used:\nThen, to install and/or load the R packages, we can use p_load() from the pacman package.\n\n\nCode\npacman::p_load(tidyverse, sf, sfdep, tmap, lubridate, plotly)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-import",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-import",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 1)",
    "section": "2.1 Data Import",
    "text": "2.1 Data Import\nLet’s begin by introducing and preparing our geospatial data set in R!\nTo import geospatial data, we will be using st_read() from the sf package.\n\n\nCode\ntaiwan_sf&lt;- st_read(dsn= \"data/geospatial\",\n                  layer = \"TAINAN_VILLAGE\")\n\n\nFrom the above, we can see that taiwan is in geodetic CRS TWD97.\nThen, I will save the taiwan_sf object into my local disk so that I can easily load it again in the future.\n\n\nCode\nwrite_rds(taiwan_sf, \"/Users/binhui-ong/IS415-GAA/Take-home_Ex/Take-home_Ex02/data/rds/taiwan_sf.rds\")\n\n\nTo open the geospatial data in the future,\n\n\nCode\ntaiwan_sf &lt;- readRDS(\"/Users/binhui-ong/IS415-GAA/Take-home_Ex/Take-home_Ex02/data/rds/taiwan_sf.rds\")\n\n\nWe can do the same for all future objects! However, I will not be displaying every of them in this exercise as it would look lengthy."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-pre-processing",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-pre-processing",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 1)",
    "section": "2.2 Data Pre-Processing",
    "text": "2.2 Data Pre-Processing\nIt is important to ensure our geospatial data is clean, in the correct coordinate reference system (CRS) and extracted to contain only relevant data to prevent complications later on.\nIn this section, we will go through the procedures to prepare our geospatial data.\n\n2.2.1 Resolving invalid geometries\nFirstly, to check whether our data sets contain invalid geometries, we can apply the following code chunk:\n\n\nCode\nlength(which(st_is_valid(taiwan_sf) == FALSE))\n\n\n[1] 0\n\n\nst_is_valid() from sf package checks for valid geometries, returning a logical TRUE or FALSE. In the above code, we are checking the number of invalid geometries by looking for those with “FALSE”. Thankfully, taiwan_sf has no such invalid geometries, and we can move to the next step.\n\n\n2.2.2 Handling missing values\nTo check for missing values in our geographic data, we can apply the following code:\n\n\nCode\ntaiwan_sf[rowSums(is.na(taiwan_sf)) != 0, ]\n\n\nSimple feature collection with 649 features and 10 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 120.0269 ymin: 22.88751 xmax: 120.6563 ymax: 23.41374\nGeodetic CRS:  TWD97\nFirst 10 features:\n      VILLCODE COUNTYNAME TOWNNAME VILLNAME        VILLENG COUNTYID COUNTYCODE\n1  67000280002     臺南市   歸仁區   六甲里    Liujia Vil.        D      67000\n2  67000350032     臺南市   安南區   青草里   Qingcao Vil.        D      67000\n3  67000150009     臺南市   七股區   溪南里     Xinan Vil.        D      67000\n4  67000150010     臺南市   七股區   七股里      Qigu Vil.        D      67000\n5  67000150008     臺南市   七股區   龍山里  Longshan Vil.        D      67000\n6  67000150017     臺南市   七股區   中寮里 Zhongliao Vil.        D      67000\n7  67000150004     臺南市   七股區   篤加里     Dujia Vil.        D      67000\n8  67000150007     臺南市   七股區   塩埕里 Yancheng  Vil.        D      67000\n9  67000150022     臺南市   七股區   三股里     Sangu Vil.        D      67000\n10 67000150023     臺南市   七股區   十份里    Shifen Vil.        D      67000\n   TOWNID TOWNCODE NOTE                       geometry\n1     D33 67000280 &lt;NA&gt; POLYGON ((120.2725 22.95868...\n2     D06 67000350 &lt;NA&gt; POLYGON ((120.1176 23.08387...\n3     D22 67000150 &lt;NA&gt; POLYGON ((120.121 23.1355, ...\n4     D22 67000150 &lt;NA&gt; POLYGON ((120.1312 23.1371,...\n5     D22 67000150 &lt;NA&gt; POLYGON ((120.0845 23.13503...\n6     D22 67000150 &lt;NA&gt; POLYGON ((120.126 23.16917,...\n7     D22 67000150 &lt;NA&gt; POLYGON ((120.1585 23.15376...\n8     D22 67000150 &lt;NA&gt; POLYGON ((120.0636 23.17069...\n9     D22 67000150 &lt;NA&gt; POLYGON ((120.0422 23.11545...\n10    D22 67000150 &lt;NA&gt; POLYGON ((120.1201 23.09364...\n\n\nSeems like all observations contain missing values, and we could see from the above result that it is due to the “NOTE” field. Let’s try checking the data frame again while excluding the “Note” field using select() from the dplyr package.\n\n\nCode\ntaiwan_sf[rowSums(is.na(taiwan_sf %&gt;% select(-NOTE))) != 0, ]\n\n\nSimple feature collection with 0 features and 10 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nGeodetic CRS:  TWD97\n [1] VILLCODE   COUNTYNAME TOWNNAME   VILLNAME   VILLENG    COUNTYID  \n [7] COUNTYCODE TOWNID     TOWNCODE   NOTE       geometry  \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nYay, seems like we have no missing values if we exclude the “Note” field!\n\n\n2.2.3 Verifying CRS\nTo check the CRS of our taiwan_sf data set, we can use st_crs() from sf package as shown below.\n\n\nCode\nst_crs(taiwan_sf)\n\n\nCoordinate Reference System:\n  User input: TWD97 \n  wkt:\nGEOGCRS[\"TWD97\",\n    DATUM[\"Taiwan Datum 1997\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"Taiwan, Republic of China - onshore and offshore - Taiwan Island, Penghu (Pescadores) Islands.\"],\n        BBOX[17.36,114.32,26.96,123.61]],\n    ID[\"EPSG\",3824]]\n\n\nWe can see that our data set is in TWD97 crs (EPSG: 3824).\n\n\n2.2.4 Selecting relevant fields\nTo speed up our analysis later, we will select and keep only some assumingly relevant fields using select() from the dplyr package. In this step, I will keep VILLCODE, VILLNAME, VILLENG, TOWNID, TOWNCODE and geometry fields.\n\n\nCode\ntaiwanstudy_sf &lt;- taiwanstudy_sf %&gt;% select(VILLCODE, VILLNAME, VILLENG, TOWNID, TOWNCODE, geometry)\n\n\n\n\n2.2.5 Extraction of study area\nNext, to extract our study areas D01, D02, D04, D06, D07, D08, D32 and D39 counties of Tainan City, Taiwan, we can use filter() from the dplyr package, and | which represents “OR”.\n\n\nCode\ntaiwanstudy_sf &lt;- taiwan_sf %&gt;% filter(TOWNID == \"D01\" | TOWNID == \"D02\" | TOWNID == \"D04\" | TOWNID == \"D06\" | TOWNID == \"D07\" | TOWNID == \"D08\" | TOWNID == \"D32\" | TOWNID == \"D39\")\n\n\nTo visualize our study area, we can employ plot() and st_geometry() from sf package.\n\n\nCode\nplot(st_geometry(taiwanstudy_sf))\n\n\n\nLooks great! We’ve successfully extracted our study area later in sf polygon features, at the village level confined to the D01, D02, D04, D06, D07, D08, D32 and D39 counties of Tainan City, Taiwan."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-import-1",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#data-import-1",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 1)",
    "section": "3.1 Data Import",
    "text": "3.1 Data Import\nThe first step to dealing with our aspatial data is to import it into our R environment. To do so, we can use read_csv() from the readr package.\n\n\nCode\nDengue_Daily &lt;- read_csv(\"data/aspatial/Dengue_Daily.csv\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#extraction-of-study-fields",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#extraction-of-study-fields",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 1)",
    "section": "3.2 Extraction of study fields",
    "text": "3.2 Extraction of study fields\nNext, we will use select() from the dplyr package again to select the relevant fields for our study. Here, we would select 發病日 which is the onset date of symptoms, and 最小統計區中心點X and 最小統計區中心點Y which are the x-coordinates and y-coordinates of the dengue cases.\n\n\nCode\nDengue_Daily &lt;- Dengue_Daily %&gt;% select(發病日, 最小統計區中心點X, 最小統計區中心點Y)\nDengue_Daily\n\n\n# A tibble: 106,861 × 3\n   發病日     最小統計區中心點X 最小統計區中心點Y\n   &lt;date&gt;     &lt;chr&gt;             &lt;chr&gt;            \n 1 1998-01-02 120.505898941     22.464206650     \n 2 1998-01-03 120.453657460     22.466338948     \n 3 1998-01-13 121.751433765     24.749214667     \n 4 1998-01-15 120.338158907     22.630316700     \n 5 1998-01-20 121.798235373     24.684507639     \n 6 1998-01-22 None              None             \n 7 1998-01-23 121.547480075     24.982467229     \n 8 1998-01-26 121.500936346     25.139302723     \n 9 1998-02-11 120.209079313     22.978859098     \n10 1998-02-16 120.313207729     22.724216594     \n# ℹ 106,851 more rows"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#checking-for-missing-values",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#checking-for-missing-values",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 1)",
    "section": "3.3 Checking for missing values",
    "text": "3.3 Checking for missing values\nAgain, it is always a good practice to check for missing values. From the earlier code, we see that missing values are denoted as “None” in this data set. Hence, we will use filter() from the dplyr package, and look for coordinate values that contain “None”.\n\n\nCode\nDengue_Daily %&gt;% filter(最小統計區中心點X == \"None\" | 最小統計區中心點Y == \"None\")\n\n\n# A tibble: 780 × 3\n   發病日     最小統計區中心點X 最小統計區中心點Y\n   &lt;date&gt;     &lt;chr&gt;             &lt;chr&gt;            \n 1 1998-01-22 None              None             \n 2 1998-05-11 None              None             \n 3 1998-07-25 None              None             \n 4 1998-07-26 None              None             \n 5 1998-07-27 None              None             \n 6 1998-07-30 None              None             \n 7 1998-09-02 None              None             \n 8 1998-09-14 None              None             \n 9 1998-09-25 None              None             \n10 2000-03-17 None              None             \n# ℹ 770 more rows\n\n\nWow, there are 780 records with no x- and y- coordinates! To prevent complications in later steps of analysis, we should remove them. To do so, we can apply the following code.\n\n\nCode\nDengue_Daily &lt;- Dengue_Daily %&gt;% filter(最小統計區中心點X != \"None\" | 最小統計區中心點Y != \"None\")\n\n\nNow, we’re done removing the records with missing coordinates."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#combine-coordinates",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#combine-coordinates",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 1)",
    "section": "3.4 Combine Coordinates",
    "text": "3.4 Combine Coordinates\nSince our coordinates are in differet columns, it only makes sense to combine them to turn them into useful data. To do so, we can use the st_as_sf from the sf package, with the respective x- and y- coordinates in the “coords” argument, and the crs of the data set in the “crs” argument.\n\n\nCode\nDengueDaily_sf &lt;- st_as_sf(Dengue_Daily, coords = c(\"最小統計區中心點X\", \"最小統計區中心點Y\"), crs = 3824)\n\n\n\n\nCode\nDengueDaily_sf\n\n\nSimple feature collection with 106081 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 118.3081 ymin: 21.92574 xmax: 121.9826 ymax: 26.15617\nGeodetic CRS:  TWD97\n# A tibble: 106,081 × 2\n   發病日                geometry\n * &lt;date&gt;             &lt;POINT [°]&gt;\n 1 1998-01-02 (120.5059 22.46421)\n 2 1998-01-03 (120.4537 22.46634)\n 3 1998-01-13 (121.7514 24.74921)\n 4 1998-01-15 (120.3382 22.63032)\n 5 1998-01-20 (121.7982 24.68451)\n 6 1998-01-23 (121.5475 24.98247)\n 7 1998-01-26  (121.5009 25.1393)\n 8 1998-02-11 (120.2091 22.97886)\n 9 1998-02-16 (120.3132 22.72422)\n10 1998-02-17   (120.34 22.60703)\n# ℹ 106,071 more rows\n\n\nNice, our coordinates are sensibly put together now!"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#extraction-of-study-time-frame",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#extraction-of-study-time-frame",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 1)",
    "section": "3.5 Extraction of Study Time Frame",
    "text": "3.5 Extraction of Study Time Frame\nThen, to limit our the study time frame to epidemiology week 31-50, 2023, we once again use filter() from the dplyr package. We also employ year() and epiweek() from the lubridate package to identify and extract our required time frame.\n\n\nCode\nDenguedates_sf &lt;- DengueDaily_sf %&gt;% filter(year(發病日) == 2023) %&gt;% filter(epiweek(發病日) &gt;=31 & epiweek(發病日)&lt;= 50)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#extraction-of-dengue-observation-data-points",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#extraction-of-dengue-observation-data-points",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 1)",
    "section": "4.1 Extraction of dengue observation data points",
    "text": "4.1 Extraction of dengue observation data points\nFirstly, let’s extract the Dengue observations that are within the regions of study. To do so, we can apply use st_join() from the sf package and apply the following code.\n\n\nCode\ndenguepoints_sf &lt;- Denguedates_sf %&gt;% st_join(taiwanstudy_sf, join = st_within)\n\n\nThe above code gives us the dengue fever layer within the study area in sf point features, confined to epidemiology weeks 31-50, 2023. Yay!\n\n\nCode\ndenguepoints_sf\n\n\nSimple feature collection with 10904 features and 3 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 118.3081 ymin: 21.98551 xmax: 121.8983 ymax: 25.20237\nGeodetic CRS:  TWD97\n# A tibble: 10,904 × 4\n# Groups:   VILLENG [249]\n   VILLENG     發病日     cases                                         geometry\n   &lt;chr&gt;       &lt;date&gt;     &lt;int&gt;                                   &lt;GEOMETRY [°]&gt;\n 1 Andong Vil. 2023-08-02     1                        POINT (120.2143 23.03558)\n 2 Andong Vil. 2023-08-16     1                         POINT (120.2141 23.0306)\n 3 Andong Vil. 2023-08-17     1                        POINT (120.2146 23.03379)\n 4 Andong Vil. 2023-09-01     1                         POINT (120.2163 23.0386)\n 5 Andong Vil. 2023-09-06     1                        POINT (120.2143 23.03558)\n 6 Andong Vil. 2023-09-08     1                        POINT (120.2136 23.03202)\n 7 Andong Vil. 2023-09-12     1                        POINT (120.2133 23.03384)\n 8 Andong Vil. 2023-09-17     2 MULTIPOINT ((120.2145 23.03455), (120.2144 23.0…\n 9 Andong Vil. 2023-09-20     1                        POINT (120.2146 23.03379)\n10 Andong Vil. 2023-09-23     2 MULTIPOINT ((120.2144 23.03224), (120.2155 23.0…\n# ℹ 10,894 more rows"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#aggregating-dengue-observations-based-on-village-boundaries",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#aggregating-dengue-observations-based-on-village-boundaries",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 1)",
    "section": "4.2 Aggregating dengue observations based on village boundaries",
    "text": "4.2 Aggregating dengue observations based on village boundaries\nTo find out identify which village each observation is in,\n\n\nCode\ndengue_sf &lt;- st_join(taiwanstudy_sf, denguepoints_sf, join = st_contains)\n\n\nTo select our relevant columns, we once again use select() from dplyr package.\n\n\nCode\ndengue_sf &lt;- dengue_sf %&gt;% select(VILLENG.x, TOWNID.x, 發病日, geometry) %&gt;% rename(VILLENG = VILLENG.x) %&gt;% rename(TOWNID = TOWNID.x)\n\n\n\n\nCode\ndengue_sf\n\n\nSimple feature collection with 18817 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 120.0627 ymin: 22.89401 xmax: 120.2925 ymax: 23.09144\nGeodetic CRS:  TWD97\nFirst 10 features:\n         VILLENG TOWNID     發病日                       geometry\n1   Qingcao Vil.    D06 2023-09-14 POLYGON ((120.1176 23.08387...\n1.1 Qingcao Vil.    D06 2023-10-12 POLYGON ((120.1176 23.08387...\n2    Bao'an Vil.    D32 2023-08-04 POLYGON ((120.2304 22.93544...\n2.1  Bao'an Vil.    D32 2023-09-10 POLYGON ((120.2304 22.93544...\n2.2  Bao'an Vil.    D32 2023-09-11 POLYGON ((120.2304 22.93544...\n2.3  Bao'an Vil.    D32 2023-09-16 POLYGON ((120.2304 22.93544...\n2.4  Bao'an Vil.    D32 2023-09-19 POLYGON ((120.2304 22.93544...\n2.5  Bao'an Vil.    D32 2023-09-20 POLYGON ((120.2304 22.93544...\n2.6  Bao'an Vil.    D32 2023-09-27 POLYGON ((120.2304 22.93544...\n2.7  Bao'an Vil.    D32 2023-09-29 POLYGON ((120.2304 22.93544...\n\n\nYou might be wondering, why did we keep the TOWNID column if our study is based on villages? Apparently, there are villages with the same names that are spread across different regions, classified in different towns. Arguably, the geometries column is sufficient to tell us that. However, due to the less readable nature of the geometry values, we might want to preserve our TOWNID first, for easier interpretation of our records.\nOne example of villages with the same names, located separately is the “成功里” village (VILLENG == “Chenggong Vil.”) as shown below.\n\n\nCode\nplot(st_geometry(filter(taiwanstudy_sf, VILLENG == \"Chenggong Vil.\")))\n\n\n\nThen, to derive the aggregated dengue observations data (without classifying our data based on date) based on village boundaries and town ID, as polygons sf data frame,\n\n\nCode\ndengue_aggregated_sf &lt;- st_join(taiwanstudy_sf, dengue_sf, join = st_contains) %&gt;% group_by (VILLENG.x, TOWNID.x) %&gt;% summarize(total_cases= n())\n\n\nThen, to check if we have prepared our data correctly thus far, we can plot a choropleth map using the various functions from the tmap package. Here, we use dengue_aggregated_sf, which aggregates all observations based on villages, as it probably gives a more intuitive and sensible check.\n\n\nCode\ntm_shape(dengue_aggregated_sf) + \n  tm_polygons(\"total_cases\") + \n  tm_layout(main.title = \"Number of dengue cases by Village\", \n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45,\n            legend.width=0.35,\n            frame = TRUE) + \n  tm_borders(alpha=0.5)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#deriving-contiguity-weights-queens-method",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#deriving-contiguity-weights-queens-method",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 1)",
    "section": "5.1: Deriving contiguity weights: Queen’s method",
    "text": "5.1: Deriving contiguity weights: Queen’s method\nTo get the contiguity weight matrix of neighbours using the Queen’s method, we can apply the following code:\n\n\nCode\nwm_q &lt;- dengue_aggregated_sf %&gt;% mutate(nb = st_contiguity(geometry),\n                                        wt = st_weights(nb, \n                                                        style = \"W\"),\n                                        .before=1)\n\n\n\n\nCode\nwm_q\n\n\nSimple feature collection with 249 features and 4 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 120.0627 ymin: 22.89401 xmax: 120.2925 ymax: 23.09144\nGeodetic CRS:  TWD97\n# A tibble: 249 × 5\n   nb        wt        VILLENG.x   total_cases                          geometry\n * &lt;nb&gt;      &lt;list&gt;    &lt;chr&gt;             &lt;int&gt;                     &lt;POLYGON [°]&gt;\n 1 &lt;int [5]&gt; &lt;dbl [5]&gt; Andong Vil.          67 ((120.2164 23.0404, 120.2154 23.…\n 2 &lt;int [5]&gt; &lt;dbl [5]&gt; Anfu Vil.           311 ((120.1932 23.0294, 120.1929 23.…\n 3 &lt;int [7]&gt; &lt;dbl [7]&gt; Anhe Vil.            73 ((120.2258 23.04424, 120.2219 23…\n 4 &lt;int [3]&gt; &lt;dbl [3]&gt; Ankang Vil.          97 ((120.2269 23.01413, 120.2274 23…\n 5 &lt;int [4]&gt; &lt;dbl [4]&gt; Anqing Vil.         111 ((120.2113 23.04414, 120.2111 23…\n 6 &lt;int [3]&gt; &lt;dbl [3]&gt; Anshun Vil.          34 ((120.2136 23.05601, 120.2136 23…\n 7 &lt;int [8]&gt; &lt;dbl [8]&gt; Anxi Vil.           110 ((120.2123 23.03689, 120.212 23.…\n 8 &lt;int [5]&gt; &lt;dbl [5]&gt; Bao'an Vil.          19 ((120.2304 22.93544, 120.2301 22…\n 9 &lt;int [6]&gt; &lt;dbl [6]&gt; Beihua Vil.          82 ((120.2034 23.00257, 120.2032 23…\n10 &lt;int [4]&gt; &lt;dbl [4]&gt; Beimen Vil.         249 ((120.2176 23.02032, 120.2173 23…\n# ℹ 239 more rows"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#performing-global-morans-i-test",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#performing-global-morans-i-test",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 1)",
    "section": "5.2: Performing Global Moran’s I test",
    "text": "5.2: Performing Global Moran’s I test\nNext, we can perform the global moran’s I test using the code below.\n\n\nCode\nglobal_moran_test(wm_q$total_cases,\n                  wm_q$nb,\n                  wm_q$wt)\n\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 12.062, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.440989474      -0.004032258       0.001361217 \n\n\nFrom the test, we can see that under the assumption of spatial random distribution, the p-value &lt; 2.2e-16. This allows us to infer that the number of dengue cases are not spatially randomly distributed across villages at 95% confidence level, with significant signs of clustering/outliers."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#performing-global-morans-i-permutation-test",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#performing-global-morans-i-permutation-test",
    "title": "Take-home Exercise 2: Application of Spatial and Spatio-temporal Analysis Methods to Discover the Distribution of Dengue Fever in Tainan City, Taiwan (Part 1)",
    "section": "5.3: Performing Global Moran’s I permutation test",
    "text": "5.3: Performing Global Moran’s I permutation test\nVery importantly, we should also conduct the global moran’s I permutation test.\n\n\nCode\nset.seed(1234)\n\n\n\n\nCode\nglobal_moran_perm(wm_q$total_cases,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim=79)\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 80 \n\nstatistic = 0.44099, observed rank = 80, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nFrom the global moran’s I permutation test, under the assumption of spatial random distribution, the p-value &lt; 2.2e-16. This further confirms that the number of dengue cases are not spatially randomly distributed across villages at 95% confidence level.\nThis exercise is continued at Take-home Exercise 2 (Part 2). Please click on this link to be directed there."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html",
    "title": "In-class Exercise 3: Kernel Density Estimation",
    "section": "",
    "text": "In this exercise, we would be using the following three data sets:\n\nCHILDCARE (geojson format)\n\nA point feature data providing both location and attribute information of childcare centres\nSource: Data.gov.sg\n\nMP14_SUBZONE_WEB_PL (ESRI shapefile format)\n\nA polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data\nSource: Data.gov.sg\n\nCostalOutline (ESRI shapefile format)\n\nA polygon feature data showing the national boundary of Singapore\nSource: SLA\n\n\n\n\n\nIn this exercise, the following five R packages will be used”\n\nsf\nspatspat\nraster\nmaptools\ntmap\n\nTo install and load the packages, we apply the following code:\n\n\nCode\n#|eval: FALSE\npacman::p_load(maptools, sf, raster, spatstat, tmap, tidyverse)\n\n\n\n\n\n\n\nIn this section, we will use st_read() from the sf package to import the three geospatial data sets into R.\n\n\nCode\n#|eval: FALSE\nchildcare_sf &lt;- st_read(dsn = \"data/ChildCareServices.geojson\") %&gt;%\n  st_transform(crs=3414)\n\n\nReading layer `ChildCareServices' from data source \n  `/Users/binhui-ong/IS415-GAA/In-class_Ex/In-class_Ex03/data/ChildCareServices.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\nCode\n#|eval: FALSE\nmpsz_sf &lt;- st_read(dsn=\"data\", layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/binhui-ong/IS415-GAA/In-class_Ex/In-class_Ex03/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\nCode\n#|eval: FALSE\nsg_sf &lt;- st_read(dsn=\"data\", layer = \"MP14_SUBZONE_WEB_PL\") %&gt;% st_union()\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/binhui-ong/IS415-GAA/In-class_Ex/In-class_Ex03/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nAlternatively, to get Coastal Outline sg_sf,\nwe can derive it from mpsz_sf.\n\n\nCode\n#|eval: FALSE\nsg_sf &lt;- mpsz_sf %&gt;% st_union()\n\n\nWe can also use st_combine() (instead of st_union()) to get sg_sf. However, unlike st_union(), internal boundaries are not resolved in st_combine().\n\n\nCode\n#|eval: FALSE\nsg_sf &lt;- mpsz_sf %&gt;% st_combine()\n\n\nLet’s examine how the mpsz_sf data set looks like on a map.\n\n\nCode\nplot(mpsz_sf)\n\n\n\n\n\n\n\nCode\nplot(sg_sf)\n\n\n\n\n\n\n\n\nNext, we will use as.ppp() from spatstat package to convert the spatial data into spatstat’s ppp object format:\n\n\nCode\nchildcare_ppp &lt;- as.ppp(childcare_sf)\n\n\n\n\n\nWe can check the duplication in a ppp object by using the following code chunk:\n\n\nCode\nany(duplicated(childcare_ppp))\n\n\n[1] FALSE\n\n\nFrom the above, we see that there is no duplicated point.\nHowever, suppose there are duplicated points, to count the number of co-incidence points, we will use multiplicity() function as shown in the code chunk below.\nTo find out how many locations have more than one point event, we can use the code:\n\n\nCode\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n\n[1] 0\n\n\nTo overcome the issue of duplicate points, we can use any of the following 3 solutions:\n\nDelete the duplicates (however, this would lead to the loss of some useful point events)\nJittering: To add a small pertubation to the duplicate points, so that they do not occupy the exact same space\nMake each point “unique”, and attach the duplicates of the points to the patterns as marks, as attributes of the points. Then, we would need analytical techniques that take into account these marks.\n\n\n\nCode\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp, \n                             retry=TRUE,\n                             nsim = 1,\n                             drop = TRUE)\n\n\n\n\nCode\nany(duplicated(childcare_ppp_jit))\n\n\n[1] FALSE\n\n\nThereafter, we should expect no more duplicated points.\n\n\n\nWe can use as.owin(). Note that the object in as.owin() must be an sf object.\n\n\nCode\nsg_owin &lt;- as.owin(sg_sf)\n\n\n\n\n\nTo extract our study areas, we use the filter() function from dplyr package. Do note that this code would only apply on sf objects.\n\n\nCode\npg &lt;- mpsz_sf %&gt;% \n  filter(PLN_AREA_N == \"PUNGGOL\")\ntm &lt;- mpsz_sf %&gt;% \n  filter(PLN_AREA_N == \"TAMPINES\")\nck &lt;- mpsz_sf %&gt;% \n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\njw &lt;- mpsz_sf %&gt;% \n  filter(PLN_AREA_N == \"JURONG WEST\")\n\n\nPlotting target planning areas:\n\n\nCode\npar(mfrow = c(2,2))\nplot(pg, main = \"Punggol\")\n\n\n\n\n\nCode\nplot(tm, main = \"Tampines\")\n\n\n\n\n\nCode\nplot(ck, main= \"Choa Chu Kang\")\n\n\n\n\n\nCode\nplot(jw, main = \"Jurong West\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#data-acquisition",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#data-acquisition",
    "title": "In-class Exercise 3: Kernel Density Estimation",
    "section": "",
    "text": "In this exercise, we would be using the following three data sets:\n\nCHILDCARE (geojson format)\n\nA point feature data providing both location and attribute information of childcare centres\nSource: Data.gov.sg\n\nMP14_SUBZONE_WEB_PL (ESRI shapefile format)\n\nA polygon feature data providing information of URA 2014 Master Plan Planning Subzone boundary data\nSource: Data.gov.sg\n\nCostalOutline (ESRI shapefile format)\n\nA polygon feature data showing the national boundary of Singapore\nSource: SLA"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#installing-and-loading-the-r-packages",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#installing-and-loading-the-r-packages",
    "title": "In-class Exercise 3: Kernel Density Estimation",
    "section": "",
    "text": "In this exercise, the following five R packages will be used”\n\nsf\nspatspat\nraster\nmaptools\ntmap\n\nTo install and load the packages, we apply the following code:\n\n\nCode\n#|eval: FALSE\npacman::p_load(maptools, sf, raster, spatstat, tmap, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#spatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#spatial-data-wrangling",
    "title": "In-class Exercise 3: Kernel Density Estimation",
    "section": "",
    "text": "In this section, we will use st_read() from the sf package to import the three geospatial data sets into R.\n\n\nCode\n#|eval: FALSE\nchildcare_sf &lt;- st_read(dsn = \"data/ChildCareServices.geojson\") %&gt;%\n  st_transform(crs=3414)\n\n\nReading layer `ChildCareServices' from data source \n  `/Users/binhui-ong/IS415-GAA/In-class_Ex/In-class_Ex03/data/ChildCareServices.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\n\nCode\n#|eval: FALSE\nmpsz_sf &lt;- st_read(dsn=\"data\", layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/binhui-ong/IS415-GAA/In-class_Ex/In-class_Ex03/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\nCode\n#|eval: FALSE\nsg_sf &lt;- st_read(dsn=\"data\", layer = \"MP14_SUBZONE_WEB_PL\") %&gt;% st_union()\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/binhui-ong/IS415-GAA/In-class_Ex/In-class_Ex03/data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nAlternatively, to get Coastal Outline sg_sf,\nwe can derive it from mpsz_sf.\n\n\nCode\n#|eval: FALSE\nsg_sf &lt;- mpsz_sf %&gt;% st_union()\n\n\nWe can also use st_combine() (instead of st_union()) to get sg_sf. However, unlike st_union(), internal boundaries are not resolved in st_combine().\n\n\nCode\n#|eval: FALSE\nsg_sf &lt;- mpsz_sf %&gt;% st_combine()\n\n\nLet’s examine how the mpsz_sf data set looks like on a map.\n\n\nCode\nplot(mpsz_sf)\n\n\n\n\n\n\n\nCode\nplot(sg_sf)\n\n\n\n\n\n\n\n\nNext, we will use as.ppp() from spatstat package to convert the spatial data into spatstat’s ppp object format:\n\n\nCode\nchildcare_ppp &lt;- as.ppp(childcare_sf)\n\n\n\n\n\nWe can check the duplication in a ppp object by using the following code chunk:\n\n\nCode\nany(duplicated(childcare_ppp))\n\n\n[1] FALSE\n\n\nFrom the above, we see that there is no duplicated point.\nHowever, suppose there are duplicated points, to count the number of co-incidence points, we will use multiplicity() function as shown in the code chunk below.\nTo find out how many locations have more than one point event, we can use the code:\n\n\nCode\nsum(multiplicity(childcare_ppp) &gt; 1)\n\n\n[1] 0\n\n\nTo overcome the issue of duplicate points, we can use any of the following 3 solutions:\n\nDelete the duplicates (however, this would lead to the loss of some useful point events)\nJittering: To add a small pertubation to the duplicate points, so that they do not occupy the exact same space\nMake each point “unique”, and attach the duplicates of the points to the patterns as marks, as attributes of the points. Then, we would need analytical techniques that take into account these marks.\n\n\n\nCode\nchildcare_ppp_jit &lt;- rjitter(childcare_ppp, \n                             retry=TRUE,\n                             nsim = 1,\n                             drop = TRUE)\n\n\n\n\nCode\nany(duplicated(childcare_ppp_jit))\n\n\n[1] FALSE\n\n\nThereafter, we should expect no more duplicated points.\n\n\n\nWe can use as.owin(). Note that the object in as.owin() must be an sf object.\n\n\nCode\nsg_owin &lt;- as.owin(sg_sf)\n\n\n\n\n\nTo extract our study areas, we use the filter() function from dplyr package. Do note that this code would only apply on sf objects.\n\n\nCode\npg &lt;- mpsz_sf %&gt;% \n  filter(PLN_AREA_N == \"PUNGGOL\")\ntm &lt;- mpsz_sf %&gt;% \n  filter(PLN_AREA_N == \"TAMPINES\")\nck &lt;- mpsz_sf %&gt;% \n  filter(PLN_AREA_N == \"CHOA CHU KANG\")\njw &lt;- mpsz_sf %&gt;% \n  filter(PLN_AREA_N == \"JURONG WEST\")\n\n\nPlotting target planning areas:\n\n\nCode\npar(mfrow = c(2,2))\nplot(pg, main = \"Punggol\")\n\n\n\n\n\nCode\nplot(tm, main = \"Tampines\")\n\n\n\n\n\nCode\nplot(ck, main= \"Choa Chu Kang\")\n\n\n\n\n\nCode\nplot(jw, main = \"Jurong West\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#installing-and-loading-r-packages",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#installing-and-loading-r-packages",
    "title": "In-class Exercise 3: Kernel Density Estimation",
    "section": "2.1 Installing and Loading R Packages",
    "text": "2.1 Installing and Loading R Packages\n\n\nCode\n#|eval: FALSE\npacman::p_load(sp ,sf, spNetwork, tmap, classInt, viridis, tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#data-import",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#data-import",
    "title": "In-class Exercise 3: Kernel Density Estimation",
    "section": "2.2 Data Import",
    "text": "2.2 Data Import\n\n\nCode\n#|eval: FALSE\nnetwork &lt;- st_read(dsn = \"data 1/geospatial\", \n                   layer = \"Punggol_St\")\n\n\nReading layer `Punggol_St' from data source \n  `/Users/binhui-ong/IS415-GAA/In-class_Ex/In-class_Ex03/data 1/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2642 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 34038.56 ymin: 40941.11 xmax: 38882.85 ymax: 44801.27\nProjected CRS: SVY21 / Singapore TM\n\n\nCode\nchildcare &lt;- st_read(dsn = \"data 1/geospatial\",\n                     layer = \"Punggol_CC\")\n\n\nReading layer `Punggol_CC' from data source \n  `/Users/binhui-ong/IS415-GAA/In-class_Ex/In-class_Ex03/data 1/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 61 features and 1 field\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 34423.98 ymin: 41503.6 xmax: 37619.47 ymax: 44685.77\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\n\n\nWe can examine the structure of the output SpatialDataFrame in RStudio.\nAlternatively, the code chunk below can be used to print the content of network SpatialLineDataFrame and childcare SpatialPointsDataFrame.\n\n\nCode\nstr(network)\n\n\nClasses 'sf' and 'data.frame':  2642 obs. of  3 variables:\n $ LINK_ID : num  1.16e+08 1.16e+08 1.16e+08 1.16e+08 1.16e+08 ...\n $ ST_NAME : chr  \"PUNGGOL RD\" \"PONGGOL TWENTY-FOURTH AVE\" \"PONGGOL SEVENTEENTH AVE\" \"PONGGOL SEVENTEENTH AVE\" ...\n $ geometry:sfc_LINESTRING of length 2642; first list element:  'XY' num [1:2, 1:2] 36547 36559 44575 44614\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA\n  ..- attr(*, \"names\")= chr [1:2] \"LINK_ID\" \"ST_NAME\"\n\n\nCode\nstr(childcare)\n\n\nClasses 'sf' and 'data.frame':  61 obs. of  2 variables:\n $ Name    : chr  \"kml_10\" \"kml_99\" \"kml_100\" \"kml_101\" ...\n $ geometry:sfc_POINT of length 61; first list element:  'XYZ' num  36174 42550 0\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA\n  ..- attr(*, \"names\")= chr \"Name\""
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#geospatial-data-visualization",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#geospatial-data-visualization",
    "title": "In-class Exercise 3: Kernel Density Estimation",
    "section": "2.3 Geospatial Data Visualization",
    "text": "2.3 Geospatial Data Visualization\nTo visualise geospatial data in a high cartographic quality and interactive manner, the mapping function of tmap package can be used as shown in the code chunk below.\n\n\nCode\ntmap_mode('view')\ntm_shape(childcare) + \ntm_dots()+\ntm_shape(network) + \ntm_lines()\n\n\n\n\n\n\n\nCode\ntmap_mode('plot')"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#network-constrained-kde-netkde-analysis",
    "href": "In-class_Ex/In-class_Ex03/In-class_Ex03.html#network-constrained-kde-netkde-analysis",
    "title": "In-class Exercise 3: Kernel Density Estimation",
    "section": "2.4 Network Constrained KDE (NetKDE) Analysis",
    "text": "2.4 Network Constrained KDE (NetKDE) Analysis\nIn this section, we will perform NetKDE analysis by using appropriate functions provided in spNetwork package.\n\n2.4.1 Preparing the lixels objects\nBefore computing NetKDE, the SpatialLines object need to be cut into lixels with a specified minimal distance. This task can be performed by using with lixelize_lines() of spNetwork as shown in the code chunk below.\n\n\nCode\nlixels &lt;- lixelize_lines(network,\n                         750,\n                         mindist = 375)\n\n\n\n\n2.4.2 Generating line centre points\n\n\nCode\nsamples &lt;- lines_center(lixels)\n\n\n\n\n2.4.3 Performing NetKDE\nWe can compute the NetKDE by using the code chunk below:\n\n\nCode\ndensities &lt;- nkde(network, \n                  events = childcare,\n                  w = rep(1,nrow(childcare)),\n                  samples = samples,\n                  kernel_name = \"quartic\",\n                  bw = 300, \n                  div= \"bw\", \n                  method = \"simple\", \n                  digits = 1, \n                  tol = 1,\n                  grid_shape = c(1,1), \n                  max_depth = 8,\n                  agg = 5, \n                  sparse = TRUE,\n                  verbose = FALSE)\n\n\n\n\n2.4.3.1 Visualizing NetKDE\nBefore we can visualise the NetKDE values, code chunk below will be used to insert the computed density values (i.e. densities) into samples and lixels objects as density field.\n\n\nCode\nsamples$density &lt;- densities\nlixels$density &lt;- densities\n\n\nRescaling to help mapping:\n\n\nCode\nsamples$density &lt;- samples$density*1000\nlixels$density &lt;- lixels$density*1000\n\n\nThe code below uses appropriate functions of tmap package to prepare interactive and high cartographic quality map visualisation.\n\n\nCode\ntmap_mode('view')\ntm_shape(lixels)+\n  tm_lines(col=\"density\")+\ntm_shape(childcare)+\n  tm_dots()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_Ex05/In-class_Ex05.html",
    "title": "In-class Exercise 5: Global and Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "Install packages\n\npacman::p_load(sf, tmap, sfdep, tidyverse)\n\n\n\nThe Data\nGeospatial Data:\n\nhunan &lt;- st_read(dsn = \"data/geospatial\",\n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `/Users/binhui-ong/IS415-GAA/In-class_Ex/In-class_Ex05/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nAttribute Data:\n\nhunan2012 &lt;- read.csv(\"data/aspatial/Hunan_2012.csv\")\n\n\nhunan_GDPPC1 &lt;- read.csv(\"data/aspatial/Hunan_GDPPC.csv\")\n\nTo join the geospatial and attribute data, and select the relevant columns of the result,\n\nhunan_GDPPC &lt;- left_join(hunan, hunan2012) %&gt;% dplyr::select(1:4, 7,15) \n\n\nwm_q &lt;- hunan_GDPPC %&gt;% \n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"))\n\n\n\nVisualizing Regional Development Indicator\n\nequal &lt;- tm_shape(hunan_GDPPC) + \n  tm_fill(\"GDPPC\", \n          n=5,\n          style = \"equal\") + \n  tm_borders(alpha=0.5) + \n  tm_layout(main.title = \"Equal interval classification\", main.title.size = 0.8)\n\nquantile &lt;- tm_shape(hunan_GDPPC) + \n  tm_fill(\"GDPPC\",\n          n = 5, \n          style = \"quantile\") + \n  tm_borders (alpha = 0.5) + \n  tm_layout(main.title = \"Equal quantile classfication\", main.title.size = 0.8)\n\ntmap_arrange(equal, quantile, asp = 1, ncol = 2 )\n\n\n\n\n\n\nGlobal Moran’s\nBest method: Global Moran’s permutation test\n\nglobal_moran_perm(wm_q$GDPPC, \n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 99)\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\nSecond best alternative: Global Moran Test\n\nglobal_moran_test(wm_q$GDPPC, \n                  wm_q$nb,\n                  wm_q$wt)\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nHowever, it is only based on one observation.\nComputing Global Moran’s I\n\nmoranI  &lt;- global_moran\nmoranI\n\nfunction (x, nb, wt, na_ok = FALSE, ...) \n{\n    listw &lt;- recreate_listw(nb, wt)\n    spdep::moran(x, listw = listw, n = length(nb), S0 = spdep::Szero(listw), \n        NAOK = na_ok, ...)\n}\n&lt;bytecode: 0x7f86f607be58&gt;\n&lt;environment: namespace:sfdep&gt;\n\n\nThis is not so helpful and not necessary."
  }
]